#!/usr/bin/env python
"""
CodeRankEmbed ä¸“ç”¨ç´¢å¼•è¯„ä¼°å·¥å…·
ä½¿ç”¨ transformers AutoModel åç«¯
"""

import argparse
import json
import logging
import os
import os.path as osp
import re
import sys
from pathlib import Path
from typing import List, Dict, Tuple

import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

from method.mapping import ASTBasedMapper, GraphBasedMapper
from method.retrieval.common import (
    instance_id_to_repo_name,
    get_problem_text,
    rank_files,
    load_index,
    load_jsonl,
)


def embed_texts(
    texts: List[str],
    model: AutoModel,  # æ˜ç¡®ç±»å‹
    tokenizer: AutoTokenizer,
    max_length: int,
    batch_size: int,
    device: torch.device,
) -> torch.Tensor:
    """CodeRankEmbed ä¸“ç”¨çš„æ–‡æœ¬ embedding å‡½æ•°"""
    from torch.utils.data import Dataset, DataLoader

    class TextDataset(Dataset):
        def __init__(self, items: List[str]):
            self.items = items

        def __len__(self):
            return len(self.items)

        def __getitem__(self, idx: int):
            encoded = tokenizer(
                self.items[idx],
                truncation=True,
                max_length=max_length,
                padding="max_length",
                return_tensors="pt",
            )
            return encoded["input_ids"].squeeze(0), encoded["attention_mask"].squeeze(0)

    ds = TextDataset(texts)
    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)
    model.eval()
    outs: List[torch.Tensor] = []
    with torch.no_grad():
        for input_ids, attn_mask in loader:
            input_ids = input_ids.to(device)
            attn_mask = attn_mask.to(device)
            # CodeRankEmbed ä½¿ç”¨ AutoModel
            outputs = model(input_ids=input_ids, attention_mask=attn_mask)
            token_embeddings = outputs[0]
            sent_emb = token_embeddings[:, 0]  # [CLS] token
            sent_emb = torch.nn.functional.normalize(sent_emb, p=2, dim=1)
            outs.append(sent_emb.cpu())
    return torch.cat(outs, dim=0)


from method.utils import clean_file_path


def load_model(
    model_name: str,
    device: torch.device,
    trust_remote_code: bool = False,
) -> Tuple[AutoModel, AutoTokenizer]:
    """åŠ è½½ CodeRankEmbed æ¨¡å‹ï¼ˆå›ºå®šä½¿ç”¨ AutoModelï¼‰"""
    print(f"Loading CodeRankEmbed model from {model_name}")

    if not os.path.exists(model_name):
        print(f"âŒ Error: Model path does not exist: {model_name}")
        raise FileNotFoundError(f"Model path not found: {model_name}")

    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=trust_remote_code
        )
        model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=trust_remote_code
        ).to(device)
        model.eval()
        print(f"âœ… Model loaded successfully")
        return model, tokenizer
    except ValueError as e:
        if "trust_remote_code" in str(e).lower():
            print(f"âŒ Error: Model requires trust_remote_code=True")
            raise
        else:
            raise


def main():
    parser = argparse.ArgumentParser(
        description="CodeRankEmbed ä¸“ç”¨ç´¢å¼•è¯„ä¼°å·¥å…·"
    )

    # æ•°æ®å‚æ•°
    parser.add_argument("--dataset_path", type=str, required=True,
                       help="Path to dataset JSONL file")
    parser.add_argument("--index_dir", type=str, required=True,
                       help="Directory containing pre-built indexes")
    parser.add_argument("--output_folder", type=str, required=True,
                       help="Output directory for results")

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_name", type=str, required=True,
                       help="CodeRankEmbed model path")
    parser.add_argument("--trust_remote_code", action="store_true",
                       help="Allow execution of model repository code")

    # æ£€ç´¢å‚æ•°
    parser.add_argument("--top_k_blocks", type=int, default=50,
                       help="Number of top blocks to retrieve")
    parser.add_argument("--top_k_files", type=int, default=20,
                       help="Number of top files to retrieve")
    parser.add_argument("--top_k_modules", type=int, default=20,
                       help="Number of top modules to retrieve")
    parser.add_argument("--top_k_entities", type=int, default=50,
                       help="Number of top entities to retrieve")
    parser.add_argument("--max_length", type=int, default=4096,
                       help="Maximum sequence length")
    parser.add_argument("--batch_size", type=int, default=8,
                       help="Batch size for embedding")

    # æ˜ å°„å‚æ•°
    parser.add_argument("--mapper_type", type=str, default="ast",
                       choices=["ast", "graph"],
                       help="Type of mapper to use")
    parser.add_argument("--repos_root", type=str,
                       default="/workspace/locbench/repos/locbench_repos",
                       help="Root directory of repositories")

    # å…¶ä»–å‚æ•°
    parser.add_argument("--gpu_id", type=int, default=0,
                       help="GPU ID to use")
    parser.add_argument("--force_cpu", action="store_true",
                       help="Force CPU usage")

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    if args.force_cpu:
        device = torch.device('cpu')
    else:
        device = torch.device(f'cuda:{args.gpu_id}')

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_folder)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    try:
        model, tokenizer = load_model(args.model_name, device, args.trust_remote_code)
    except Exception as e:
        print(f"Failed to load model: {e}")
        return

    # åŠ è½½æ•°æ®é›†
    print(f"Loading dataset from {args.dataset_path}")
    dataset = load_jsonl(args.dataset_path)

    print(f"Loaded {len(dataset)} instances")

    # åˆå§‹åŒ–æ˜ å°„å™¨
    if args.mapper_type == "ast":
        mapper = ASTBasedMapper(args.repos_root)
    else:
        mapper = GraphBasedMapper(args.repos_root)

    results = []

    # ç¼“å­˜ç´¢å¼•ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
    index_cache: Dict[str, Tuple[torch.Tensor, List[dict]]] = {}

    def get_cached_index(repo_name: str):
        if repo_name not in index_cache:
            embeddings, metadata = load_index(repo_name, args.index_dir)
            # ç´¢å¼•ä¿ç•™åœ¨ CPUï¼Œé¿å… GPU å†…å­˜ä¸è¶³
            index_cache[repo_name] = (embeddings, metadata)
        return index_cache[repo_name]

    # å¤„ç†ç»Ÿè®¡
    index_found = 0
    index_missing = 0
    missing_repos = []

    for instance in tqdm(dataset, desc="Processing instances"):
        instance_id = instance.get("instance_id", "")
        repo_name = instance_id_to_repo_name(instance_id)

        try:
            # åŠ è½½ç´¢å¼•ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            embeddings, metadata = get_cached_index(repo_name)

            if embeddings is None or metadata is None:
                # ç´¢å¼•ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºç»“æœ
                index_missing += 1
                missing_repos.append(repo_name)
                results.append({
                    "instance_id": instance_id,
                    "found_files": [],
                    "found_modules": [],
                    "found_entities": [],
                    "raw_output_loc": []
                })
                continue

            index_found += 1

            # è·å–é—®é¢˜æ–‡æœ¬
            query_text = get_problem_text(instance)
            if not query_text:
                print(f"Warning: No query text found for instance {instance_id}")
                continue

            # ç¼–ç æŸ¥è¯¢
            query_embedding = embed_texts([query_text], model, tokenizer, args.max_length, 1, device)[0]

            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä¸´æ—¶ç§»åˆ° GPUï¼‰
            query_emb_gpu = query_embedding.to(device)
            embeddings_gpu = embeddings.to(device)  # ä¸´æ—¶ç§»åˆ° GPU
            similarities = torch.matmul(query_emb_gpu.unsqueeze(0), embeddings_gpu.t()).squeeze(0)  # (num_blocks,)
            similarities = similarities.cpu()  # ç§»å› CPU ä»¥ä¾¿åç»­å¤„ç†

            # è·å–top-kå—ï¼ˆç¡®ä¿ä¸è¶…è¿‡å¯ç”¨å—æ•°ï¼‰
            k = min(args.top_k_blocks, len(similarities))
            if k == 0:
                found_files = []
                found_modules = []
                found_entities = []
            else:
                top_k_values, top_k_indices = torch.topk(similarities, k)
                block_scores = list(zip(top_k_indices.tolist(), top_k_values.tolist()))

                # è·å–æ–‡ä»¶çº§åˆ«ç»“æœ
                found_files = rank_files(block_scores, metadata, args.top_k_files, repo_name)

                # æ˜ å°„ä»£ç å—åˆ°å‡½æ•°/æ¨¡å—
                # æ¸…ç† top_blocks ä¸­çš„ file_pathï¼Œä½¿å…¶ä¸ GT æ ¼å¼ä¸€è‡´
                top_blocks = []
                for idx, _ in block_scores:
                    block = metadata[idx].copy()  # å¤åˆ¶ä»¥é¿å…ä¿®æ”¹åŸå§‹ metadata
                    original_path = block.get('file_path', '')
                    if original_path:
                        block['file_path'] = clean_file_path(original_path, repo_name)
                    top_blocks.append(block)

                found_modules, found_entities = mapper.map_blocks_to_entities(
                    blocks=top_blocks,
                    instance_id=instance_id,
                    top_k_modules=args.top_k_modules,
                    top_k_entities=args.top_k_entities,
                )

            # ä¿å­˜ç»“æœ
            results.append({
                "instance_id": instance_id,
                "found_files": found_files,
                "found_modules": found_modules,
                "found_entities": found_entities,
                "raw_output_loc": []
            })

        except Exception as e:
            print(f"Error processing instance {instance_id}: {e}")
            results.append({
                "instance_id": instance_id,
                "error": str(e),
                "found_files": [],
                "found_modules": [],
                "found_entities": [],
            })

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆç»“æœç»Ÿä¸€ç”¨ found_filesï¼Œä¸ loc_outputs.jsonl ä¸€è‡´ï¼‰
    total_instances = len(results)
    successful_instances = sum(1 for r in results if r.get('found_files') and len(r['found_files']) > 0)
    failed_instances = sum(1 for r in results if 'error' in r)

    if successful_instances > 0:
        avg_files = sum(len(r.get('found_files', [])) for r in results) / successful_instances
    else:
        avg_files = 0

    # ä¿å­˜ç»“æœ
    output_path = output_dir / "loc_outputs.jsonl"
    with output_path.open('w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')

    print(f"Results saved to {output_path}")
    print(f"ğŸ“Š Retrieval Summary:")
    print(f"   Total instances: {total_instances}")
    print(f"   Successful: {successful_instances} ({successful_instances/total_instances:.1%})")
    print(f"   Failed: {failed_instances}")
    print(f"   Average files per instance: {avg_files:.1f}")

    # è¾“å‡ºç´¢å¼•æŸ¥æ‰¾ç»Ÿè®¡
    print(f"\nIndex Statistics:")
    print(f"  Found: {index_found}/{len(dataset)}")
    print(f"  Missing: {index_missing}/{len(dataset)}")
    if missing_repos:
        unique_missing = list(set(missing_repos))[:10]
        print(f"  Missing repos (sample): {unique_missing}")

    if successful_instances > 0:
        print(f"\nâœ… Retrieval completed successfully!")
    else:
        print(f"\nâŒ No successful retrievals!")


if __name__ == "__main__":
    main()
