{"summary": "Implements a 2‑dimensional spatial transformer that applies multi‑head attention with a gated linear unit activation to image‑like tensors, supporting configurable depth, dropout, linear projection, cross‑attention only mode, and optional memory‑efficient attention for faster and lower‑memory processing.", "business_intent": "Provide a reusable, high‑performance transformer component for diffusion‑based image generation and other vision models, enabling efficient attention mechanisms and flexible configuration to improve quality and speed of generated visuals.", "keywords": ["spatial transformer", "GLU activation", "multi-head attention", "cross-attention", "memory-efficient attention", "Flax", "JAX", "image generation", "diffusion models", "dropout", "configurable depth"], "summary_hash": "0deabeaf9918", "cached_at": "2026-02-09T04:00:18+00:00"}