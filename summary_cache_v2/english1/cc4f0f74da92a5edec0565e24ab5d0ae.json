{"summary": "Implements a configurable transformer layer that can operate as an encoder or a decoder, optionally inserting a cross‑attention block for seq2seq usage, and includes utilities for pruning attention heads and handling input embeddings.", "business_intent": "Offer a flexible TensorFlow component for constructing transformer‑based models, enabling developers to build encoder‑only, decoder‑only, or encoder‑decoder architectures for NLP and related tasks such as language modeling, translation, or contact prediction.", "keywords": ["transformer", "encoder", "decoder", "cross-attention", "self-attention", "seq2seq", "TensorFlow", "attention heads pruning", "input embeddings", "model layer"], "summary_hash": "9f244e36fd7f", "cached_at": "2026-02-09T09:51:42+00:00"}