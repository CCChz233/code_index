{"summary": "A deprecated neural network layer that applies alpha dropout to randomly mask inputs while preserving the mean and variance required for SELU selfâ€‘normalizing activations.", "business_intent": "Offer regularization during model training by dropping units in a way that maintains statistical properties of the data, though the implementation is outdated and should be replaced by the current version.", "keywords": ["alpha dropout", "regularization", "deprecated", "SELU", "neural network layer", "mean preservation", "variance preservation", "training"], "summary_hash": "5ac6ebea1713", "cached_at": "2026-02-09T11:21:37+00:00"}