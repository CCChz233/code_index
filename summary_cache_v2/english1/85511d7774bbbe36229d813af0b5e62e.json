{"summary": "A wrapper around the 4‑bit linear module that allows the layer to be created directly on a target device, supports lazy meta‑device initialization, automatically re‑quantizes weights when a state dict is loaded, and can materialize the quantized parameters on demand.", "business_intent": "Enable memory‑efficient, low‑precision linear operations for large neural networks, simplifying deployment and checkpoint loading on GPUs while preserving performance through automatic quantization handling.", "keywords": ["linear layer", "4-bit quantization", "device placement", "meta device", "re-quantization", "state dict loading", "materialization", "wrapper", "bitsandbytes"], "summary_hash": "03a50a6966d5", "cached_at": "2026-02-08T08:30:17+00:00"}