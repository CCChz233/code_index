{"summary": "Provides the decoder component of a sequence‑to‑sequence model built on an LSTM, employing a lifted scan transform for efficient recurrent computation. It processes prior token embeddings and hidden states, optionally uses teacher forcing, and produces probability distributions over a predefined vocabulary.", "business_intent": "Generate target sequences from encoded inputs for tasks such as machine translation, summarization, or other conditional language generation applications.", "keywords": ["decoder", "LSTM", "seq2seq", "teacher forcing", "vocabulary", "recurrent neural network", "scan transform", "language generation", "neural translation"], "summary_hash": "54895544f7c6", "cached_at": "2026-02-09T11:53:03+00:00"}