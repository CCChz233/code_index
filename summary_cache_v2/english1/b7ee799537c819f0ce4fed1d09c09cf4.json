{"summary": "The script loads a CLIP model to embed real image captions and synthetic images, computes cosine similarity scores between each image and the corresponding captions, averages the scores per sub‑folder (representing a generation configuration), and writes the results to a CSV file.", "business_intent": "Provide an automated metric for assessing the visual fidelity of text‑to‑image generation pipelines by quantifying how well generated images align with their source captions, enabling model benchmarking and quality monitoring.", "keywords": ["CLIP", "image‑text similarity", "synthetic image evaluation", "caption alignment", "metric computation", "CSV reporting", "PyTorch", "OpenAI CLIP", "model benchmarking"], "summary_hash": "d91e1d6e24c9", "cached_at": "2026-02-08T11:44:42+00:00"}