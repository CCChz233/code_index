{"summary": "Implements the KERPLE relative position embedding that injects linear bias terms into attention scores according to forward and backward distances, supporting both auto‑regressive decoding and joint encoder scenarios.", "business_intent": "Provide an efficient, symmetric relative‑position encoding for transformer models to improve sequence order handling and overall performance in language generation and encoding tasks.", "keywords": ["relative position embedding", "linear bias", "attention", "auto-regressive decoder", "joint encoder", "transformer", "KERPLE", "sequence modeling"], "summary_hash": "f4316ab32a0d", "cached_at": "2026-02-08T09:52:14+00:00"}