{"summary": "Defines data structures for managing per‑layer attention key and value caches in a HookedTransformer, allowing storage, retrieval, and incremental updating of past attention tensors to speed up text generation.", "business_intent": "Improve inference efficiency of transformer language models by reusing previously computed attention data, reducing redundant computation during step‑wise generation.", "keywords": ["attention cache", "key-value caching", "incremental generation", "transformer", "HookedTransformer", "past key values", "performance optimization", "torch", "dataclass"], "summary_hash": "402674018914", "cached_at": "2026-02-08T13:20:45+00:00"}