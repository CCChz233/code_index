{"summary": "Evaluates a sentence embedding model by measuring whether the similarity between an anchor sentence and its positive example exceeds that with a negative example by a configurable margin, and reports accuracy (e.g., cosine similarity accuracy) over a dataset of triplets.", "business_intent": "Offer a standardized metric to assess and compare the discriminative power of embedding models for tasks that require distinguishing related from unrelated sentences, supporting model selection, validation, and monitoring in NLP pipelines.", "keywords": ["triplet evaluation", "sentence embeddings", "similarity margin", "anchor-positive-negative", "accuracy metric", "cosine similarity", "model assessment", "NLP evaluation", "ranking performance", "dataset evaluation"], "summary_hash": "bb33dd70f070", "cached_at": "2026-02-08T13:47:50+00:00"}