{"summary": "A transformer model class that integrates the BigBird sparse-attention architecture with Pegasus sequence-to-sequence capabilities to perform question answering over long documents.", "business_intent": "Enable efficient, high‑accuracy question answering on extensive textual inputs for applications such as document search, knowledge retrieval, and customer support.", "keywords": ["BigBird", "Pegasus", "question answering", "long documents", "sparse attention", "transformer", "NLP", "model inference", "fine‑tuning"], "summary_hash": "49342c854749", "cached_at": "2026-02-09T06:52:20+00:00"}