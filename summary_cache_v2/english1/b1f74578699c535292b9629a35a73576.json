{"summary": "A configuration container that encapsulates all bitsandbytes quantization options for loading a model, allowing exclusive selection of 8‑bit or 4‑bit modes and exposing fine‑grained controls such as outlier thresholds, module exclusion, CPU offload, weight and compute data types, quantization type, double‑quantization and storage format.", "business_intent": "Enable developers and researchers to efficiently reduce model memory footprint and accelerate inference by providing a single, extensible interface for low‑bit quantization settings compatible with bitsandbytes, thereby simplifying deployment of large language models on limited hardware.", "keywords": ["bitsandbytes", "quantization", "8-bit", "4-bit", "LLM.int8", "FP4", "NF4", "model configuration", "memory optimization", "inference speed", "CPU offload", "threshold", "double quantization"], "summary_hash": "2ca9a73e10de", "cached_at": "2026-02-09T03:54:11+00:00"}