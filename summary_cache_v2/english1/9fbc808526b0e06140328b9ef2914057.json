{"summary": "Implements the UniSpeech-SAT architecture for self‑supervised speech pre‑training, providing a forward computation that produces contrastive logits and supporting configurable components such as feature encoders, extractors, and Gumbel temperature scaling.", "business_intent": "Facilitate large‑scale speech representation learning to boost downstream speech recognition or audio understanding applications while allowing selective freezing of model parts to reduce training cost.", "keywords": ["speech pretraining", "self‑supervised learning", "contrastive loss", "feature encoder", "feature extractor", "Gumbel temperature", "UniSpeech", "model freezing"], "summary_hash": "7ec4ebee44fa", "cached_at": "2026-02-09T12:05:15+00:00"}