{"summary": "Fast tokenizer for CPM models that leverages the Jieba segmentation library to pre‑tokenize Chinese text, handling special tokens, token type IDs, batch encoding, decoding and vocabulary persistence.", "business_intent": "Enable efficient preprocessing of Chinese text for CPM‑based language models, supporting downstream NLP applications such as language modeling, text generation, and other Chinese language tasks.", "keywords": ["Jieba", "Chinese tokenization", "CPM", "fast tokenizer", "pre‑tokenization", "special tokens", "batch encoding", "decoding", "vocabulary management", "token type IDs"], "summary_hash": "8188c0fa148a", "cached_at": "2026-02-09T09:23:45+00:00"}