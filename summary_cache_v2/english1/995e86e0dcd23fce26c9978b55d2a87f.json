{"summary": "The module defines custom distributed samplers that extend PyTorch's sampling mechanisms to handle uneven dataset partitions and per‑sample weighted sampling across multiple processes, while supporting shuffling, seeding, and drop‑last behavior.", "business_intent": "Facilitate scalable and balanced data loading for distributed deep‑learning training, allowing each worker to receive appropriate numbers of samples and to perform weighted random selection, thereby improving training efficiency and flexibility.", "keywords": ["distributed sampling", "PyTorch", "data loader", "uneven split", "weighted random sampling", "multi‑process", "shuffling", "seed", "drop_last", "replica", "rank"], "summary_hash": "6e17ddb6ca31", "cached_at": "2026-02-08T13:29:31+00:00"}