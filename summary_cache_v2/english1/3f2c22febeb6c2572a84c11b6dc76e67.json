{"summary": "Implements a metric that assesses the factual correctness of language‑model answers by breaking them into individual claims, applying natural‑language‑inference against reference passages, and summarizing the results as precision, recall or F1 scores according to a configurable mode and beta parameter.", "business_intent": "Enable developers and researchers to quantitatively evaluate and improve the factual reliability of responses generated by retrieval‑augmented generation systems or other LLM applications.", "keywords": ["factual correctness", "metric", "natural language inference", "claim decomposition", "precision", "recall", "F1 score", "beta weighting", "RAG evaluation", "language model assessment"], "summary_hash": "9b175a3e04c4", "cached_at": "2026-02-08T22:50:14+00:00"}