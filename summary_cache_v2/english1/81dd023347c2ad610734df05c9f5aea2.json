{"summary": "Provides acceptance tests that verify multi‑GPU support in the transformer‑lens library, checking that a GPT‑2 medium model can be instantiated across one or several GPUs, that model blocks are correctly assigned to distinct devices, and that activation caches respect device boundaries.", "business_intent": "Validate and guarantee correct device allocation and caching behavior for large transformer models when deployed on multi‑GPU systems, giving users confidence that the library functions reliably in distributed hardware environments.", "keywords": ["multi‑GPU", "device placement", "transformer model", "activation cache", "pytest", "torch", "HookedTransformer", "acceptance testing"], "summary_hash": "72cd70ca1b11", "cached_at": "2026-02-08T13:22:08+00:00"}