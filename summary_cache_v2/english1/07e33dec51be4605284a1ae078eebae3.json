{"summary": "Provides a neural‑network layer that computes scaled dot‑product attention from query, key, and value tensors, handling a non‑standard ordering of the tensor split operation.", "business_intent": "Offers a reusable attention component for transformer‑based models that delivers efficient QKV attention computation while supporting alternative split arrangements to fit diverse architectural requirements.", "keywords": ["attention", "QKV", "transformer", "neural network module", "scaled dot-product", "tensor split order", "efficient computation", "forward pass", "flops estimation"], "summary_hash": "bd2a6594b3c8", "cached_at": "2026-02-08T09:00:30+00:00"}