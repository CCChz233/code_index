{"summary": "This module provides sparse adaptive optimizers for node embedding layers in DGL, maintaining per‑embedding gradient statistics and adjusting learning rates while updating only the embeddings with non‑zero gradients. It includes support for CPU pinned memory and NCCL‑based distributed synchronization.", "business_intent": "Accelerate and scale training of graph neural networks with large node embedding tables by minimizing memory consumption and communication costs during optimization.", "keywords": ["sparse optimizer", "adaptive learning rate", "node embeddings", "DGL", "PyTorch", "pinned memory", "distributed training", "NCCL", "gradient accumulation", "moment statistics"], "summary_hash": "0758af80c553", "cached_at": "2026-02-09T00:47:01+00:00"}