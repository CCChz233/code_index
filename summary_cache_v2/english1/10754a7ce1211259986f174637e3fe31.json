{"summary": "A TensorFlow Keras layer that implements an efficient multi‑head attention mechanism and provides functionality to prune unnecessary attention heads.", "business_intent": "Enable scalable and resource‑efficient attention in deep learning models while allowing model size reduction through head pruning.", "keywords": ["TensorFlow", "Keras", "attention", "multi‑head", "pruning", "efficient", "neural network", "layer", "model compression"], "summary_hash": "90fb4eb37a6d", "cached_at": "2026-02-09T09:51:24+00:00"}