{"summary": "Implements the KERPLE relative position embedding used in Megatron‑based transformer models. The class computes linear bias terms that are added to attention scores, handling both forward (auto‑regressive) and backward (bidirectional) distance calculations to support decoder and encoder layers.", "business_intent": "Enhance the accuracy and flexibility of large‑scale language models by providing a sophisticated positional bias mechanism that improves attention modeling for both generation and understanding tasks.", "keywords": ["relative position embedding", "KERPLE", "attention bias", "forward distance", "backward distance", "decoder", "encoder", "Megatron", "transformer", "NLP", "positional encoding", "PyTorch"], "summary_hash": "6e017d42957b", "cached_at": "2026-02-08T11:23:43+00:00"}