{"summary": "Implements the Gaussian Error Linear Unit (GELU) activation, offering a callable interface that transforms input tensors and determines output specifications.", "business_intent": "Provide a ready-to-use GELU activation component for deep learning models to introduce nonâ€‘linear behavior during forward passes.", "keywords": ["GELU", "activation function", "neural networks", "deep learning", "non-linear transformation", "output specification"], "summary_hash": "9c9cea4d2785", "cached_at": "2026-02-09T11:34:13+00:00"}