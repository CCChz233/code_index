{"summary": "Implements a multi‑layer Set Transformer encoder that transforms a set of node features into contextualized node embeddings using multi‑head attention and position‑wise feed‑forward networks. The architecture can be built from either standard Set Attention Blocks or Induced Set Attention Blocks, with configurable hidden sizes, number of heads, feed‑forward dimension, layers, and dropout rates. It operates on DGL graphs, handling both single‑graph and batched inputs, and returns node‑wise representations.", "business_intent": "Provides a reusable neural module for processing unordered sets or graph node collections in deep learning pipelines, enabling permutation‑invariant feature learning for tasks such as graph‑level or node‑level prediction within graph neural network frameworks.", "keywords": ["set transformer", "encoder", "multi-head attention", "permutation invariant", "graph neural network", "DGL", "induced set attention", "set attention block", "dropout", "feed-forward network"], "summary_hash": "481fe59c0fcb", "cached_at": "2026-02-08T23:51:17+00:00"}