{"summary": "We need to produce JSON with summary, business_intent, keywords list.\n\nWe need to infer from file-level metadata and function summaries.\n\nFile path: nemo/collections/asr/data/audio_to_audio.py\n\nDocstring: none.\n\nImports: many audio processing libs, dataset, neural types.\n\nClass summaries: ASRAudioProcessor: prepares audio examples by loading, optionally normalizing, segmenting, and synchronizing multiple audio signals (input, target, reference) along with associated embedding vectors, returning them in a structured dictionary ready for model consumption.\n\nAudioToTargetDataset: not described but likely similar.\n\nAudioToTargetWithEmbeddingDataset: configurable dataset class that reads a manifest describing input audio files, corresponding target audio files, and associated embedding vectors. It loads audio at a specified sample rate, optionally extracts synchronized random subsegments, applies duration-based filtering, channel selection, and optional global normalization, returning a tuple of input, target, and embedding for each sample.\n\nAudioToTargetWithReferenceDataset: dataset class that reads a manifest describing input, target, and optional reference audio files, loads them at a specified sample rate, and returns synchronized or independent audio segments. Supports configurable keys, optional fixed or random sub‑segment durations, channel selection, length filtering, maximum utterance limits, and optional normalization based on a chosen signal.\n\nBaseAudioDataset: generic dataset abstraction for audio data, managing a collection of examples and applying a configurable audio processor to each item.\n\nStandalone function: _audio_collate_fn: no usage info.\n\nThus the module provides dataset classes for ASR tasks where audio inputs and targets (and optionally reference audio or embeddings) are loaded, processed, and collated for training models. It includes an audio processor that handles loading, normalization, segmentation, synchronization, and packaging of audio signals and embeddings.\n\nBusiness intent: Provide flexible data loading pipelines for speech recognition models, supporting paired audio, reference audio, and embedding inputs, enabling training and evaluation with proper preprocessing.\n\nKeywords: audio dataset, speech recognition, ASR, audio preprocessing, manifest, embedding, reference audio, normalization, segmentation, PyTorch Dataset, collate function.\n\nNow produce JSON.\n\nNeed to ensure no function names verbatim? The constraint: \"Do NOT list function names verbatim.\" That refers to not listing function names in summary/keywords? Probably avoid naming specific functions like _audio_collate_fn. So we can just describe functionality.\n\nLet's craft summary: \"The module defines a set of dataset utilities for automatic speech recognition that load audio files (input, target, optional reference)", "business_intent": "We need to produce JSON with summary, business_intent, keywords list.\n\nWe need to infer from file-level metadata and function summaries.\n\nFile path: nemo/collections/asr/data/audio_to_audio.py\n\nDocstring: none.\n\nImports: many audio processing libs, dataset, neural types.\n\nClass summaries: ASRAudioProcessor: prepares audio examples by loading, optionally normalizing, segmenting, and synchronizing multiple audio signals (input, target, reference) along with associated embedding vectors, returning them in a structured dictionary ready for model consumption.\n\nAudioToTargetDataset: not described but likely similar.\n\nAudioToTargetWithEmbeddingDataset: configurable dataset class that reads a manifest describing input audio files, corresponding target audio files, and associated embedding vectors. It loads audio at a specified sample rate, optionally extracts synchronized random subsegments, applies duration-based filtering, channel selection, and optional global normalization, returning a tuple of input, target, and embedding for each sample.\n\nAudioToTargetWithReferenceDataset: dataset class that reads a manifest describing input, target, and optional reference audio files, loads them at a specified sample rate, and returns synchronized or independent audio segments. Supports configurable keys, optional fixed or random sub‑segment durations, channel selection, length filtering, maximum utterance limits, and optional normalization based on a chosen signal.\n\nBaseAudioDataset: generic dataset abstraction for audio data, managing a collection of examples and applying a configurable audio processor to each item.\n\nStandalone function: _audio_collate_fn: no usage info.\n\nThus the module provides dataset classes for ASR tasks where audio inputs and targets (and optionally reference audio or embeddings) are loaded, processed, and collated for training models. It includes an audio processor that handles loading, normalization, segmentation, synchronization, and packaging of audio signals and embeddings.\n\nBusiness intent: Provide flexible data loading pipelines for speech recognition models, supporting paired audio, reference audio, and embedding inputs, enabling training and evaluation with proper preprocessing.\n\nKeywords: audio dataset, speech recognition, ASR, audio preprocessing, manifest, embedding, reference audio, normalization, segmentation, PyTorch Dataset, collate function.\n\nNow produce JSON.\n\nNeed to ensure no function names verbatim? The constraint: \"Do NOT list function names verbatim.\" That refers to not listing function names in summary/keywords? Probably avoid naming specific functions like _audio_collate_fn. So we can just describe functionality.\n\nLet's craft summary: \"The module defines a set of dataset utilities for automatic speech recognition that load audio files (input, target, optional reference)", "keywords": [], "summary_hash": "0df93e24ca5a", "cached_at": "2026-02-08T11:09:05+00:00"}