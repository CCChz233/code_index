{"summary": "A configurable basic tokenizer for RoCBert that cleans input text, optionally lowercases and strips accents, splits on punctuation, tokenizes Chinese characters, and respects a set of tokens that must remain intact.", "business_intent": "Prepare raw textual data into token sequences suitable for RoCBert models, enabling downstream natural language processing applications.", "keywords": ["tokenization", "text preprocessing", "lowercasing", "punctuation splitting", "accent stripping", "Chinese character handling", "never-split tokens", "RoCBert", "NLP", "basic tokenizer"], "summary_hash": "1bac21dd0749", "cached_at": "2026-02-09T11:09:20+00:00"}