{"summary": "The module defines an evaluator that measures how well a sentence embedding model captures semantic similarity by computing pairwise similarity scores (cosine, Euclidean, Manhattan) for sentence pairs and comparing these scores against reference similarity labels using Pearson and Spearman correlation coefficients.", "business_intent": "Provide a standardized metric to assess and compare the quality of sentence embedding models based on their alignment with humanâ€‘annotated similarity judgments.", "keywords": ["sentence embeddings", "similarity evaluation", "Pearson correlation", "Spearman correlation", "cosine similarity", "Euclidean distance", "Manhattan distance", "model benchmarking", "semantic similarity", "embedding quality"], "summary_hash": "08311ab6f0dd", "cached_at": "2026-02-08T13:56:00+00:00"}