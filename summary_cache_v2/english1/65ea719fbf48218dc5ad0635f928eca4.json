{"summary": "TensorFlow layer implementing RoFormer attention with rotary position embeddings, handling weight initialization, forward computation, and optional head pruning.", "business_intent": "Provide a ready‑to‑use attention component for transformer models that supports rotary positional encoding and head pruning to improve performance and enable model compression.", "keywords": ["TensorFlow", "attention", "RoFormer", "rotary position embedding", "transformer", "head pruning", "neural network layer"], "summary_hash": "c386aa408b53", "cached_at": "2026-02-09T09:15:04+00:00"}