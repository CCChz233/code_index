{"summary": "Implements a gated linear unit activation that applies a linear transformation followed by GELU gating, serving as a modular component in neural network layers.", "business_intent": "Provide a reusable activation module for deep learning models, particularly transformer-based architectures, to enhance model expressiveness and training stability.", "keywords": ["activation", "gated linear unit", "GELU", "neural network", "deep learning", "transformer", "module", "forward pass"], "summary_hash": "22c38a75c506", "cached_at": "2026-02-08T09:01:37+00:00"}