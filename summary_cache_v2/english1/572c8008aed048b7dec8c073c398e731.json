{"summary": "Implements the encoder component of the Informer architecture, stacking a configurable number of self‑attention layers together with distillation layers that progressively compress the sequence length.", "business_intent": "Provides an efficient way to encode long sequential inputs for tasks such as time‑series forecasting, reducing computational load while preserving essential temporal patterns.", "keywords": ["Informer", "encoder", "self-attention", "distillation", "sequence compression", "time series forecasting", "deep learning", "transformer", "configurable layers"], "summary_hash": "f74adcbfb97e", "cached_at": "2026-02-09T10:38:53+00:00"}