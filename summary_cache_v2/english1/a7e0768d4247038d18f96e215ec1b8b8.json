{"summary": "Implements a dataset class that prepares tokenized text examples for prompt‑tuning frozen GPT models. It loads raw data, tokenizes, inserts virtual prompt placeholders, enforces length limits, optionally adds BOS/EOS tokens, pads sequences, creates loss masks, and provides separate collate functions for training and inference within the Megatron‑based NLP framework.", "business_intent": "Enable efficient prompt‑learning workflows for large language models by supplying ready‑to‑train and inference‑ready batches, reducing engineering effort for developers building GPT‑based applications.", "keywords": ["GPT", "prompt learning", "virtual prompt", "dataset", "tokenization", "padding", "loss mask", "collate", "Megatron", "NLP", "language modeling", "training", "inference"], "summary_hash": "db3342e6f8e8", "cached_at": "2026-02-08T11:30:03+00:00"}