{"summary": "A lightweight container that holds the default hyperparameter values for a stochastic gradient descent (SGD) optimizer, such as learning rate, momentum, weight decay, and related settings.", "business_intent": "Provide a ready‑to‑use set of SGD optimizer parameters for training neural networks, simplifying configuration and ensuring consistent default behavior across experiments.", "keywords": ["SGD", "optimizer", "hyperparameters", "learning rate", "momentum", "weight decay", "PyTorch", "default configuration"], "summary_hash": "f0748efdf193", "cached_at": "2026-02-08T10:15:31+00:00"}