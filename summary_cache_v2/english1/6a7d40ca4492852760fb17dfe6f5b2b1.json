{"summary": "This module defines a mixin that equips neural network components with Perturbed Attention Guidance (PAG) capabilities. It prepares, scales, and injects custom attention processors into selected layers, enabling dynamic manipulation of attention scores during inference.", "business_intent": "To provide a flexible mechanism for controlling attention behavior in diffusion models, allowing developers to steer image generation outcomes through guided perturbations of the attention mechanism.", "keywords": ["Perturbed Attention Guidance", "attention processor", "mixins", "diffusion models", "torch", "neural network modification", "layer scaling", "custom attention"], "summary_hash": "b4000527c591", "cached_at": "2026-02-09T05:17:08+00:00"}