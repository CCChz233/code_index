{"summary": "A configuration container that encapsulates all architectural and training hyperparameters for a Bloom transformer model, offering defaults that mirror the reference implementation while allowing customization of vocabulary size, hidden dimensions, layer count, attention heads, normalization epsilon, dropout rates, residual connection handling, caching behavior, and experimental parallelism or exact‑attention options.", "business_intent": "Enable developers to reliably create and fine‑tune Bloom models with a consistent, reproducible set of parameters, providing control over model structure and runtime features such as caching and tensor parallelism.", "keywords": ["Bloom", "configuration", "transformer", "hyperparameters", "vocab size", "hidden size", "layers", "attention heads", "dropout", "layer normalization", "residual connection", "cache", "tensor parallelism", "exact attention", "PretrainedConfig"], "summary_hash": "d6201df0a53c", "cached_at": "2026-02-09T11:31:52+00:00"}