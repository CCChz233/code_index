{"summary": "Encapsulates the state and operations required by an MPI worker to execute TensorRT‑LLM inference, managing model loading, GPU resources, and communication with the coordinating process.", "business_intent": "Provide a scalable, high‑performance worker context that enables distributed large language model inference using TensorRT acceleration across multiple nodes.", "keywords": ["MPI", "worker", "context", "TensorRT", "LLM", "inference", "distributed", "GPU", "model loading", "communication"], "summary_hash": "8d158343743a", "cached_at": "2026-02-08T10:12:45+00:00"}