{"summary": "Implements the decoder component of a Transformer architecture for speech recognition, offering multi‑layer decoding with masked self‑attention, encoder‑decoder attention, position‑wise feed‑forward processing, layer normalization, dropout, and cached memory for efficient incremental inference.", "business_intent": "Enable generation of token sequences from encoded speech features in ASR models, supporting both training and low‑latency inference through cached attention states.", "keywords": ["transformer", "decoder", "self-attention", "cross-attention", "feed-forward", "layer normalization", "dropout", "caching", "incremental decoding", "ASR", "speech recognition", "PyTorch"], "summary_hash": "4a43f7ff4254", "cached_at": "2026-02-08T11:17:40+00:00"}