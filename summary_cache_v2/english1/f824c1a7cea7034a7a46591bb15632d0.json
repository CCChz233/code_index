{"summary": "Provides a comprehensive strategy for training models on multiple XLA devices using fully sharded data parallelism. It configures the FSDP wrapper, manages module placement on devices, handles distributed communication primitives, supports activation checkpointing, and offers flexible checkpoint saving options (full, sharded, sequential).", "business_intent": "Enable efficient large‑scale model training on TPUs by reducing memory consumption and simplifying the orchestration of XLA distributed operations, allowing developers to focus on model development rather than low‑level device management.", "keywords": ["XLA", "TPU", "Fully Sharded Data Parallel", "distributed training", "memory sharding", "activation checkpointing", "auto wrap policy", "checkpointing", "sharded checkpoint", "sequential save", "optimizer step", "precision handling", "communication primitives"], "summary_hash": "a8834ee26085", "cached_at": "2026-02-08T08:25:12+00:00"}