{"summary": "A structured container that encapsulates all tensors produced by a sequence-to-sequence language model, including loss, prediction scores, cached attention keys/values, hidden states, attention maps, and Mixture-of-Experts routing logits for both encoder and decoder.", "business_intent": "Enable downstream applications such as translation, summarization, or any text generation workflow to retrieve model predictions, perform efficient incremental decoding, and compute auxiliary expert losses, thereby supporting training, inference, and analysis pipelines.", "keywords": ["sequence-to-sequence", "language model output", "loss", "logits", "past key values", "hidden states", "attention weights", "router logits", "Mixture of Experts", "encoder", "decoder", "caching", "text generation"], "summary_hash": "31a23537f65e", "cached_at": "2026-02-09T06:29:06+00:00"}