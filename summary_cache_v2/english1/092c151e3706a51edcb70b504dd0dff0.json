{"summary": "Defines a Transformer‑based left‑to‑right language model that encapsulates tokenization, dataset handling, loss computation, metric tracking, and the full training/validation/testing workflow for sequence prediction tasks.", "business_intent": "Offers a plug‑and‑play neural language model that can be fine‑tuned on custom text corpora to generate or predict next tokens, supporting research, development, and production of NLP applications.", "keywords": ["Transformer", "language model", "left-to-right", "tokenization", "dataset", "training", "validation", "testing", "cross‑entropy loss", "perplexity", "PyTorch Lightning", "NeMo", "NLP", "sequence prediction"], "summary_hash": "c05084749361", "cached_at": "2026-02-08T11:35:19+00:00"}