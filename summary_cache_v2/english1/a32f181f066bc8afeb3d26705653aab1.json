{"summary": "A scheduler that wraps an optimizer and raises its learning rate exponentially from the current value to a target final value over a predefined number of iterations.", "business_intent": "Enable a smooth exponential warmâ€‘up of the learning rate during model training to improve convergence and stability.", "keywords": ["learning rate scheduler", "exponential increase", "optimizer wrapper", "iteration based", "warm-up", "training hyperparameter"], "summary_hash": "4edca2d9791f", "cached_at": "2026-02-08T08:16:35+00:00"}