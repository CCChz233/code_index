{"summary": "Implements a multi-head attention layer that processes query, key, and value tensors and supports pruning of individual attention heads to reduce computational load.", "business_intent": "Provide a configurable attention component for transformer-based models, enabling efficient inference and model size reduction through head pruning.", "keywords": ["attention", "multi-head", "transformer", "pruning", "neural network", "layer", "model compression"], "summary_hash": "0d088256e662", "cached_at": "2026-02-09T10:18:01+00:00"}