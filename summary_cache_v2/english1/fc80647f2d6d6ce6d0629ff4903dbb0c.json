{"summary": "Implements a Graphormer layer that applies dense multi‑head self‑attention to node feature tensors, optionally incorporating additive or multiplicative attention bias, followed by a feed‑forward network with configurable activation, dropout and layer‑normalization ordering.", "business_intent": "Provides a reusable building block for graph neural networks that captures long‑range dependencies and structural information through transformer‑style attention, facilitating improved graph representation learning in downstream tasks.", "keywords": ["graph transformer", "multi-head attention", "attention bias", "layer normalization", "dropout", "feed-forward network", "graph neural network", "representation learning"], "summary_hash": "2a639e484036", "cached_at": "2026-02-08T23:51:58+00:00"}