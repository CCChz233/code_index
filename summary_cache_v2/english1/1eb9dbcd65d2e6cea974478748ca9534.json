{"summary": "Provides a thin wrapper that links a DeepSpeed LambdaLR scheduler with one or more PyTorch optimizers, offering a simple step operation to advance the learning‑rate schedule during training.", "business_intent": "Enable seamless learning‑rate scheduling within DeepSpeed‑accelerated training pipelines.", "keywords": ["DeepSpeed", "learning rate scheduler", "optimizer integration", "PyTorch", "training loop", "wrapper", "distributed training"], "summary_hash": "ea7ff2a15ff6", "cached_at": "2026-02-09T02:10:37+00:00"}