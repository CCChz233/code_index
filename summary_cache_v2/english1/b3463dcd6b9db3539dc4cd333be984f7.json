{"summary": "Implements a multimodal encoder-decoder model that consumes video (visual) signals and generates textual transcriptions using Connectionist Temporal Classification (CTC). The model integrates NeMo ASR components for loss computation, decoding, and evaluation, and supports training, validation, inference, and export for deployment.", "business_intent": "Provide a ready‑to‑use visual speech recognition solution for applications such as lip‑reading, silent speech interfaces, video captioning, and accessibility tools that require transcription of spoken content from video streams.", "keywords": ["visual speech recognition", "CTC", "multimodal ASR", "lip reading", "video-to-text", "NeMo", "PyTorch Lightning", "exportable model", "speech‑computer vision"], "summary_hash": "f6fa937f7762", "cached_at": "2026-02-08T10:59:43+00:00"}