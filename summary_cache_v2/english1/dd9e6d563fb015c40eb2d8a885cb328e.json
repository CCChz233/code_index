{"summary": "Implements a ReLU activation whose backward pass is modified to propagate only positive gradients, enabling guided backpropagation for visualizing neural network decisions.", "business_intent": "Provide an activation layer that supports guided backpropagation, allowing developers to generate interpretable saliency maps and other explanation artifacts for deep learning models.", "keywords": ["ReLU", "guided backpropagation", "activation function", "gradient modification", "neural network visualization", "model interpretability", "deep learning"], "summary_hash": "03b1fe616b49", "cached_at": "2026-02-08T11:15:32+00:00"}