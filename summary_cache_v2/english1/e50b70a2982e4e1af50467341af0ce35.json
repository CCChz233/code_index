{"summary": "Implements the Gaussian Error Linear Unit (GeLU) activation, offering a forward operation that applies this non‑linear transformation to input data.", "business_intent": "Provide a ready‑to‑use GeLU activation for deep learning models to enhance their expressive power and performance.", "keywords": ["GeLU", "activation function", "neural networks", "deep learning", "non-linear transformation", "forward pass", "machine learning"], "summary_hash": "7439eedc57db", "cached_at": "2026-02-09T09:27:50+00:00"}