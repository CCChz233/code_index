{"summary": "Implements a single decoder layer of a Transformer model, performing masked self‑attention, encoder‑decoder attention, and a position‑wise feed‑forward network with configurable hidden size, inner dimension, number of heads, dropout rates, and activation function, and supports both pre‑ and post‑layer‑normalization configurations.", "business_intent": "Provides a reusable component for building autoregressive language generation, machine translation, and other sequence‑to‑sequence models that rely on Transformer decoder architectures.", "keywords": ["Transformer", "decoder block", "self-attention", "cross-attention", "multi-head attention", "feed-forward network", "layer normalization", "dropout", "hidden size", "activation function", "NLP", "sequence modeling"], "summary_hash": "3b936b107d65", "cached_at": "2026-02-08T09:46:31+00:00"}