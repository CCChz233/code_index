{"summary": "Implements rotary positional embeddings for the Qwen2 model, handling cosine/sine cache creation and applying the embeddings to input tensors during the forward pass.", "business_intent": "Provide efficient relative position information to improve the model's language understanding and generation performance.", "keywords": ["rotary embedding", "positional encoding", "cosine sine cache", "transformer", "Qwen2", "forward pass", "neural network", "embedding"], "summary_hash": "73f6c0d26e21", "cached_at": "2026-02-09T08:11:34+00:00"}