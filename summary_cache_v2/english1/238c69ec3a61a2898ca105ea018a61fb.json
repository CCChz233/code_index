{"summary": "Implements a core transformer block that applies self‑attention followed by a feed‑forward network to transform input sequences.", "business_intent": "Provides a reusable component for constructing transformer‑based models used in natural language processing, computer vision, or other sequence modeling applications.", "keywords": ["transformer", "self-attention", "feed-forward", "neural network", "sequence modeling", "deep learning"], "summary_hash": "604e1e62061b", "cached_at": "2026-02-08T08:51:13+00:00"}