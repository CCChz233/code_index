{"summary": "We need to produce JSON with summary, business_intent, keywords list.\n\nWe have two files: spark.py and spark_source.py. Need to summarize module responsibilities: Spark offline store for Feast, handling storage/retrieval, batch writes, point-in-time joins, retrieval job, conversion to Arrow/Spark DataFrames, on-demand feature views, persisting/exporting. Also defines Spark-specific data source and saved-dataset storage classes, config options, conversion between internal objects and protobuf, mapping Spark types to Feast value types.\n\nBusiness intent: Provide offline feature store capabilities using Spark for Feast, enabling efficient historical feature retrieval and storage, supporting various data formats and integration with Feast's data model.\n\nKeywords: Spark, offline store, Feast, historical features, batch write, point-in-time join, retrieval job, Arrow, DataFrame, on-demand feature view, data source, saved dataset, file format, SQL query, table name, protobuf conversion, type mapping.\n\nNow produce JSON.\n\nNeed to ensure no function names verbatim. That's fine.\n\nLet's craft summary: \"The module implements a Spark‑based offline store for Feast, offering mechanisms to write and read historical feature data, execute point‑in‑time joins, and run retrieval jobs that output results as Arrow or Spark DataFrames. It also defines Spark‑specific source and saved‑dataset classes that encapsulate configuration such as file format, location, SQL query, and table name, and handle conversion between Feast objects and protobuf messages as well as type mapping between Spark and Feast.\"\n\nBusiness intent: \"Enable Feast users to leverage Apache Spark for offline feature engineering and serving, providing scalable storage, retrieval, and transformation of feature data within Spark environments.\"\n\nKeywords list.\n\nLet's output JSON. Ensure proper quoting and array.\n\nProceed.\n{\n  \"summary\": \"The module provides a Spark‑based offline store for Feast, handling the storage and retrieval of historical feature data, batch writes, point‑in‑time joins, and execution of retrieval jobs that can return results as Arrow or Spark DataFrames. It also defines Spark‑specific source and saved‑dataset classes that encapsulate configuration options (file format, location, SQL query, table name), perform conversions between Feast internal objects and protobuf messages, and map Spark data types to Feast value types.\",\n  \"business_intent\": \"Allow Feast users to utilize Apache Spark for offline feature engineering and serving, delivering scalable, configurable mechanisms for persisting, querying, and joining feature data within Spark environments.\",\n  \"keywords\": [\n    \"Spark\",\n    \"Feast\",", "business_intent": "We need to produce JSON with summary, business_intent, keywords list.\n\nWe have two files: spark.py and spark_source.py. Need to summarize module responsibilities: Spark offline store for Feast, handling storage/retrieval, batch writes, point-in-time joins, retrieval job, conversion to Arrow/Spark DataFrames, on-demand feature views, persisting/exporting. Also defines Spark-specific data source and saved-dataset storage classes, config options, conversion between internal objects and protobuf, mapping Spark types to Feast value types.\n\nBusiness intent: Provide offline feature store capabilities using Spark for Feast, enabling efficient historical feature retrieval and storage, supporting various data formats and integration with Feast's data model.\n\nKeywords: Spark, offline store, Feast, historical features, batch write, point-in-time join, retrieval job, Arrow, DataFrame, on-demand feature view, data source, saved dataset, file format, SQL query, table name, protobuf conversion, type mapping.\n\nNow produce JSON.\n\nNeed to ensure no function names verbatim. That's fine.\n\nLet's craft summary: \"The module implements a Spark‑based offline store for Feast, offering mechanisms to write and read historical feature data, execute point‑in‑time joins, and run retrieval jobs that output results as Arrow or Spark DataFrames. It also defines Spark‑specific source and saved‑dataset classes that encapsulate configuration such as file format, location, SQL query, and table name, and handle conversion between Feast objects and protobuf messages as well as type mapping between Spark and Feast.\"\n\nBusiness intent: \"Enable Feast users to leverage Apache Spark for offline feature engineering and serving, providing scalable storage, retrieval, and transformation of feature data within Spark environments.\"\n\nKeywords list.\n\nLet's output JSON. Ensure proper quoting and array.\n\nProceed.\n{\n  \"summary\": \"The module provides a Spark‑based offline store for Feast, handling the storage and retrieval of historical feature data, batch writes, point‑in‑time joins, and execution of retrieval jobs that can return results as Arrow or Spark DataFrames. It also defines Spark‑specific source and saved‑dataset classes that encapsulate configuration options (file format, location, SQL query, table name), perform conversions between Feast internal objects and protobuf messages, and map Spark data types to Feast value types.\",\n  \"business_intent\": \"Allow Feast users to utilize Apache Spark for offline feature engineering and serving, delivering scalable, configurable mechanisms for persisting, querying, and joining feature data within Spark environments.\",\n  \"keywords\": [\n    \"Spark\",\n    \"Feast\",", "keywords": [], "summary_hash": "ed8d49728207", "cached_at": "2026-02-09T00:38:51+00:00"}