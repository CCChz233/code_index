{"summary": "The module defines a Lightning callback that automatically applies stochastic weight averaging (SWA) during model training. It controls when averaging starts, updates the averaged parameters each epoch, synchronizes batchâ€‘norm statistics, and offers utilities to save, load, and transfer the SWA model state.", "business_intent": "Enable users to improve model generalization and stability by seamlessly integrating SWA into their PyTorch Lightning training pipelines without manual bookkeeping.", "keywords": ["stochastic weight averaging", "SWA", "PyTorch Lightning", "callback", "model averaging", "batch normalization", "optimizer", "learning rate scheduler", "checkpointing", "training stability"], "summary_hash": "e9128e4a1b99", "cached_at": "2026-02-08T08:55:53+00:00"}