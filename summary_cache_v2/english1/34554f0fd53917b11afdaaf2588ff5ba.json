{"summary": "Provides a plugin that enables FP8 mixed‑precision training using NVIDIA's Transformer Engine, handling weight and compute dtypes, delayed‑scaling configuration, automatic substitution of linear and layer‑norm layers, and offering utilities for input/output conversion and context management during model initialization and forward passes.", "business_intent": "Accelerate deep‑learning model training by leveraging FP8 precision through Transformer Engine, reducing memory usage and increasing throughput while abstracting the complexity of dtype handling and layer replacement.", "keywords": ["FP8", "Transformer Engine", "mixed precision", "training plugin", "delayed scaling", "layer replacement", "compute dtype", "NVIDIA", "performance optimization", "deep learning"], "summary_hash": "502f244b8f00", "cached_at": "2026-02-08T08:30:21+00:00"}