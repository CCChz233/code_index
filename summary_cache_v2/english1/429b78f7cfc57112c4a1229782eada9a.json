{"summary": "Defines a neural machine translation model that incorporates a bottleneck latent variable between the encoder and decoder. The model can be trained with standard cross‑entropy, variational auto‑encoder, or masked‑image‑modeling losses, and includes methods for encoding source text, compressing to a latent space, decoding to target text, batch inference, loss calculation, and training/evaluation loops. Also provides a small utility to create either a linear projection layer or an identity mapping as needed.", "business_intent": "Offer a versatile, high‑performance translation component for the NeMo ecosystem that enables research and production use‑cases requiring latent‑space compression, multi‑objective training, and efficient inference, thereby accelerating development of advanced NMT systems.", "keywords": ["neural machine translation", "bottleneck latent variable", "encoder‑decoder", "cross‑entropy loss", "variational auto‑encoder", "masked image modeling", "batch inference", "loss computation", "training loop", "PyTorch Lightning", "NeMo framework"], "summary_hash": "0b3e557b2fb3", "cached_at": "2026-02-08T11:34:04+00:00"}