{"summary": "Provides CPU-based all-to-all communication utilities that support distributed sparse optimizer operations in DGL's PyTorch backend.", "business_intent": "Facilitates scalable, highâ€‘performance training of graph neural networks by enabling efficient data exchange across multiple workers for sparse parameter updates.", "keywords": ["distributed", "optimizer", "sparse", "all-to-all", "communication", "CPU", "PyTorch", "DGL", "parallel training", "graph neural networks"], "summary_hash": "d37652827d6b", "cached_at": "2026-02-09T01:00:29+00:00"}