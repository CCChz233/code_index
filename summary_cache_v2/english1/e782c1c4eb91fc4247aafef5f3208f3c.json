{"summary": "Implements the self‑attention mechanism used in the Longformer architecture, efficiently computing local (sliding‑window) and optional global attention for very long input sequences.", "business_intent": "Provides a scalable attention layer that enables natural‑language‑processing models to handle lengthy documents and other long‑range data, supporting tasks such as classification, summarization, and information retrieval.", "keywords": ["Longformer", "self‑attention", "sliding window", "global attention", "efficient transformer", "long sequences", "NLP", "deep learning"], "summary_hash": "e0de3431fb56", "cached_at": "2026-02-09T07:10:05+00:00"}