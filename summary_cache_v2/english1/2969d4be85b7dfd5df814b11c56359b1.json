{"summary": "A configurable encoder block for a UNet that applies a series of residual convolutions, optional downsampling, and cross‑attention transformer layers to transform input feature maps while reducing spatial resolution.", "business_intent": "Enable flexible, memory‑efficient conditioning of generative or diffusion models by providing a modular downsampling component that integrates external context through cross‑attention during the encoding stage.", "keywords": ["UNet", "downsampling", "residual block", "cross-attention", "transformer", "attention heads", "timestep embedding", "group normalization", "flash attention", "configurable encoder"], "summary_hash": "fbc8aa9cf83f", "cached_at": "2026-02-08T11:38:55+00:00"}