{"summary": "The module showcases how to compress a SentenceTransformer model using integer quantization, export the quantized model in ONNX and OpenVINO formats, and assess the impact on inference speed, model size, and embedding quality.", "business_intent": "Enable developers to deploy smaller, faster sentence embedding models on CPUâ€‘only environments by applying quantization techniques, thereby reducing infrastructure costs and improving response times for NLP applications.", "keywords": ["quantization", "model compression", "CPU inference", "SentenceTransformer", "ONNX", "OpenVINO", "embedding evaluation", "performance speedup", "model size reduction"], "summary_hash": "5e6ea0fe2ba3", "cached_at": "2026-02-08T13:58:17+00:00"}