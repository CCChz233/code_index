{"summary": "Provides a configurable transformer layer that performs multi‑head attention (including optional cross‑attention), layer normalization, and a feed‑forward network with selectable activation and dropout, serving as a core building block in the Allegro model.", "business_intent": "Facilitate the construction of deep, flexible transformer architectures for generative AI tasks within the Allegro framework.", "keywords": ["transformer", "multi-head attention", "cross-attention", "layer normalization", "feed-forward", "dropout", "activation function", "neural network block", "Allegro model"], "summary_hash": "09150a16e162", "cached_at": "2026-02-09T04:39:02+00:00"}