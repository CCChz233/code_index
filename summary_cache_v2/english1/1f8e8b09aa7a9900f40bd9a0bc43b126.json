{"summary": "This module implements a hard attention Graph Attention Network (HardGAT) using DGL and PyTorch. It defines a custom gated attention layer that performs hard top‑k edge selection and a full GAT model that produces node embeddings, optimized with Adam.", "business_intent": "Provide a ready‑to‑use reference implementation for researchers and engineers to apply hard‑attention graph neural networks to citation graph datasets (Cora, Citeseer, Pubmed) for tasks such as node classification, link prediction, and representation learning.", "keywords": ["graph neural network", "hard attention", "GAT", "DGL", "PyTorch", "node embeddings", "Adam optimizer", "top‑k edge selection", "citation graph", "representation learning"], "summary_hash": "896a01ceeb07", "cached_at": "2026-02-09T00:53:42+00:00"}