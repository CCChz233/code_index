{"summary": "Provides an invariant point attention layer that integrates sequence embeddings with 3D coordinate information in a rotation‑ and translation‑invariant manner, following Algorithm 22 to generate attention‑weighted representations for protein structure modeling.", "business_intent": "Enhance protein folding and structural prediction pipelines by delivering a geometry‑aware attention mechanism that improves accuracy and robustness for biotech and pharmaceutical research.", "keywords": ["invariant point attention", "protein folding", "geometric attention", "rotation invariance", "deep learning", "ESMFold", "algorithm 22", "structural biology", "neural network layer"], "summary_hash": "cc35651f9120", "cached_at": "2026-02-09T09:49:41+00:00"}