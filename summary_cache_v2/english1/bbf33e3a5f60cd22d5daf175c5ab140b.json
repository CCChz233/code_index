{"summary": "Provides a Lightning training strategy that integrates PyTorch's Fully Sharded Data Parallel (FSDP) into the Lightning ecosystem. It configures sharding policies, mixed‑precision handling, device and process‑group setup, activation checkpointing, and manages checkpoint and optimizer state serialization to enable memory‑efficient, large‑scale distributed training.", "business_intent": "Allow developers to train very large neural networks across multiple GPUs or nodes with reduced memory consumption and seamless checkpointing, leveraging Lightning's high‑level API while benefiting from FSDP's scalability and performance.", "keywords": ["FSDP", "Fully Sharded Data Parallel", "Lightning", "distributed training", "model sharding", "mixed precision", "checkpointing", "optimizer state", "activation checkpointing", "process groups", "memory efficiency", "large‑scale training"], "summary_hash": "62b9a92d96db", "cached_at": "2026-02-08T08:52:42+00:00"}