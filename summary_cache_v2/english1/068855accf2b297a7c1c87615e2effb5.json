{"summary": "Implements batched greedy inference for Recurrent Neural Network Transducer (RNNT) models using ONNX runtime, handling encoder and decoder joint operations, initial state preparation, and blank token indexing.", "business_intent": "Provide a high‑throughput, low‑latency speech‑to‑text inference engine for production environments by leveraging ONNX‑optimized RNNT models with efficient batched greedy decoding.", "keywords": ["ONNX", "RNNT", "greedy decoding", "batched inference", "speech recognition", "encoder", "decoder", "joint network", "state management", "blank token"], "summary_hash": "324a7ac6997f", "cached_at": "2026-02-08T09:31:10+00:00"}