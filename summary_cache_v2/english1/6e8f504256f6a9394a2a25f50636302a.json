{"summary": "Implements the self‑attention layer used in Vision Transformer masked autoencoders, handling projection of queries, keys and values, scaling, dropout and output aggregation, while also providing utilities to remove unnecessary attention heads.", "business_intent": "Provides the core attention computation for computer‑vision transformer models and enables model size reduction and faster inference through head pruning.", "keywords": ["vision transformer", "masked autoencoder", "self‑attention", "head pruning", "neural network layer", "deep learning", "model compression", "inference"], "summary_hash": "0293df1eefc7", "cached_at": "2026-02-09T11:42:52+00:00"}