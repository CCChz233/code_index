{"summary": "This module provides a mixin that equips diffusion models with full LoRA (Low‑Rank Adaptation) adapter support. It implements loading, saving, activation, deactivation, fusion, scaling, device placement, and state‑dict extraction for LoRA weights, along with utilities for processing weight files and configuring adapters for text encoders.", "business_intent": "Enable efficient fine‑tuning of diffusion models using parameter‑efficient LoRA adapters, allowing users to load, merge, and manage low‑rank weight modifications without altering the original model architecture.", "keywords": ["LoRA", "adapter", "diffusion model", "parameter-efficient fine-tuning", "weight fusion", "state dict", "PyTorch", "PEFT", "transformers", "text encoder"], "summary_hash": "68519d8c024e", "cached_at": "2026-02-09T05:11:05+00:00"}