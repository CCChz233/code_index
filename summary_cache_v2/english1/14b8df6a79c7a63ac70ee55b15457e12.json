{"summary": "Encapsulates the pre‑training output layers for a BigBird transformer model, offering a forward helper that transforms hidden states into task‑specific predictions such as masked language modeling.", "business_intent": "Supports the training and fine‑tuning of large‑scale NLP models based on the BigBird architecture, enabling developers to build and pre‑train language models for downstream applications.", "keywords": ["BigBird", "pretraining", "heads", "transformer", "masked language modeling", "neural network", "forward pass", "PyTorch", "NLP", "language model"], "summary_hash": "25516f562cd8", "cached_at": "2026-02-09T08:47:35+00:00"}