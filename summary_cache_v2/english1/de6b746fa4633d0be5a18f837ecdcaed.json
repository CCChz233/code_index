{"summary": "A data structure that bundles the outputs of an Electra pre‑training model, including token prediction scores and optionally the hidden representations and attention matrices from each transformer layer.", "business_intent": "To provide a standardized container for downstream components to access logits for loss calculation, inspect hidden states for analysis, or retrieve attention patterns for debugging and further processing in Electra pre‑training pipelines.", "keywords": ["logits", "hidden_states", "attentions", "Electra", "pretraining", "model output", "Flax", "JAX", "sequence length", "vocab size", "attention weights"], "summary_hash": "b6ee2b432411", "cached_at": "2026-02-09T08:21:31+00:00"}