{"summary": "Implements the output head for masked language modeling in a RoBERTa‑based Luke model, handling weight tying with the token embeddings and generating logits for masked token prediction.", "business_intent": "Provide a reusable component that enables fine‑tuning and inference of the Luke model on masked language modeling tasks, supporting downstream NLP applications such as pre‑training, domain adaptation, and token‑level prediction.", "keywords": ["masked language modeling", "head", "RoBERTa", "Luke", "weight tying", "forward pass", "logits", "token prediction", "NLP", "transformer"], "summary_hash": "af8be0815f89", "cached_at": "2026-02-09T10:45:26+00:00"}