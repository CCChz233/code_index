{"summary": "A plugin that customizes mixed‑precision autocasting for FSDP training with Megatron‑core, restricting the autocast scope to the forward‑output‑and‑loss callable rather than the entire training/validation/test step.", "business_intent": "Provide correct mixed‑precision handling for large‑scale distributed training, ensuring backward operations are not inadvertently autocast, thereby improving stability and performance.", "keywords": ["mixed precision", "autocast", "FSDP", "Megatron", "distributed training", "forward context", "backward", "plugin", "PyTorch Lightning"], "summary_hash": "dd2591c23cc0", "cached_at": "2026-02-08T09:41:58+00:00"}