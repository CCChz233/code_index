{"summary": "Implements a NEZHA-based masked language model, adding a prediction head to the transformer and handling input preparation and embedding management for token prediction tasks.", "business_intent": "Enable applications that require masked token inference such as text completion, fill‑in‑the‑blank, and downstream NLP fine‑tuning using a Chinese‑optimized language model.", "keywords": ["NEZHA", "masked language modeling", "transformer", "output embeddings", "text generation", "NLP", "language model", "token prediction"], "summary_hash": "82e3e157c0ab", "cached_at": "2026-02-09T08:16:07+00:00"}