{"summary": "Generates a block‑diagonal causal attention mask that limits each query to keys within its own block, excludes padded regions, respects causal ordering, and enforces a configurable local window around the block’s end.", "business_intent": "Enable high‑performance transformer attention kernels to apply precise masking for padded, block‑structured sequences while supporting causal and window‑restricted local attention.", "keywords": ["attention mask", "block diagonal", "causal", "local window", "padded keys", "transformer", "fmha", "bias", "sequence lengths"], "summary_hash": "bc9786faec70", "cached_at": "2026-02-08T23:23:22+00:00"}