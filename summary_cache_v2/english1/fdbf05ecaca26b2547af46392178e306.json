{"summary": "Provides a dataset implementation and its configuration for fill‑in‑the‑middle (FIM) training of GPT models within the Megatron‑Core framework. The dataset wraps an indexed collection of documents, supports configurable sampling, split allocation, sequence permutation and shuffling of document indices to generate training examples suitable for FIM objectives.", "business_intent": "Enable scalable and flexible preparation of training data for large‑scale GPT language models that use the fill‑in‑the‑middle objective, facilitating efficient data handling and reproducibility in Megatron‑Core pipelines.", "keywords": ["GPT", "fill-in-the-middle", "FIM", "Megatron", "dataset", "language modeling", "sampling", "configuration", "NLP", "data loading", "document indexing"], "summary_hash": "b46fe99a0d3c", "cached_at": "2026-02-08T11:30:54+00:00"}