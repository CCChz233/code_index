{"summary": "A pretrained multilingual sequence‑to‑sequence model that uses a mixture‑of‑experts architecture to generate target text conditioned on source input.", "business_intent": "Enable high‑quality, scalable translation and other conditional text generation tasks across many languages for applications such as multilingual communication, content localization, and cross‑language AI services.", "keywords": ["multilingual", "translation", "conditional generation", "Mixture of Experts", "seq2seq", "transformer", "pretrained model", "inference", "language coverage", "NLLB"], "summary_hash": "6c872e8f805a", "cached_at": "2026-02-09T07:16:11+00:00"}