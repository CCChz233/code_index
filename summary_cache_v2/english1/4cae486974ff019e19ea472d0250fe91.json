{"summary": "Encapsulates a tensor quantized to 8‑bit integers along with its scaling factors, providing a compact representation of neural‑network weights for efficient computation.", "business_intent": "Enables low‑precision model deployment by reducing memory usage and computational load, allowing faster inference on edge or resource‑constrained devices.", "keywords": ["quantization", "int8", "tensor", "scales", "weight", "neural network", "inference acceleration", "model compression", "low‑precision arithmetic", "edge deployment"], "summary_hash": "5fd8229e0f18", "cached_at": "2026-02-09T11:48:46+00:00"}