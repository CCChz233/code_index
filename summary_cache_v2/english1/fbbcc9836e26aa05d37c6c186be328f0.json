{"summary": "The module supplies neural‑network loss components tailored for sequence modeling, offering cross‑entropy and negative log‑likelihood calculations that automatically mask padding tokens, support label‑smoothing regularization, and allow optional restriction to the most recent tokens with configurable reduction behavior.", "business_intent": "Enable more robust and stable training of sequence models such as language or speech recognizers by providing regularized loss functions that handle padding and smoothing, thereby improving convergence and generalization.", "keywords": ["cross entropy", "label smoothing", "sequence loss", "padding mask", "negative log likelihood", "token reduction", "NeMo", "PyTorch", "neural module"], "summary_hash": "e522629bf459", "cached_at": "2026-02-08T10:53:16+00:00"}