{"summary": "Implements a sparse multi‑layer perceptron used in Switch Transformer architectures, handling expert gating, conditional activation, and efficient forward computation of only the selected experts.", "business_intent": "Provide a scalable, compute‑efficient neural component that enables large language models to route inputs through a subset of specialized MLP experts, reducing inference and training costs while maintaining performance.", "keywords": ["sparse MLP", "Switch Transformer", "mixture of experts", "gating", "conditional computation", "efficient scaling", "neural network"], "summary_hash": "53f5ca78dc8b", "cached_at": "2026-02-09T07:27:03+00:00"}