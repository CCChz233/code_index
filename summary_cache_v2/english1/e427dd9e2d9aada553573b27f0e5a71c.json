{"summary": "Implements the IPEX backend for the benchmarking suite, handling model loading (both pretrained and weight‑less), input preparation, and execution of inference operations such as forward passes, text generation, and prefill, with optional data‑parallel distribution using Intel Extension for PyTorch.", "business_intent": "Enable fast, scalable inference on Intel CPUs by leveraging the Intel Extension for PyTorch, allowing the benchmark framework to evaluate model performance under realistic, high‑throughput scenarios.", "keywords": ["Intel Extension for PyTorch", "IPEX", "backend", "model loading", "inference", "generation", "prefill", "weightless model", "data parallel", "distributed inference", "benchmark"], "summary_hash": "613e1574a30d", "cached_at": "2026-02-09T02:30:05+00:00"}