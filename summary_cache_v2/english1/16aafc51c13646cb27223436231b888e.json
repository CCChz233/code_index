{"summary": "Implements a spatial transformer that operates on 2‑D image‑like tensors, supporting continuous latents, patched or vectorized discrete representations, with configurable multi‑head attention, depth, dropout, cross‑attention conditioning and adaptive normalization.", "business_intent": "Provide a versatile neural module for image generation and diffusion models, enabling efficient attention‑based processing of latent images and conditional guidance.", "keywords": ["2D transformer", "image attention", "multi-head", "cross-attention", "diffusion", "latent representation", "positional embedding", "adaptive layer norm", "gradient checkpointing", "vector embeddings"], "summary_hash": "68c0465c802b", "cached_at": "2026-02-09T04:38:19+00:00"}