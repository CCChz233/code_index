{"summary": "The module implements a metric that calculates BLEU scores to evaluate how closely generated responses match reference texts within single-turn interactions, integrating with the framework's callback and configuration systems.", "business_intent": "Provide an automated way to assess the quality and fidelity of generated language outputs in retrievalâ€‘augmented generation pipelines, enabling developers to benchmark and improve model performance.", "keywords": ["BLEU", "evaluation metric", "generated text quality", "reference comparison", "natural language generation", "RAG assessment", "single-turn sample", "model benchmarking", "language model evaluation"], "summary_hash": "5a4e6478636f", "cached_at": "2026-02-08T22:50:47+00:00"}