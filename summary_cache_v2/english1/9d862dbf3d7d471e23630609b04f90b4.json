{"summary": "Implements a single decoder layer for speech-to-text models, transforming encoder outputs and prior token embeddings into the next hidden representation through attention and feed‑forward mechanisms.", "business_intent": "Support end‑to‑end speech recognition by decoding acoustic representations into textual token sequences.", "keywords": ["decoder layer", "speech-to-text", "transformer", "attention", "feed‑forward", "neural network", "sequence modeling", "acoustic representation"], "summary_hash": "a70f650030e2", "cached_at": "2026-02-09T10:47:20+00:00"}