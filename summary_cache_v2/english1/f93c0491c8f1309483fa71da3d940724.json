{"summary": "A flexible transformer model based on RoBERTa with pre‑layer normalization that can operate as a pure encoder using self‑attention or as a decoder when configured, adding a cross‑attention layer between self‑attention blocks for seq2seq scenarios.", "business_intent": "Enable developers to integrate a versatile language model component that supports both encoding and decoding tasks, facilitating applications such as translation, summarization, and other natural language generation pipelines.", "keywords": ["transformer", "RoBERTa", "pre‑layer norm", "encoder", "decoder", "cross‑attention", "self‑attention", "seq2seq", "head pruning", "embeddings"], "summary_hash": "2e83043cbd86", "cached_at": "2026-02-09T09:10:21+00:00"}