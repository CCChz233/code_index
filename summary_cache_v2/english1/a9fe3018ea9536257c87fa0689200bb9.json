{"summary": "Implements a single encoder block of the Marian transformer architecture, transforming input token representations through self‑attention and position‑wise feed‑forward sub‑layers with layer normalization and dropout.", "business_intent": "Provides contextualized hidden states for source sequences in neural machine translation and other sequence‑to‑sequence models.", "keywords": ["Marian", "encoder layer", "transformer", "self-attention", "feed-forward network", "layer normalization", "dropout", "neural machine translation", "sequence encoding"], "summary_hash": "2d092badb5a0", "cached_at": "2026-02-09T11:28:18+00:00"}