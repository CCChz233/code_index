{"summary": "Encapsulates a MobileBERT transformer model configured for pre‑training tasks such as masked language modeling and next‑sentence prediction.", "business_intent": "Offer a lightweight, on‑device‑friendly BERT variant that can be pre‑trained or fine‑tuned for natural language processing applications where computational resources are limited.", "keywords": ["MobileBERT", "pretraining", "transformer", "masked language modeling", "next sentence prediction", "lightweight NLP", "on‑device inference", "deep learning"], "summary_hash": "8022a8a11a31", "cached_at": "2026-02-09T07:13:14+00:00"}