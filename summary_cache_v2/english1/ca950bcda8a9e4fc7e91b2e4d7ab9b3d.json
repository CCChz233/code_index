{"summary": "Implements the scaled dot‑product attention mechanism for the CogVideoX model, applying rotary positional embeddings to query and key tensors while omitting spatial normalization.", "business_intent": "Enable efficient and position‑aware attention computation within the CogVideoX video generation pipeline.", "keywords": ["scaled dot-product attention", "rotary embedding", "query", "key", "CogVideoX", "processor", "transformer", "video model", "no spatial normalization"], "summary_hash": "a4e06fcc16fc", "cached_at": "2026-02-09T04:06:04+00:00"}