{"summary": "Defines a configurable feed‑forward multi‑layer perceptron block used in transformer‑style architectures, applying two linear transformations with a non‑linear activation and optional dropout to token embeddings.", "business_intent": "Provides a reusable neural network component for MONAI models that require transformer‑based processing of medical imaging data, enabling flexible depth, activation, and regularization of token representations.", "keywords": ["feed-forward", "transformer", "multi-layer perceptron", "activation", "dropout", "linear projection", "token embedding", "MONAI", "PyTorch", "neural network block"], "summary_hash": "d320666d30b3", "cached_at": "2026-02-08T13:22:22+00:00"}