{"summary": "Implements the Gaussian Error Linear Unit (GELU) activation used in Bloom transformer models, exposing a callable interface that transforms input tensors.", "business_intent": "Enable fast and reliable GELU computation within Bloom-based language models for training and inference pipelines.", "keywords": ["GELU", "activation function", "Bloom model", "transformer", "neural network", "callable", "setup helper", "deep learning", "inference", "training"], "summary_hash": "36b86d547f0d", "cached_at": "2026-02-09T11:32:01+00:00"}