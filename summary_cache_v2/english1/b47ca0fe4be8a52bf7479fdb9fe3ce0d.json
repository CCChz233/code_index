{"summary": "A tokenizer class tailored for the DPR context encoder that mirrors the behavior of a BERT tokenizer, performing full tokenization including punctuation splitting and WordPiece subword segmentation.", "business_intent": "Enable preprocessing of passage text for dense passage retrieval models by converting raw strings into token IDs compatible with the DPR context encoder.", "keywords": ["tokenization", "DPR", "context encoder", "BERT tokenizer", "WordPiece", "punctuation splitting", "NLP preprocessing", "dense passage retrieval"], "summary_hash": "3db420a2b406", "cached_at": "2026-02-09T10:58:05+00:00"}