{"summary": "The module provides batch sampling utilities for Megatron-style language model pretraining in NeMo. It defines an abstract sampler and two concrete implementations: one that deterministically yields batches with explicit start and end token positions, and another that generates shuffled mini-batches for random sampling. Both handle dataset indexing, batch size calculations, and micro-batch considerations for distributed training.", "business_intent": "Facilitate efficient and flexible data loading for large-scale Megatron language model training, supporting both sequential and random batch generation to improve training throughput and model performance in distributed environments.", "keywords": ["Megatron", "batch sampler", "pretraining", "language modeling", "NeMo", "data loading", "iterator", "random shuffling", "micro-batch", "distributed training", "torch"], "summary_hash": "c028d9804224", "cached_at": "2026-02-08T12:10:09+00:00"}