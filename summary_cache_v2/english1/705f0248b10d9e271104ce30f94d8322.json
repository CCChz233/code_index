{"summary": "Provides PyTorch Lightning plugins that manage data sampling and batch configuration for NeMo models, including a specialized sampler for Megatron‑LM that computes global and micro‑batch sizes, integrates with training step events, and adapts data loaders for distributed execution.", "business_intent": "Enable reliable and efficient data loading for large‑scale, distributed model training by automatically handling batch calculations and sampler behavior, thereby simplifying the setup of Megatron‑based training pipelines.", "keywords": ["data sampler", "PyTorch Lightning", "distributed training", "batch size", "micro‑batch", "global batch", "Megatron", "NeMo", "data loader adaptation", "training step integration"], "summary_hash": "a09c83ebc2e6", "cached_at": "2026-02-08T10:50:26+00:00"}