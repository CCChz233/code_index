{"summary": "Implements rotary positional embeddings for transformer architectures, providing a module that generates sinusoidal rotation matrices based on token positions and utilities to apply these rotations to token hidden states.", "business_intent": "Enable efficient incorporation of relative positional information in large-scale language models, improving accuracy and scalability of NLP systems built on Megatron-style transformers.", "keywords": ["rotary embedding", "positional encoding", "relative position", "transformer", "Megatron", "PyTorch", "NLP", "neural network"], "summary_hash": "fa3a6f22ffc2", "cached_at": "2026-02-08T11:25:29+00:00"}