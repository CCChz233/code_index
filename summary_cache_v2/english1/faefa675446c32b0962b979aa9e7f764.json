{"summary": "Provides a WordPiece tokenizer that breaks input text into subword tokens using a predefined vocabulary, handling unknown tokens and offering a straightforward tokenization method.", "business_intent": "Enable efficient preprocessing of textual data for NLP and machine‑learning pipelines, especially transformer‑based language models, by converting raw strings into sequences of subword tokens.", "keywords": ["WordPiece", "subword tokenization", "vocabulary", "unknown token handling", "text preprocessing", "NLP", "language model", "tokenizer"], "summary_hash": "5cad3abff801", "cached_at": "2026-02-09T11:55:31+00:00"}