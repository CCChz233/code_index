{"summary": "Implements the GEGLU activation, a gated linear unit variant that splits the input tensor along its last dimension into two equal halves, applies a sigmoid to the second half, and multiplies it elementâ€‘wise with the first half, yielding an output tensor with half the original feature size.", "business_intent": "Offer a differentiable, efficient activation function for deep learning models, especially transformer architectures, to improve expressive power and training dynamics.", "keywords": ["activation", "gated linear unit", "GEGLU", "sigmoid", "tensor split", "transformer", "neural network", "deep learning", "element-wise multiplication"], "summary_hash": "4dfc5a623fe8", "cached_at": "2026-02-08T11:49:14+00:00"}