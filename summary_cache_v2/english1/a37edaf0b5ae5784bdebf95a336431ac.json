{"summary": "Implements the relative position bias used in T5-style transformers, converting token distance into bucketed indices and producing bias tensors that are added to attention scores.", "business_intent": "Provide a reusable component for neural language models to incorporate relative positional information, improving accuracy on various natural language processing tasks.", "keywords": ["relative position bias", "T5", "transformer", "attention", "bucketed positions", "positional encoding", "NLP"], "summary_hash": "b587447cd359", "cached_at": "2026-02-08T09:52:05+00:00"}