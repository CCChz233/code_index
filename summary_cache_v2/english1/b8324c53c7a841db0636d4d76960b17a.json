{"summary": "Provides a causal block-diagonal attention mask that partitions queries and keys into matching blocks, restricting each query to attend only to keys within its own block and only to positions that precede it in that block.", "business_intent": "Facilitate efficient, autoregressive‑compatible attention computation in transformer architectures by limiting attention to block‑wise causal neighborhoods, thereby reducing runtime and memory usage while preserving correct sequence ordering.", "keywords": ["attention mask", "causal", "block diagonal", "transformer", "fast multi‑head attention", "bias", "sequence modeling", "blockwise computation", "autoregressive"], "summary_hash": "1716beb6f632", "cached_at": "2026-02-08T23:23:09+00:00"}