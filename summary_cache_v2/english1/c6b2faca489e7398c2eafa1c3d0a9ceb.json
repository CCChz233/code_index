{"summary": "A Flax neural network module that implements a two‑layer feed‑forward block with a gated linear unit activation, optional dropout, and configurable data type. The hidden size is fixed at four times the input dimension, and the output dimension matches the input dimension.", "business_intent": "Supply a ready‑to‑use feed‑forward component for transformer‑style models in Flax, enabling fast construction of deep learning pipelines that require a standardized linear‑activation‑linear transformation with regularization.", "keywords": ["Flax", "feed-forward", "linear layers", "GLU", "dropout", "neural network", "transformer", "activation", "hidden dimension", "dtype"], "summary_hash": "da1fbea2d294", "cached_at": "2026-02-09T04:00:21+00:00"}