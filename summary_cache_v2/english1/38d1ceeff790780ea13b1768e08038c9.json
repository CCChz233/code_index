{"summary": "A wrapper for a Megatron GPT transformer that inserts three IA3 adapters per layer to scale key, value, and feed‑forward activations, handling adapter configuration, state management, and model loading/saving.", "business_intent": "Provide a parameter‑efficient fine‑tuning solution for large language models, allowing task‑specific adaptation through lightweight IA3 adapters that can be trained, saved, and re‑infused for fast inference.", "keywords": ["IA3", "adapters", "Megatron", "GPT", "transformer", "parameter-efficient fine-tuning", "task adaptation", "state_dict", "model loading", "inference"], "summary_hash": "8a07cc36c6af", "cached_at": "2026-02-08T10:07:39+00:00"}