{"summary": "Implements the Gaussian Error Linear Unit activation, handling forward evaluation, gradient computation, and symbolic graph representation.", "business_intent": "Provide a reusable GELU activation component for neural network libraries to enable non-linear transformations and support automatic differentiation.", "keywords": ["GELU", "activation function", "neural networks", "forward evaluation", "gradient computation", "automatic differentiation", "symbolic graph", "deep learning"], "summary_hash": "3af880c55566", "cached_at": "2026-02-08T09:47:43+00:00"}