{"summary": "Implements the self‑attention mechanism of the Longformer model, combining local sliding‑window attention with optional global attention to efficiently process very long input sequences.", "business_intent": "Provide a scalable attention layer for NLP models that need to handle long documents while keeping computational and memory requirements manageable.", "keywords": ["self-attention", "Longformer", "sliding window", "global attention", "efficient transformer", "long sequences", "memory optimization", "NLP", "attention computation", "chunking"], "summary_hash": "7750052b70d3", "cached_at": "2026-02-09T11:12:11+00:00"}