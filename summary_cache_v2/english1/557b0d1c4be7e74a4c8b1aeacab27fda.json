{"summary": "Provides tools to convert NeMo language model checkpoints into TensorRT‑LLM format, build optimized inference engines, manage prompt‑tuning tables, and execute high‑throughput, low‑latency inference (including Triton server integration).", "business_intent": "Accelerate deployment of large language models by offering a streamlined pathway from NeMo checkpoints to production‑grade TensorRT‑LLM inference, reducing latency and resource usage for AI applications.", "keywords": ["export", "checkpoint conversion", "TensorRT-LLM", "LLM inference", "NeMo", "GPU acceleration", "prompt tuning", "Triton integration", "engine building", "refit", "streaming inference"], "summary_hash": "e8b13cba17fe", "cached_at": "2026-02-08T08:22:09+00:00"}