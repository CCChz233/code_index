{"summary": "The module defines two graph neural network architectures that stack self‑attention graph convolution layers with hierarchical node pooling (SAGPool) and aggregate the remaining node features via a global readout to produce fixed‑size graph embeddings. It also provides a helper to instantiate the appropriate network.", "business_intent": "Offer ready‑to‑use GNN models for tasks such as graph classification or regression, leveraging attention‑based pooling to capture hierarchical structure and generate compact graph representations for downstream machine‑learning pipelines.", "keywords": ["graph neural network", "self‑attention pooling", "SAGPool", "hierarchical pooling", "DGL", "PyTorch", "graph classification", "global readout", "graph embedding"], "summary_hash": "ffe6cc282cda", "cached_at": "2026-02-09T00:19:42+00:00"}