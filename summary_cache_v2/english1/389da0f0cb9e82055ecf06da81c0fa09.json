{"summary": "Implements a distributed data loader that parallels PyTorch's DataLoader semantics while using DGL's pre‑launched worker processes to perform graph neighborhood sampling and assemble mini‑batches across multiple machines.", "business_intent": "Facilitate large‑scale graph neural network training by providing an efficient, multi‑process data loading pipeline that reduces sampling latency and scales across a distributed cluster.", "keywords": ["distributed training", "data loader", "graph neural networks", "DGL", "multiprocessing", "neighborhood sampling", "mini-batch", "parallelism", "PyTorch compatibility", "sampler pool"], "summary_hash": "6e7228454594", "cached_at": "2026-02-09T00:39:03+00:00"}