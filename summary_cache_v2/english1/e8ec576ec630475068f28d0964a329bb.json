{"summary": "Implements an iterator that partitions a global sequence of dataset indices into equal-sized shards for each parallel process or GPU, delivering the appropriate subset of each batch to the corresponding worker.", "business_intent": "Enable efficient distributed data loading for multi‑GPU or multi‑process training by automatically sharding batches so that each worker processes its own portion without overlap.", "keywords": ["sharding", "sampler", "batch", "distributed training", "multi‑GPU", "parallel processes", "data loading", "index partitioning"], "summary_hash": "274773e49e83", "cached_at": "2026-02-09T06:21:47+00:00"}