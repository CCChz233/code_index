{"summary": "Provides a set of builder classes that assemble and configure PyTorch Lightning Trainer objects specifically for Megatron-based models, handling distributed strategies, precision plugins, gradient scaling, and model‑specific options such as pipeline parallelism and fine‑tuning configurations.", "business_intent": "Streamline and standardize the setup of large‑scale Megatron training pipelines within NeMo, reducing boilerplate and ensuring correct integration of mixed‑precision, distributed, and model‑specific training features for faster and more reliable model development.", "keywords": ["Megatron", "Trainer builder", "PyTorch Lightning", "distributed training", "mixed precision", "gradient scaling", "pipeline parallelism", "NLP", "BERT", "T5", "Stable Diffusion", "LMPP", "custom plugins", "checkpoint IO"], "summary_hash": "c486b8540e09", "cached_at": "2026-02-08T11:19:34+00:00"}