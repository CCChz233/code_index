{"summary": "A configuration container for Marian translation models that stores all architectural and training hyperparameters—including vocabulary size, model dimension, encoder/decoder layer counts, attention heads, feed‑forward sizes, activation function, dropout rates, maximum position embeddings, weight initialization scale, layer‑drop probabilities, embedding scaling, cache usage, and forced EOS token ID—and inherits from a generic pretrained configuration to be passed when instantiating a MarianModel.", "business_intent": "Enable developers and researchers to define, customize, and reproduce Marian transformer models for neural machine translation by specifying model architecture and training settings in a single, reusable configuration object.", "keywords": ["Marian", "configuration", "transformer", "encoder", "decoder", "vocab size", "model dimension", "attention heads", "feed-forward network", "activation function", "dropout", "layerdrop", "position embeddings", "initialization", "cache", "forced EOS token", "machine translation"], "summary_hash": "16403c8476a2", "cached_at": "2026-02-09T11:28:02+00:00"}