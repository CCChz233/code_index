{"summary": "A configuration container for the NashMD training process, extending the generic online DPO settings with a specialized mixture coefficient that controls how much the model's logits are blended with a reference model's logits, optionally varying per training epoch.", "business_intent": "Enable users to fineâ€‘tune NashMD models by specifying hyperparameters, especially a flexible logit mixing ratio, to improve performance and stability during online reinforcement learning.", "keywords": ["configuration", "training", "mixture coefficient", "logit blending", "online DPO", "epoch scheduling", "hyperparameters", "NashMD", "trainer"], "summary_hash": "180959bbc261", "cached_at": "2026-02-09T05:54:17+00:00"}