{"summary": "This module implements a client for the Predibase inference service, exposing chat completion and embedding generation capabilities through a unified interface. It validates configuration, maps OpenAI‑style parameters to Predibase equivalents, performs synchronous and asynchronous HTTP requests, supports streaming responses, and parses the service output into Litellm‑compatible response objects.", "business_intent": "Allow applications using the Litellm framework to seamlessly integrate Predibase as an LLM provider, offering OpenAI‑compatible chat and embedding functionality without requiring direct handling of Predibase API specifics.", "keywords": ["Predibase", "chat completion", "embeddings", "HTTP client", "asynchronous", "streaming", "configuration mapping", "error handling", "OpenAI compatibility", "Litellm integration"], "summary_hash": "14cecce82aa2", "cached_at": "2026-02-08T07:44:33+00:00"}