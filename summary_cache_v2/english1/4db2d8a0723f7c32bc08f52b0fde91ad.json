{"summary": "Provides an attention bias mask for block‑diagonal causal attention where each block may have a distinct causality offset and the key/value tensors are padded to a uniform length. The mask enforces that queries attend only to keys within the same block, excludes padded positions, and respects the per‑block causal ordering.", "business_intent": "Enable efficient and flexible transformer attention handling variable‑length blocks and padded key/value sequences, allowing models to apply causal constraints with per‑block offsets without extra runtime overhead.", "keywords": ["attention mask", "block diagonal", "causal attention", "offset per block", "key padding", "value padding", "sequence lengths", "transformer", "fast multi‑head attention", "bias generation"], "summary_hash": "9becf2862d6a", "cached_at": "2026-02-08T23:23:19+00:00"}