{"summary": "Implements a lightweight multi‑head attention module designed for MobileViT models, performing the attention computation and supporting head removal to streamline the network.", "business_intent": "Provide an efficient attention component for vision transformer models on mobile and edge devices, allowing reduced computational cost and memory usage through optional head pruning.", "keywords": ["attention", "mobile", "vision transformer", "lightweight", "multi‑head", "head pruning", "efficiency", "edge devices"], "summary_hash": "af98cc6f60d6", "cached_at": "2026-02-09T10:37:31+00:00"}