{"summary": "Manages an ensemble of models to generate token sequences using beam search, handling encoder hidden states, one‑step forward passes, probability averaging, length penalty computation, and providing utilities to freeze or unfreeze model parameters during inference.", "business_intent": "Enhance the quality and robustness of generated text for applications such as translation, summarization, or conversational AI by leveraging multiple models in a coordinated beam‑search decoding process.", "keywords": ["ensemble", "beam search", "sequence generation", "probability averaging", "length penalty", "encoder hidden states", "model freezing", "inference"], "summary_hash": "c26bc8077290", "cached_at": "2026-02-08T09:46:21+00:00"}