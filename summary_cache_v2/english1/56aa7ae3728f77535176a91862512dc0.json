{"summary": "A strategy class that applies user‑provided parallelization logic to a PyTorch model, configuring a 2‑dimensional device mesh that combines fully‑sharded data parallelism with tensor parallelism, and managing the distributed environment, rank assignment, and checkpoint I/O.", "business_intent": "Enable developers to scale large models across multiple GPUs and nodes with custom parallelism schemes while handling the underlying distributed setup and reliable checkpoint saving/restoring.", "keywords": ["model parallelism", "data parallel", "tensor parallel", "distributed training", "device mesh", "checkpointing", "PyTorch", "FSDP2", "GPU scaling", "experimental API"], "summary_hash": "884fd5405bfe", "cached_at": "2026-02-08T08:25:57+00:00"}