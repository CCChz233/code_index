{"summary": "Provides a temporary all-to-all data transformation that reshapes tensors from a hidden‑parallel layout to a sequence‑parallel layout, offering simple forward and backward helper operations.", "business_intent": "Facilitate distributed model training by allowing quick conversion between hidden‑parallel and sequence‑parallel tensor representations, serving as a stop‑gap implementation until the functionality is integrated into the core library.", "keywords": ["all-to-all", "hidden parallel", "sequence parallel", "data reshaping", "forward helper", "backward helper", "distributed training", "model parallelism", "temporary workaround"], "summary_hash": "59009d1d527e", "cached_at": "2026-02-08T09:50:45+00:00"}