{"summary": "Provides an example script that sets up and runs parameter‑efficient fine‑tuning (PEFT) for the Neva multimodal language model using Megatron's trainer infrastructure and Hydra configuration.", "business_intent": "Demonstrate how to adapt a large multimodal LLM to specific tasks or domains with minimal parameter updates, enabling efficient customization for enterprise or research applications.", "keywords": ["multimodal", "large language model", "Neva", "PEFT", "parameter‑efficient fine‑tuning", "Megatron", "trainer", "Hydra", "configuration", "example script"], "summary_hash": "7cca8ea49194", "cached_at": "2026-02-08T10:37:06+00:00"}