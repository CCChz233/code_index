{"summary": "Provides model‑parallel linear layers that partition the weight matrix across devices—one variant splits columns and the other splits rows—handling local weight slices, optional bias, and the required collective communication to produce correct outputs in a distributed setting.", "business_intent": "Facilitate scalable training of large neural networks by distributing linear transformations across multiple GPUs or accelerators, reducing memory footprint per device and improving throughput for both model‑parallel and sequence‑parallel workloads.", "keywords": ["model parallelism", "linear layer", "weight sharding", "column parallel", "row parallel", "distributed training", "collective communication", "bias handling", "PyTorch", "sequence parallel"], "summary_hash": "56dcde216885", "cached_at": "2026-02-08T23:29:41+00:00"}