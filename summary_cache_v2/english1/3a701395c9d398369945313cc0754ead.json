{"summary": "Applies layer normalization to the final dimension of input tensors, adjusting activations to have zero mean and unit variance with optional learnable scale and shift.", "business_intent": "Provide stable and efficient training for Performer-based models in the Flax ecosystem by normalizing activations across the feature axis.", "keywords": ["layer normalization", "Flax", "Performer", "neural network", "activation scaling", "stable training", "tensor normalization", "last axis"], "summary_hash": "e14dfbdd8e8d", "cached_at": "2026-02-09T06:00:21+00:00"}