{"summary": "A model that constructs a four‑dimensional causal attention mask, ensuring that each element can only attend to preceding elements across all dimensions for use in attention mechanisms.", "business_intent": "Provide a ready‑to‑use mask generator for causal attention in multi‑dimensional (e.g., spatiotemporal) transformer models, simplifying model setup and improving training correctness.", "keywords": ["causal attention", "4D mask", "attention mask generation", "transformer", "spatiotemporal", "mask model"], "summary_hash": "d3dea393e48c", "cached_at": "2026-02-09T04:17:22+00:00"}