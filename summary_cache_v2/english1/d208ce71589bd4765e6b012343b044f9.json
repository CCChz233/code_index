{"summary": "A structured container that holds all relevant tensors produced by a sequence-to-sequence language model, including optional loss, prediction logits, cached attention key/value pairs, decoder and encoder hidden states, and attention weight matrices.", "business_intent": "Facilitates training, inference, and analysis of seq2seq tasks such as translation, summarization, and text generation by delivering model outputs needed for loss computation, fast incremental decoding, and interpretability of encoder/decoder representations.", "keywords": ["sequence-to-sequence", "language model", "output container", "loss", "logits", "past key values", "decoder hidden states", "decoder attentions", "cross attentions", "encoder hidden states", "encoder attentions", "caching", "inference", "training", "interpretability"], "summary_hash": "8035fa508b61", "cached_at": "2026-02-09T06:29:02+00:00"}