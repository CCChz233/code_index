{"summary": "Encodes input sequences using a Bert-like transformer architecture enhanced with relative position bias, providing access to the attention mask and relative positional embeddings.", "business_intent": "Facilitate models that need to capture relative positional relationships, improving performance on language or speech tasks where positionâ€‘aware attention is critical.", "keywords": ["transformer encoder", "relative position bias", "attention mask", "positional embeddings", "Bert architecture", "sequence encoding", "deep learning", "NLP", "speech processing"], "summary_hash": "8d0df2e96de7", "cached_at": "2026-02-09T08:14:54+00:00"}