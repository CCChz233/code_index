{"summary": "Implements backend‑agnostic tensor storage classes that slice feature tensors and move the resulting slices to a specified device, exposing a simple fetch API for DGL graph data.", "business_intent": "Facilitate fast and memory‑efficient access to node/edge features during GNN training and inference by providing a unified tensor storage layer that works with multiple deep‑learning frameworks and supports CPU/GPU placement.", "keywords": ["tensor storage", "feature slicing", "device placement", "backend agnostic", "graph neural networks", "DGL", "GPU", "CPU", "efficient retrieval"], "summary_hash": "e812f1ab78c2", "cached_at": "2026-02-09T00:59:21+00:00"}