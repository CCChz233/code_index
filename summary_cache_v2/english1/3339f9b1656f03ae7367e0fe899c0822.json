{"summary": "Implements forward and backward operations for edge-wise softmax normalization, turning raw edge scores into probability distributions and supporting gradient propagation.", "business_intent": "Allow graph neural network components to apply softmax over edges for attention mechanisms, enabling effective training and inference of edge-weighted models.", "keywords": ["edge softmax", "forward pass", "backward pass", "normalization", "graph neural network", "attention", "gradient", "probability distribution"], "summary_hash": "a4fd11258dbd", "cached_at": "2026-02-08T23:49:12+00:00"}