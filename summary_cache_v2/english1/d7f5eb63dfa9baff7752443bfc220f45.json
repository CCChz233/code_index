{"summary": "The module supplies utilities for DeepSpeed integration, defining structures that capture per‑layer metadata and aggregate model‑wide statistics. It computes average parameters per shard, total and trainable parameter counts, and formats this information into a concise summary report for models run with DeepSpeed.", "business_intent": "Enable Lightning users to assess and report the memory and parameter distribution of their models when using DeepSpeed, facilitating optimization, debugging, and documentation of model parallelism.", "keywords": ["DeepSpeed", "model summary", "parameter count", "sharding", "trainable parameters", "layer metadata", "PyTorch", "Lightning", "memory footprint"], "summary_hash": "018eb3047bb7", "cached_at": "2026-02-08T09:01:05+00:00"}