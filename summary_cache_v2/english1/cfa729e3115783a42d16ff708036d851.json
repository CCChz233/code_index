{"summary": "A command‑line utility that combines LoRA adapter weights with a base Megatron‑GPT language model, producing a single merged model file ready for inference. It automatically handles any tensor‑parallel or pipeline‑parallel configuration used during LoRA training, so users do not need to specify parallelism details.", "business_intent": "Enable seamless deployment of LoRA‑fine‑tuned GPT models by consolidating adapter parameters into the original model, simplifying model serving, distribution, and downstream applications.", "keywords": ["LoRA", "weight merging", "GPT", "Megatron", "model parallelism", "tensor parallel", "pipeline parallel", "NeMo", "checkpoint conversion", "inference preparation"], "summary_hash": "502fe00e35f1", "cached_at": "2026-02-08T11:48:35+00:00"}