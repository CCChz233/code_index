{"summary": "Provides a mixed‑precision plugin for Megatron‑style models in NeMo Lightning, automatically converting tensors, modules and optimizer states to the chosen low‑precision format, supplying a forward‑pass context, and coordinating gradient scaling and optimizer steps for stable, memory‑efficient training.", "business_intent": "Accelerate training of large transformer models by leveraging reduced‑precision arithmetic while preserving numerical stability, thereby lowering GPU memory usage and increasing throughput.", "keywords": ["mixed precision", "Megatron", "NeMo Lightning", "PyTorch Lightning", "gradient scaling", "optimizer step", "FP16", "BF16", "training acceleration", "large models"], "summary_hash": "3c0991f8ac70", "cached_at": "2026-02-08T10:50:30+00:00"}