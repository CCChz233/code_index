{"summary": "Implements a rotary positional embedding with linear scaling, extending the base Persimmon rotary embedding and managing cached cosine/sine tables for efficient use in transformer models.", "business_intent": "Provide scalable and efficient positional encoding to improve transformer-based language model performance, especially for the Persimmon architecture.", "keywords": ["rotary embedding", "linear scaling", "positional encoding", "transformer", "cosine cache", "sine cache", "Persimmon", "NLP", "deep learning"], "summary_hash": "e16a6f392000", "cached_at": "2026-02-09T09:17:03+00:00"}