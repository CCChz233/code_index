{"summary": "Implements a transformer model that can operate as an encoder with self‑attention or as a decoder with an added cross‑attention layer, following the original \"Attention Is All You Need\" design. When initialized as a decoder with cross‑attention enabled, it accepts encoder hidden states to support sequence‑to‑sequence processing.", "business_intent": "Provides a flexible neural component for building encoder‑only, decoder‑only, or encoder‑decoder architectures, facilitating applications such as language generation, translation, or other sequence modeling tasks.", "keywords": ["transformer", "self-attention", "cross-attention", "encoder", "decoder", "seq2seq", "attention mechanism", "configurable architecture"], "summary_hash": "28b593dbd929", "cached_at": "2026-02-09T09:50:38+00:00"}