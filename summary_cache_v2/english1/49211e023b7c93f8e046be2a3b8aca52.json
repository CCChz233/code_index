{"summary": "Configuration container for the orthogonal transformer attention mechanism, defining the number of landmarks, the fraction of query samples to subsample, and the strategy for selecting landmarks to approximate the softmax computation.", "business_intent": "Provide a flexible way to adjust attention approximation parameters, allowing developers and researchers to balance accuracy and efficiency when deploying scalable transformer models.", "keywords": ["attention", "transformer", "configuration", "landmarks", "softmax approximation", "subsampling", "selection strategy", "hyperparameters"], "summary_hash": "3ea58947558d", "cached_at": "2026-02-08T23:20:31+00:00"}