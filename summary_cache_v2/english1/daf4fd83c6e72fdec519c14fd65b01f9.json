{"summary": "Internal utility that governs when gradient synchronization is performed during the backward pass of Fully Sharded Data Parallel training, allowing selective enabling or disabling of communication.", "business_intent": "Improve distributed training efficiency by reducing unnecessary gradient synchronization overhead, thereby accelerating model convergence and resource utilization.", "keywords": ["FSDP", "backward pass", "gradient synchronization", "distributed training", "communication optimization", "sync control"], "summary_hash": "4184fd84ffe1", "cached_at": "2026-02-08T08:25:40+00:00"}