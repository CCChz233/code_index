{"summary": "Generates patch and positional embeddings for a Swin transformer architecture, with optional inclusion of a learnable mask token for masked modeling.", "business_intent": "Provide visual token representations that can be consumed by downstream transformer models for document image understanding, classification, or other computer vision applications.", "keywords": ["patch embedding", "positional encoding", "mask token", "Swin transformer", "visual embeddings", "document understanding", "computer vision"], "summary_hash": "10111a91c633", "cached_at": "2026-02-09T09:41:29+00:00"}