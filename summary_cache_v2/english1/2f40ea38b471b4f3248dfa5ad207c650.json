{"summary": "The module defines a compositional attention component that splits the attention operation into a search phase (softmax over query‑key products) and a retrieval phase (value aggregation guided by learned rules). It offers configurable model, attention, key, value, and selection dimensions, and supports bias, dropout, causal masking, and optional non‑linear scoring for the retrieval step.", "business_intent": "Provide a versatile and extensible attention layer for transformer‑based models, enabling more expressive and modular attention mechanisms that can be tailored to various research and production scenarios.", "keywords": ["attention", "compositional attention", "transformer", "query", "key", "value", "softmax", "retrieval", "dropout", "causal masking", "configurable dimensions", "rule‑based aggregation", "PyTorch"], "summary_hash": "288b4af5bbe4", "cached_at": "2026-02-08T23:31:15+00:00"}