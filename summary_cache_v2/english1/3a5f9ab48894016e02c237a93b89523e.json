{"summary": "Implements a fast scaled dot‑product attention processor that leverages PyTorch 2.0 fused projection kernels. It combines the query, key and value linear layers into a single fused operation for self‑attention, and fuses key and value for cross‑attention, delivering higher throughput and lower memory usage.", "business_intent": "Provide an optimized, plug‑in attention component for transformer models that reduces computational overhead and latency, enabling developers to accelerate inference and training on PyTorch 2.0 platforms.", "keywords": ["scaled dot-product attention", "fused projection", "self‑attention", "cross‑attention", "PyTorch 2.0", "performance optimization", "transformer", "experimental"], "summary_hash": "c28aa9753caa", "cached_at": "2026-02-09T03:32:22+00:00"}