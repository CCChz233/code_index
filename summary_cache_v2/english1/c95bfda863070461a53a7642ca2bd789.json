{"summary": "Implements a single decoder block of the MBart transformer model, encapsulating self‑attention, encoder‑decoder attention, feed‑forward processing, layer normalization and dropout to transform decoder hidden states.", "business_intent": "Enables multilingual text generation tasks such as translation, summarization, and cross‑lingual language modeling by providing the core decoding computation for the MBart architecture.", "keywords": ["MBart", "decoder layer", "transformer", "self‑attention", "cross‑attention", "feed‑forward", "layer normalization", "multilingual", "sequence generation"], "summary_hash": "05bd96735fa8", "cached_at": "2026-02-09T11:04:34+00:00"}