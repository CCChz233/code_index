{"summary": "A lightweight wrapper that augments a linear transformation with a low‑rank adaptation module, mimicking LoRA behavior for testing purposes.", "business_intent": "Enable developers to experiment with and validate LoRA‑style adapters in neural network models without modifying the underlying linear layers.", "keywords": ["LoRA", "adapter", "linear layer", "wrapper", "testing", "low‑rank", "neural network", "fine‑tuning"], "summary_hash": "60f350706e65", "cached_at": "2026-02-09T04:28:11+00:00"}