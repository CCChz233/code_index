{"summary": "A Keras layer that performs linear transformations using a user‑specified Einstein summation expression, supporting arbitrary tensor ranks, optional bias and activation, shape inference, low‑rank adaptation (LoRA) for efficient fine‑tuning, and quantized execution (int8/float8).", "business_intent": "Enable developers to replace standard dense layers with a highly configurable einsum‑based operation that can handle complex input/output shapes, apply fine‑tuning shortcuts, and run efficiently on quantized hardware.", "keywords": ["einsum", "dense layer", "Keras", "custom linear transformation", "bias", "activation", "LoRA", "low-rank adaptation", "quantization", "int8", "float8", "gradient", "variable management"], "summary_hash": "5b99a718cd7a", "cached_at": "2026-02-09T12:03:52+00:00"}