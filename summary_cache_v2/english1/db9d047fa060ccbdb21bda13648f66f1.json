{"summary": "A structured container that holds all relevant tensors produced by a transformer model, including the final hidden representations, optional cached key/value tensors for fast incremental decoding, per‑layer hidden states, attention weight matrices, and Mixture‑of‑Experts routing logits.", "business_intent": "Enable efficient inference, sequential generation, and detailed analysis of transformer models by providing a unified, cache‑aware output format that supports both standard attention mechanisms and expert routing diagnostics.", "keywords": ["last_hidden_state", "past_key_values", "hidden_states", "attentions", "router_logits", "transformer", "caching", "mixture_of_experts", "model_output", "sequence_generation"], "summary_hash": "839095e913ed", "cached_at": "2026-02-09T06:28:28+00:00"}