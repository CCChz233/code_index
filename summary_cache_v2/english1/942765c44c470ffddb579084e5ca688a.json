{"summary": "The module defines a PyTorch Lightning callback that records a full training iteration as a CUDA graph, handling its creation, execution, and cleanup across the training lifecycle while excluding data loading and learning‑rate scheduling. It also provides a static buffer manager for pre‑allocated tensors and a collection of helper utilities that integrate with Lightning hooks such as training step, optimizer step, and automatic mixed‑precision handling.", "business_intent": "To boost model training speed on NVIDIA GPUs by reducing kernel launch overhead through CUDA graph capture, while offering seamless integration with the Lightning training loop and supporting distributed and mixed‑precision setups.", "keywords": ["CUDA graph", "training acceleration", "PyTorch Lightning", "static buffer management", "mixed precision", "distributed training", "GPU performance", "graph capture", "NeMo", "optimizer integration"], "summary_hash": "c65ad55998fc", "cached_at": "2026-02-08T10:51:03+00:00"}