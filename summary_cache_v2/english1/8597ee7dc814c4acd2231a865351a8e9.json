{"summary": "A TensorFlow loss module that computes the cross‑entropy for causal (autoregressive) language modeling, comparing model logits to target token IDs and automatically skipping any positions marked with a label of -100.", "business_intent": "Enable straightforward training and fine‑tuning of next‑token prediction models such as GPT‑style generators within TensorFlow pipelines, supporting tasks like text generation, language model adaptation, and downstream NLP applications.", "keywords": ["causal language modeling", "next token prediction", "TensorFlow loss", "cross entropy", "ignore -100 label", "autoregressive model", "language model training", "NLP"], "summary_hash": "20bea0b86651", "cached_at": "2026-02-09T06:20:17+00:00"}