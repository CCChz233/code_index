{"summary": "Implements a neural network module that computes query‑key‑value (QKV) attention while preserving the legacy input and output head shapes. Includes utilities for initializing the layer, estimating FLOPs, and executing the forward attention computation.", "business_intent": "Serve as a compatible, drop‑in replacement for legacy QKV attention components in transformer models, ensuring seamless integration with existing architectures and checkpoint formats.", "keywords": ["QKV attention", "legacy compatibility", "transformer", "neural network module", "forward pass", "FLOPs estimation", "head shaping", "PyTorch"], "summary_hash": "926a77611ae4", "cached_at": "2026-02-08T08:57:12+00:00"}