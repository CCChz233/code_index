{"summary": "This module implements the integration layer for communicating with the Ollama language model service. It defines configuration handling, custom error types, and provides functions to perform chat completions, including asynchronous calls and streaming responses, using various HTTP client libraries. The implementation also supports processing of tool calls and response parsing within the litellm framework.", "business_intent": "Allow developers to incorporate Ollama-powered AI chat capabilities into their applications, offering both synchronous and asynchronous completion APIs with streaming support, while handling errors and configuration consistently.", "keywords": ["Ollama", "chat completion", "asynchronous", "streaming", "HTTP client", "error handling", "configuration", "tool calls", "litellm integration"], "summary_hash": "ebce42321629", "cached_at": "2026-02-08T07:43:22+00:00"}