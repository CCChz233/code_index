{"summary": "A wrapper around a BART decoder that handles correct loading of pretrained checkpoints when the decoder is combined with an EncoderDecoderModel, and supplies a straightforward forward pass.", "business_intent": "Allow pretrained causal language model checkpoints to be integrated into encoder‑decoder pipelines, simplifying development of generation‑focused applications like summarization, translation, or chat.", "keywords": ["BART", "decoder wrapper", "pretrained checkpoint loading", "EncoderDecoderModel", "causal language model", "model integration", "forward helper", "text generation"], "summary_hash": "fbc1a7a98df3", "cached_at": "2026-02-09T08:57:36+00:00"}