{"summary": "Implements a block‑sparse feed‑forward (MLP) layer that selects the top two expert sub‑networks for each token, as used in the Mixtral transformer architecture.", "business_intent": "Offer a computationally efficient, sparsely activated MLP component to reduce FLOPs and memory usage while preserving model capacity in large language models.", "keywords": ["Mixtral", "block-sparse", "top‑2", "MLP", "feed‑forward", "sparse activation", "expert routing", "neural network", "transformer", "efficiency"], "summary_hash": "e17f15cc7b60", "cached_at": "2026-02-09T10:15:19+00:00"}