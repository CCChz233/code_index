{"summary": "Manages caching for LLM API calls, handling synchronous and asynchronous requests, streaming and embedding responses, cache lookup, storage decisions, and logging integration.", "business_intent": "Reduce latency and API costs by reusing prior LLM outputs through an efficient cache layer.", "keywords": ["caching", "LLM", "async", "sync", "streaming", "embeddings", "cache retrieval", "cache storage", "logging", "performance"], "summary_hash": "41f7e4fe0209", "cached_at": "2026-02-08T06:58:05+00:00"}