{"summary": "Defines an enumeration of floating‑point precision levels (full, half, and brain) used to describe how numeric values are stored and processed.", "business_intent": "Enables applications, especially in machine learning and scientific computing, to select the appropriate precision for performance‑accuracy trade‑offs.", "keywords": ["precision", "floating point", "FP32", "FP16", "BF16", "enumeration", "numeric type", "performance", "accuracy", "data type"], "summary_hash": "d58d8969b9a3", "cached_at": "2026-02-09T02:12:07+00:00"}