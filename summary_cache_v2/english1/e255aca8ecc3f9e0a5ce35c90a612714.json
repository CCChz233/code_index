{"summary": "A custom exception that signals the language model's response was cut off because the maximum token limit was reached.", "business_intent": "Enable calling code to detect and handle truncated LLM outputs, such as by retrying, requesting additional tokens, or informing the user.", "keywords": ["exception", "incomplete output", "token limit", "LLM", "truncation", "error handling", "max tokens"], "summary_hash": "b4e149be96fc", "cached_at": "2026-02-09T06:28:11+00:00"}