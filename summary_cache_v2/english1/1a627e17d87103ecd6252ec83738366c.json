{"summary": "This module implements the ALiBi (Attention with Linear Biases) relative position embedding for Megatron-style transformer models. It generates linear bias tensors based on token distances, handling both auto‑regressive decoding and symmetric encoder contexts, and includes utilities for constructing the required slope values.", "business_intent": "Enable efficient and scalable relative position bias computation in large‑scale transformer models to improve attention handling for various NLP tasks, particularly in auto‑regressive and encoder‑decoder scenarios.", "keywords": ["ALiBi", "relative position embedding", "transformer", "attention bias", "Megatron", "auto-regressive", "encoder", "PyTorch", "slope calculation", "bias tensor"], "summary_hash": "af71a5783042", "cached_at": "2026-02-08T11:25:26+00:00"}