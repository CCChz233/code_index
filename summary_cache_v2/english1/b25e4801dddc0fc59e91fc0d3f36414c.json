{"summary": "Defines a set of dataclass-based configuration objects that capture all architectural and hyper‑parameter details of a large language model required for TensorRT‑LLM inference, including attention, decoder layers, embeddings, layer normalization, linear transformations, MLPs, and mixture‑of‑experts components, together with utilities to construct these configurations from NeMo checkpoints and to serialize/deserialize them.", "business_intent": "Facilitate the conversion of NeMo large language models into TensorRT‑LLM inference engines by providing a structured, programmatic representation of model architecture and quantization settings.", "keywords": ["model configuration", "large language model", "TensorRT-LLM", "NeMo", "attention", "decoder layer", "embedding", "layernorm", "linear", "MLP", "Mixture of Experts", "serialization", "dataclass", "export"], "summary_hash": "4f7c10a3895b", "cached_at": "2026-02-08T11:39:13+00:00"}