{"summary": "A Flax module that performs spatial downsampling while applying a stack of cross‑attention transformer layers to 2‑D feature maps, optionally followed by a downsampling operation.", "business_intent": "Supply a reusable building block for transformer‑based UNet models in image generation pipelines, enabling efficient multi‑head cross‑attention and configurable downsampling with support for memory‑efficient attention.", "keywords": ["cross attention", "downsampling", "UNet", "transformer", "Flax", "diffusion models", "multi‑head attention", "memory efficient attention", "dropout", "image synthesis"], "summary_hash": "c188d1c3e908", "cached_at": "2026-02-09T04:31:22+00:00"}