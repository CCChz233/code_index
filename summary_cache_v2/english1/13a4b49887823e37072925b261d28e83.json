{"summary": "A configuration holder that defines options for applying BitsAndBytes low‑bit (4‑bit or 8‑bit) quantization to neural network models, specifying whether to quantize, data types, thresholds, double‑quantization, and module exclusion lists.", "business_intent": "Enable developers to compress models and accelerate inference by providing a simple, customizable interface for low‑precision quantization, making large models runnable on limited hardware.", "keywords": ["quantization", "bitsandbytes", "4-bit", "8-bit", "model compression", "low‑precision", "configuration", "torch dtype", "module exclusion", "double quantization"], "summary_hash": "bd48bb96fa59", "cached_at": "2026-02-09T02:11:34+00:00"}