{"summary": "Provides a command‑line tool that reads one or more raw text files, tokenizes the content with a specified tokenizer, splits the token sequences into fixed‑size chunks, and stores the chunks in tar archives for use as a transformer language‑model training dataset.", "business_intent": "Facilitates the preparation of large‑scale, efficiently loadable tokenized text corpora for training transformer language models used in automatic speech recognition neural rescoring pipelines.", "keywords": ["tokenization", "tar archive", "dataset creation", "language model", "transformer", "ASR", "neural rescoring", "chunking", "command line", "preprocessing"], "summary_hash": "34d5225533cd", "cached_at": "2026-02-08T11:51:49+00:00"}