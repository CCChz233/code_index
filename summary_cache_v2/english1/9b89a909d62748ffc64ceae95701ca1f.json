{"summary": "The module defines a set of lightweight wrapper classes that encapsulate xFormers transformer models for the Long Range Arena (LRA) benchmark. It provides single‑ and dual‑self‑consistency model wrappers, a processing head with optional dual outputs, and a PyTorch Lightning training module that handles optimizer setup, training/validation/testing steps, and epoch‑level metric aggregation. Helper utilities for token handling and pooling are also included.", "business_intent": "Enable researchers and engineers to quickly configure, train, and evaluate transformer models on LRA tasks, supporting self‑consistency experiments and dual‑output heads within a standardized training framework.", "keywords": ["transformer", "benchmark", "Long Range Arena", "PyTorch Lightning", "model wrapper", "self-consistency", "dual head", "pooling", "xFormers", "training loop"], "summary_hash": "81ac49f8dcbf", "cached_at": "2026-02-08T23:30:47+00:00"}