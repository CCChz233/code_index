{"summary": "Implements a single encoder layer of the Autoformer architecture, encapsulating the attention and feed‑forward operations used to encode temporal sequences.", "business_intent": "Provides a reusable building block for constructing Autoformer models that perform long‑term time‑series forecasting.", "keywords": ["autoformer", "encoder layer", "time series", "forecasting", "attention", "feed-forward", "neural network", "deep learning", "PyTorch", "transformer"], "summary_hash": "88a03225d0d4", "cached_at": "2026-02-09T10:34:23+00:00"}