{"summary": "Implements the Rectified Linear Unit (ReLU) activation, providing a callable interface that applies the non‑linear function and computes the corresponding output specification.", "business_intent": "Provide a reusable activation component for machine‑learning models to introduce non‑linearity, enhance model expressiveness, and support efficient forward computation.", "keywords": ["relu", "activation", "neural network", "non-linear", "forward pass", "output specification", "callable", "helper"], "summary_hash": "71c4b208acc3", "cached_at": "2026-02-09T11:33:39+00:00"}