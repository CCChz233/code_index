{"summary": "Defines a neural network layer that applies a gated linear unit variant using GELU as the gating function. The layer linearly projects the input to twice the output size, splits the projection, activates one half with GELU, and multiplies it with the other half to produce the final output.", "business_intent": "Offer a ready‑to‑use activation component that can be integrated into deep learning models to improve representation capacity and training dynamics.", "keywords": ["GEGLU", "gated linear unit", "GELU activation", "neural network layer", "linear projection", "bias term", "dimensionality reduction", "deep learning", "activation function"], "summary_hash": "966e3a64324e", "cached_at": "2026-02-09T04:05:03+00:00"}