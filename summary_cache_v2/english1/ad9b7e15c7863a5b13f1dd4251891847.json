{"summary": "The module implements utilities for parameter‑efficient prompt tuning of large language models. It defines a cache for pre‑computed prompt encoder outputs, a learnable prompt embedding container, an encoder that transforms textual prompts into virtual token embeddings, and a lightweight two‑layer tensor‑parallel MLP used by the encoder.", "business_intent": "Enable fast and memory‑efficient p‑tuning by providing reusable prompt representations and parallelizable encoding components that can be integrated into NeMo NLP models for downstream tasks.", "keywords": ["prompt tuning", "p-tuning", "virtual token embeddings", "inference cache", "tensor parallel MLP", "learnable embeddings", "NeMo", "parameter-efficient fine-tuning", "large language models"], "summary_hash": "4d34148946cb", "cached_at": "2026-02-08T11:20:46+00:00"}