{"summary": "The file defines a set of classes and functions that wrap and adapt PyTorch data loading utilities for distributed and accelerated training. It includes a DataLoaderAdapter that inherits from the wrapped DataLoader class for drop-in compatibility, a DataLoaderDispatcher that routes data loading calls to the appropriate device-specific loader, and a DataLoaderShard that handles sharding of data across multiple processes. Additional utilities include a mixin for tracking loader state, a shard wrapper for iterable datasets, a device loader wrapper for XLA devices, a seedable random sampler, and a skip batch sampler for discarding initial batches. The file also provides helper functions for retrieving samplers and preparing data loaders.", "business_intent": "To provide a flexible, extensible, and acceleratorâ€‘aware data loading pipeline that can be easily integrated into distributed training workflows, supporting checkpointing, reproducibility, and efficient data sharding across devices.", "keywords": ["DataLoader", "PyTorch", "accelerator", "distributed training", "sharding", "state tracking", "seedable sampler", "skip batches", "XLA", "checkpointing"], "summary_hash": "303f12473457", "cached_at": "2026-02-09T02:17:48+00:00"}