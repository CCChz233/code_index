{"summary": "Implements a block‑diagonal attention bias where each block enforces causal ordering. Queries can only attend to keys within the same block and only to keys that are not later than the block’s final query, while allowing an unrestricted prefix region. The mask validates that each block contains at least as many keys as queries to avoid undefined softmax results.", "business_intent": "Enable efficient, causally constrained attention in transformer architectures by providing a reusable mask that limits attention to intra‑block positions, supporting models that need block‑wise causal processing with optional non‑causal prefixes.", "keywords": ["attention mask", "block diagonal", "causal attention", "transformer", "FMHA", "query-key restriction", "prefix handling", "softmax safety"], "summary_hash": "7e580333ef59", "cached_at": "2026-02-08T23:23:13+00:00"}