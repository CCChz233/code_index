{"summary": "Provides a torchmetrics‑compatible metric that aggregates loss values across all distributed processes to compute the true average loss during multi‑GPU/TPU training, while preserving gradient flow.", "business_intent": "Enable reliable monitoring and reporting of loss in large‑scale distributed model training, supporting better model evaluation, debugging, and performance tracking.", "keywords": ["distributed training", "global average loss", "torchmetrics", "multi‑process aggregation", "gradient safe", "loss monitoring", "parallel computing"], "summary_hash": "eac2a770c79e", "cached_at": "2026-02-08T10:53:07+00:00"}