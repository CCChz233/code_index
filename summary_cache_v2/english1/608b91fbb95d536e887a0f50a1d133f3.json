{"summary": "Encapsulates the configurable hyperparameters for the Adafactor optimizer, including learning rate, momentum coefficient, epsilon values, weight decay, scaling options, relative step handling, and warm‑up behavior.", "business_intent": "Provides a structured way to set and manage optimizer settings for training neural networks with the memory‑efficient Adafactor algorithm.", "keywords": ["Adafactor", "optimizer", "hyperparameters", "learning rate", "beta1", "epsilon", "weight decay", "scale parameter", "relative step", "warmup", "configuration", "adaptive learning rate", "sublinear memory"], "summary_hash": "b769eaaadfcf", "cached_at": "2026-02-08T10:16:02+00:00"}