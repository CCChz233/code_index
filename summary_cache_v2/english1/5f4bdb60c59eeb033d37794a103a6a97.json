{"summary": "Provides a PyTorch-compatible dataset that prepares and serves examples for fine‑tuning or inference of RETRO language models, handling tokenization, prompt templating, length filtering, optional BOS/EOS tokens, and batch collation with padding and loss‑mask creation.", "business_intent": "Enable developers to efficiently train or evaluate RETRO models on custom data by supplying a ready‑to‑use dataset that respects model token constraints and generates appropriate inputs for loss computation.", "keywords": ["RETRO", "fine-tuning", "dataset", "tokenization", "prompt templates", "sequence length", "padding", "loss mask", "training", "inference", "batch collation"], "summary_hash": "dcf85178e745", "cached_at": "2026-02-08T10:01:46+00:00"}