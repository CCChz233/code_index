{"summary": "Utility module that offers helper routines to transform linear layers into their quantized counterparts based on a model's quantization configuration, supporting FP8, INT8 smooth‑quant, and no‑quantization modes for TensorRT LLM export.", "business_intent": "Facilitate efficient deployment of large language models on TensorRT by providing configurable quantization pathways that reduce memory and compute requirements while preserving model accuracy.", "keywords": ["quantization", "linear layer", "FP8", "INT8", "smooth quant", "TensorRT", "LLM export", "performance optimization", "model compression"], "summary_hash": "c8befa8c2df7", "cached_at": "2026-02-08T11:39:09+00:00"}