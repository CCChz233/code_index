{"summary": "Manages half‑precision (FP16 or BF16) training for Megatron models by providing an autocast context, handling optional gradient scaling, and coordinating optimizer steps while assuming a master‑parameter (FP32) optimizer.", "business_intent": "Accelerate large‑scale model training and reduce memory usage by utilizing half‑precision arithmetic with stable optimizer updates.", "keywords": ["half‑precision", "FP16", "BF16", "mixed precision", "torch.autocast", "GradScaler", "optimizer step", "Megatron", "training acceleration", "GPU efficiency"], "summary_hash": "75cb238290e5", "cached_at": "2026-02-08T09:42:08+00:00"}