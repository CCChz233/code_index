{"summary": "Base class that implements common functionality for non‑fast tokenizers, managing tokenization, special‑token handling, conversion between tokens and IDs, and utilities for downloading, caching and loading pretrained tokenizers, as well as a unified mechanism for adding new tokens to the vocabulary.", "business_intent": "Provide a consistent, extensible tokenizer API for NLP models, simplifying text preprocessing and enabling easy integration of custom tokens and pretrained resources.", "keywords": ["tokenization", "pretrained models", "vocabulary extension", "special tokens", "encoding", "decoding", "caching", "loading", "NLP preprocessing", "slow tokenizer"], "summary_hash": "1dd1da928610", "cached_at": "2026-02-09T06:20:08+00:00"}