{"summary": "Provides low‑level GPU kernels implemented with Numba CUDA to compute the forward (alpha), backward (beta) and gradient terms required for the Recurrent Neural Network Transducer (RNNT) loss, including support for multi‑blank and time‑dependent transition variants, along with helper log‑probability utilities.", "business_intent": "Accelerate RNNT loss calculation during speech‑recognition model training by leveraging parallel GPU execution, thereby improving training speed and scalability.", "keywords": ["RNNT", "GPU", "CUDA", "Numba", "kernel", "forward variables", "backward variables", "gradient", "multi‑blank", "time‑dependent transition", "speech recognition", "ASR", "loss computation", "log probability"], "summary_hash": "ec978a6c592c", "cached_at": "2026-02-08T11:17:06+00:00"}