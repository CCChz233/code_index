{"summary": "This module provides a handler that automatically converts a PyTorch model into a TensorRT-optimized version and integrates it with an Ignite engine, managing precision settings and the model's lifecycle during execution to accelerate inference.", "business_intent": "Enable MONAI users to speed up medical imaging inference pipelines by leveraging TensorRT acceleration within Ignite workflows, reducing latency and increasing throughput without manual compilation steps.", "keywords": ["TensorRT", "model optimization", "Ignite engine", "handler", "precision", "acceleration", "MONAI", "inference", "lifecycle management", "deep learning"], "summary_hash": "0099fa005416", "cached_at": "2026-02-08T13:08:45+00:00"}