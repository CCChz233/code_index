{"summary": "Implements a configurable transformer block that can apply layer normalization before or after attention and feed‑forward layers, supports various self‑ and cross‑attention configurations, and incorporates dropout and adaptive feed‑forward components, mirroring the UniDiffuser architecture.", "business_intent": "Provide a flexible building block for diffusion‑model transformers, allowing developers to tailor normalization placement and attention mechanisms to match UniDiffuser specifications and improve model performance.", "keywords": ["transformer", "layer normalization", "pre-layernorm", "post-layernorm", "self-attention", "cross-attention", "dropout", "feed-forward", "diffusion model", "residual connection", "configurable architecture"], "summary_hash": "290722c859a9", "cached_at": "2026-02-09T04:12:14+00:00"}