{"summary": "Implements a decoder layer for a GLM transformer model, encapsulating self‑attention, cross‑attention, and feed‑forward sub‑layers, and integrates with TensorFlow/Keras model construction and execution.", "business_intent": "Provides a reusable component for building and deploying large language models that generate text, supporting both training and inference workflows.", "keywords": ["transformer", "decoder layer", "GLM", "self-attention", "cross-attention", "feed-forward network", "TensorFlow", "Keras", "neural network", "text generation", "model building", "inference"], "summary_hash": "4744c434e565", "cached_at": "2026-02-09T10:35:53+00:00"}