{"summary": "This module provides a comprehensive test suite for JAX's fused attention StableHLO implementation, verifying the correctness of scaled dot‑product attention across multiple data layouts, mask types, precision modes (including FP8), inference settings, large head dimensions, sliding‑window mechanisms, and sharding configurations.", "business_intent": "Validate and ensure reliable functionality of high‑performance attention kernels used in deep learning models, supporting low‑precision inference and diverse execution environments.", "keywords": ["attention", "dot-product", "FP8", "JAX", "StableHLO", "cuDNN", "unit testing", "masking", "causal", "padding", "sliding window", "sharding", "inference", "large head"], "summary_hash": "e2c684035b6e", "cached_at": "2026-02-09T12:03:03+00:00"}