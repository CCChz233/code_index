{"summary": "In-memory dataset class that prepares source‑target pairs for T5 supervised fine‑tuning, converting inputs from the GPT‑style SFT format into the sequence‑to‑sequence representation required by the model and providing standard indexing, length, and batch collation methods.", "business_intent": "Facilitate efficient loading, preprocessing, and batching of T5 fine‑tuning data stored in memory, enabling training pipelines that need source‑target handling compatible with GPT‑style supervised fine‑tuning datasets.", "keywords": ["T5", "sequence-to-sequence", "in-memory dataset", "supervised fine-tuning", "GPT SFT format", "source processing", "target processing", "collate function", "PyTorch DataLoader", "training data preparation"], "summary_hash": "144212d213e8", "cached_at": "2026-02-08T10:02:11+00:00"}