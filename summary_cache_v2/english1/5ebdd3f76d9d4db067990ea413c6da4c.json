{"summary": "Provides a transformer decoder component for Megatron retrieval models, handling attention mask creation, forward computation, and checkpointable state management.", "business_intent": "Support scalable, retrievalâ€‘augmented generation in large language model deployments.", "keywords": ["transformer", "decoder", "attention mask", "forward pass", "state dict", "checkpoint", "Megatron", "retrieval", "deep learning", "model parallelism"], "summary_hash": "c20992f94ee4", "cached_at": "2026-02-08T09:49:27+00:00"}