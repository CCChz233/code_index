{"summary": "Implements a flash‑attention forward operator with split‑K parallelism that can consume quantized key/value tensors (int4 with optional group‑wise layout or fp8 row‑wise). It handles dequantization, reshapes inputs for various bias configurations (including paged attention), and applies multi‑query tricks before invoking the low‑level kernel.", "business_intent": "Enable efficient, low‑precision attention computation with split‑K parallelism, supporting various quantized formats and bias types for high‑throughput transformer inference.", "keywords": ["flash attention", "split‑K", "quantized", "int4", "fp8", "dequantization", "bias", "paged attention", "multi‑query", "kernel"], "summary_hash": "4568cfceb0fe", "cached_at": "2026-02-08T23:33:17+00:00"}