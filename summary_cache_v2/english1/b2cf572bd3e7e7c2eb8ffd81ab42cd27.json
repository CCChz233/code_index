{"summary": "Implements a self‑attention layer that restricts attention to a local neighbourhood of each token, handling mask creation, extraction of relevant hidden states, and the forward computation.", "business_intent": "Enable efficient processing of long sequences in transformer models by limiting attention scope, reducing memory and compute while preserving contextual information.", "keywords": ["self‑attention", "local window", "transformer", "attention mask", "sequence modeling", "neural network", "efficient computation"], "summary_hash": "da5a1ac4cace", "cached_at": "2026-02-09T08:31:18+00:00"}