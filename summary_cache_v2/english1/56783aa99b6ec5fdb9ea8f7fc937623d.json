{"summary": "Provides a configurable abstraction for attention feature maps and implements random‑feature based softmax kernel approximations, enabling linear‑time attention calculations in transformer models.", "business_intent": "Accelerate transformer training and inference by reducing the quadratic cost of softmax attention to linear complexity, improving scalability and performance in real‑world applications.", "keywords": ["attention", "feature maps", "random features", "softmax approximation", "linear attention", "transformer efficiency", "orthogonal random features", "hyperbolic kernel", "configuration", "dataclass"], "summary_hash": "72c7d6c4de90", "cached_at": "2026-02-08T23:34:58+00:00"}