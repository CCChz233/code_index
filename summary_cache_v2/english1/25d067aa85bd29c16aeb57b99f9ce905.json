{"summary": "A neural module that receives token representations from a BERT encoder and outputs per-token classification scores used during BERT pretraining.", "business_intent": "Provides the core functionality needed to train or fine-tune BERT models on token-level pretraining objectives, improving language understanding for downstream NLP applications.", "keywords": ["BERT", "token classification", "pretraining", "masked language modeling", "deep learning", "neural network", "PyTorch"], "summary_hash": "fb972e02fa19", "cached_at": "2026-02-08T09:44:09+00:00"}