{"summary": "Defines the abstract data source model and concrete implementations (Kafka, Kinesis, push, request) for Feast, handling configuration, schema inspection, temporal field handling, validation, and protobuf serialization.", "business_intent": "Allow feature‑store users to declare, configure, and validate various batch and streaming sources of feature data, enabling correct ingestion, point‑in‑time joins, and integration with the Feast ecosystem.", "keywords": ["feature store", "data source", "Kafka", "Kinesis", "streaming", "schema", "validation", "protobuf", "temporal join", "deduplication", "configuration", "push source", "request source"], "summary_hash": "821b0e03a7eb", "cached_at": "2026-02-09T00:18:13+00:00"}