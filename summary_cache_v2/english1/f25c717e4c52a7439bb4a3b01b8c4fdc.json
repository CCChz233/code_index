{"summary": "Defines a base representation for attention masks that are generated on‑the‑fly inside the kernel via a user‑provided callable, storing shape, offset, query indices and the mask function for fast indexed access.", "business_intent": "Enable high‑performance, dynamic computation of attention masks in deep‑learning kernels, reducing memory traffic and supporting flexible offset handling for causal or sliding‑window attention patterns.", "keywords": ["attention mask", "kernel computation", "callable mask function", "offset handling", "shape metadata", "query sequence", "performance optimization", "dynamic mask generation"], "summary_hash": "b518bb6c224d", "cached_at": "2026-02-09T11:49:08+00:00"}