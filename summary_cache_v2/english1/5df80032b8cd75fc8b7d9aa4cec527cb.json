{"summary": "Represents a lazily evaluated full attention mask where every token can attend to every other token, providing lightweight handling of the mask without materializing a dense matrix.", "business_intent": "Facilitate the creation and use of unrestricted attention patterns in transformer-like models, improving code clarity and memory efficiency when a full mask is required.", "keywords": ["attention mask", "full mask", "lazy evaluation", "transformer", "token attention", "shape helper", "equality", "hashability"], "summary_hash": "d7cb2f9e6c54", "cached_at": "2026-02-09T11:49:20+00:00"}