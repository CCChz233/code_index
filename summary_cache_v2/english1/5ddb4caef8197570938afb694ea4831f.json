{"summary": "Implements a flash‑attention forward operator with split‑K parallelism that can consume quantized key/value tensors (int4 with optional group‑wise layout or fp8 row‑wise). It handles dequantization, reshapes inputs for various bias configurations (including paged attention), and applies multi‑query tricks before invoking the low‑level kernel.", "business_intent": "Accelerate transformer attention layers by reducing memory bandwidth and compute through fused quantized K/V handling and split‑K execution, enabling faster, more memory‑efficient inference and training at scale.", "keywords": ["flash attention", "split-k", "int4 quantization", "fp8 quantization", "key/value dequantization", "paged attention", "multiquery", "kernel reshaping", "high performance", "transformer optimization"], "summary_hash": "4e7458558114", "cached_at": "2026-02-08T23:24:20+00:00"}