{"summary": "A configuration container for DinatModel that encapsulates all architectural hyperparameters such as patch size, channel count, embedding dimension, encoder depths, attention heads, kernel size, dilations, MLP ratio, bias flags, dropout rates, activation function, initialization scale, layer‑norm epsilon, layer‑scale value, and optional backbone output specifications. It inherits from PretrainedConfig, enabling seamless model creation and reproducibility.", "business_intent": "Provide developers a flexible, serializable way to define and modify the Dinat model architecture and its output behavior, facilitating model instantiation, fine‑tuning, and integration with pretrained checkpoints.", "keywords": ["configuration", "Dinat", "vision transformer", "patch embedding", "attention", "dropout", "layer normalization", "hyperparameters", "backbone", "output features"], "summary_hash": "6bc9f5510751", "cached_at": "2026-02-09T11:10:56+00:00"}