{"summary": "The optimizers package supplies a collection of training-time utilities for MONAI models, including a learning-rate range test, a suite of custom learning-rate schedulers (exponential, linear, warm-up cosine), an implementation of the Novograd adaptive optimizer, and helper functions to build optimizer parameter groups based on user-defined filters.", "business_intent": "Enable researchers and developers to more easily tune and accelerate the training of medical imaging deep‑learning models by providing ready‑to‑use learning-rate strategies and optimizer configurations that improve convergence and stability.", "keywords": ["optimizer", "learning rate scheduler", "learning rate finder", "Novograd", "parameter groups", "PyTorch", "medical imaging", "training utilities", "adaptive gradient", "warmup cosine decay", "exponential decay", "linear decay"], "summary_hash": "4ff39fc34451", "cached_at": "2026-02-08T13:29:10+00:00"}