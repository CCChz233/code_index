{"summary": "Represents a lazily evaluated 2â€‘D attention mask that limits each query token to attend only to key/value tokens within a configurable local window, optionally shifted by an offset.", "business_intent": "Provide an efficient mechanism for generating local attention masks in neural network models, reducing computation and memory usage while enforcing the desired attention scope.", "keywords": ["local attention", "mask", "window size", "offset", "lazy evaluation", "transformer", "sequence", "shape", "performance"], "summary_hash": "a7c33e223161", "cached_at": "2026-02-09T11:49:15+00:00"}