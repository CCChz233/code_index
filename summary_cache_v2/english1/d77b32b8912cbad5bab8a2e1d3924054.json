{"summary": "Defines neural modules that take token embeddings from a language encoder and produce per‑token classification scores, supporting both generic token‑level tasks (e.g., NER) and the masked token prediction head used during BERT pre‑training.", "business_intent": "Facilitate the development of token‑wise classification models within the NeMo ecosystem, allowing users to fine‑tune or pre‑train language models for tasks such as named‑entity recognition, part‑of‑speech tagging, and BERT masked‑token objectives.", "keywords": ["token classification", "BERT pretraining", "named entity recognition", "per-token logits", "NeMo", "NLP", "neural module", "PyTorch", "MLP head", "language model fine‑tuning"], "summary_hash": "a3437ed59f2e", "cached_at": "2026-02-08T11:20:52+00:00"}