{"summary": "Provides a custom CLIP tokenizer that lets users refer to a concept with a single placeholder token, which is automatically expanded into multiple underlying tokens. It handles token addition, placeholder replacement during encoding, gradient masking, and saving/loading of embeddings for both training and inference.", "business_intent": "Streamline the workflow for creating and using multi-token textual inversion embeddings in CLIP models, reducing manual token management and enabling seamless integration into training and inference pipelines.", "keywords": ["CLIP", "tokenizer", "multi-token", "textual inversion", "placeholder token", "embedding management", "gradient mask", "training", "inference"], "summary_hash": "4cf9d51492f3", "cached_at": "2026-02-09T05:06:10+00:00"}