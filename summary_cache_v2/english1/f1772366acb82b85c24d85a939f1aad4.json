{"summary": "Provides a causal attention mask that mirrors a lower‑triangular mask but is offset so the final query can attend to the final key when query and key lengths differ, preventing attention to future positions.", "business_intent": "Supports transformer‑based models in autoregressive or decoding contexts where query and key sequence lengths vary, ensuring proper causal masking to maintain temporal order and prevent information leakage.", "keywords": ["causal mask", "lower triangular", "attention", "transformer", "query-key length mismatch", "autoregressive decoding", "mask generation", "triangular mask shift", "future token blocking"], "summary_hash": "a07d908ab959", "cached_at": "2026-02-08T23:22:45+00:00"}