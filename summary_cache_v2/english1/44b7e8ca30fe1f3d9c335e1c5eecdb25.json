{"summary": "Provides transformer layer classes that automatically cast tensors to the most suitable precision for mixed‑precision training, handling forward computation, offset calculations, and sharded state management, along with a utility to retrieve the layer specification.", "business_intent": "Facilitate efficient, memory‑optimized training of large GPT models by integrating automatic mixed‑precision and tensor‑parallelism within the NeMo Megatron framework.", "keywords": ["transformer", "autocast", "mixed precision", "GPT", "Megatron", "NeMo", "language modeling", "tensor parallelism", "sharded state", "performance optimization"], "summary_hash": "d70634148224", "cached_at": "2026-02-08T11:38:18+00:00"}