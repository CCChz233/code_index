{"summary": "A neural‑network layer that adds L1 and/or L2 activity regularization terms to the loss function while passing the input unchanged, keeping the same output shape as the input.", "business_intent": "Enable models to achieve better generalization and sparsity by penalizing large or numerous activations during training, reducing over‑fitting.", "keywords": ["regularization", "L1", "L2", "activity penalty", "neural network layer", "loss augmentation", "model generalization", "sparsity"], "summary_hash": "b2c025d05cf4", "cached_at": "2026-02-09T12:01:30+00:00"}