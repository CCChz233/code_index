{"summary": "Utility functions that support attention mechanisms by handling mask transformations (e.g., reshaping key‑padding masks, converting boolean masks to additive forms, merging multiple masks) and providing linear‑algebra helpers such as an iterative pseudo‑inverse computation.", "business_intent": "Enable flexible and efficient implementation of transformer‑style attention layers by offering reusable mask‑processing and matrix‑operation utilities.", "keywords": ["attention", "mask", "padding mask", "boolean mask", "additive mask", "mask merging", "pseudo-inverse", "iterative", "reshape", "transformer", "utility"], "summary_hash": "2d55e96975d3", "cached_at": "2026-02-08T23:31:26+00:00"}