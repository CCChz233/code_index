{"summary": "Implements rotary positional embeddings for transformer models, generating and caching cosine and sine tables and applying them to input tensors during the forward computation.", "business_intent": "Provide an efficient and reusable component for adding positional information to token representations, enhancing model accuracy and speed in natural language processing applications.", "keywords": ["rotary embedding", "positional encoding", "cosine cache", "sine cache", "transformer", "sequence modeling", "phi model", "tensor transformation", "performance optimization"], "summary_hash": "5720a7c9e379", "cached_at": "2026-02-09T08:32:58+00:00"}