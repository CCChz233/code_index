{"summary": "Implements a NEZHA transformer encoder layer with support for chunked feed‑forward processing.", "business_intent": "Provide an efficient, memory‑optimized transformer block for training and inference of large language models.", "keywords": ["NEZHA", "transformer", "encoder layer", "chunked feed‑forward", "deep learning", "natural language processing", "memory efficiency", "neural network"], "summary_hash": "f9102d996214", "cached_at": "2026-02-09T08:15:36+00:00"}