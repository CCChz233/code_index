{"summary": "Encapsulates the XLM transformer model, managing its initialization, input embeddings, forward computation, and optional pruning of attention heads.", "business_intent": "Provide a ready-to-use multilingual language model for cross‑lingual NLP tasks such as translation, classification, and representation learning.", "keywords": ["XLM", "multilingual", "transformer", "language model", "embeddings", "pruning heads", "forward pass", "NLP", "cross‑lingual"], "summary_hash": "7b1ada2c0cdc", "cached_at": "2026-02-09T10:39:27+00:00"}