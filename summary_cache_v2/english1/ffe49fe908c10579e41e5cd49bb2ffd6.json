{"summary": "Implements the BERT pooling layer that extracts the [CLS] token embedding, applies a linear transformation followed by a tanh activation, and returns a fixed-size pooled representation for downstream processing.", "business_intent": "Enables downstream NLP applications such as sentence classification, sentiment analysis, or any task requiring a condensed representation of a sequence by providing a ready-to-use BERT pooler component.", "keywords": ["BERT", "pooling layer", "transformer", "CLS token", "dense projection", "tanh activation", "PyTorch", "NLP", "sentence representation", "classification"], "summary_hash": "7292d7aa5f7e", "cached_at": "2026-02-08T11:41:59+00:00"}