{"summary": "A Flax implementation of a Performer‑based transformer that can operate as an encoder using only self‑attention or as a decoder by inserting cross‑attention layers between self‑attention blocks.", "business_intent": "Provide an efficient, flexible neural architecture for sequence‑to‑sequence and representation learning tasks, enabling fast training and inference in JAX environments.", "keywords": ["Flax", "Performer", "Transformer", "self‑attention", "cross‑attention", "encoder", "decoder", "JAX", "efficient attention", "sequence modeling"], "summary_hash": "6a0c6e5d76c5", "cached_at": "2026-02-09T06:00:51+00:00"}