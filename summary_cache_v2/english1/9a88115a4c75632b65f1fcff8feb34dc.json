{"summary": "Implements the AnswerCorrectness metric, which evaluates how accurate a generated answer is compared to a reference by combining factual faithfulness and semantic similarity. The metric prepares simplified statements from the reference, checks their presence in the generated answer, leverages an AnswerSimilarity helper, and aggregates per‑turn scores into a final correctness value.", "business_intent": "Enable developers and data scientists to quantitatively assess the quality of LLM‑generated answers in retrieval‑augmented generation pipelines, supporting quality control, model benchmarking, and improvement of conversational AI systems.", "keywords": ["answer correctness", "metric", "factual accuracy", "semantic similarity", "RAG evaluation", "LLM", "question answering", "score aggregation", "faithfulness", "similarity assessment"], "summary_hash": "93958b5e9e06", "cached_at": "2026-02-08T22:50:03+00:00"}