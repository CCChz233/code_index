{"summary": "A configurable transformer backbone that can operate as a pure encoder using self‑attention or as a decoder by inserting cross‑attention layers, supporting optional encoder‑decoder interaction for sequence‑to‑sequence applications.", "business_intent": "Supply a versatile neural component for building NLP systems such as translation, summarization, or any task requiring encoding of input text and conditional generation, while allowing model size reduction through head pruning.", "keywords": ["transformer", "self-attention", "cross-attention", "encoder", "decoder", "seq2seq", "language modeling", "head pruning", "embeddings"], "summary_hash": "089394451732", "cached_at": "2026-02-09T08:15:59+00:00"}