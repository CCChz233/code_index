{"summary": "Implements a rotary positional embedding mechanism with linear scaling for GPTNeoX models, extending the base rotary embedding to adjust positional representations proportionally across sequence lengths.", "business_intent": "Improve the accuracy and flexibility of transformer-based language models by providing scalable positional encodings that can be efficiently cached and reused during inference and training.", "keywords": ["rotary embedding", "linear scaling", "positional encoding", "GPTNeoX", "transformer", "NLP", "cosine sine cache", "embedding scaling"], "summary_hash": "eedc82b5c175", "cached_at": "2026-02-09T08:27:56+00:00"}