{"summary": "Implements a rotary positional embedding with dynamic NTK scaling to adapt cosine‑sine positional encodings for varying sequence lengths.", "business_intent": "Provides transformers with scalable positional representations that preserve performance on long or variable‑length inputs, supporting applications like language modeling, text generation, and other sequence‑based AI tasks.", "keywords": ["rotary embedding", "dynamic NTK scaling", "positional encoding", "cosine sine cache", "transformer", "phi model", "neural tangent kernel", "sequence length adaptation", "embedding cache"], "summary_hash": "ad44b46b3730", "cached_at": "2026-02-09T08:33:06+00:00"}