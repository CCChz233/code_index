{"summary": "A structured container that holds all relevant outputs from a sequence‑to‑sequence time‑series model, including the decoder's final hidden state, cached attention key/value tensors for fast incremental decoding, optional hidden states and attention maps from both encoder and decoder layers, as well as preprocessing parameters (loc, scale) and static covariates.", "business_intent": "Facilitate efficient inference and post‑processing for forecasting applications by providing quick access to model internals and enabling cache‑based decoding, while preserving scaling information and static features needed for accurate time‑series predictions.", "keywords": ["time series", "seq2seq", "model output", "hidden states", "past key values", "caching", "decoder", "encoder", "attention", "scaling", "static features", "PyTorch", "forecasting"], "summary_hash": "084354c1f8be", "cached_at": "2026-02-09T06:30:09+00:00"}