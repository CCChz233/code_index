{"summary": "Defines metrics that compute relevance scores for generated answers or responses in a single‑turn RAG setting, using language model prompts and embedding similarity.", "business_intent": "Enable developers to quantitatively evaluate and improve the relevance of AI‑generated answers within retrieval‑augmented generation pipelines.", "keywords": ["answer relevance", "response relevance", "metric", "RAG evaluation", "LLM scoring", "embedding similarity", "single turn sample", "quality assessment"], "summary_hash": "7a9c549ac59f", "cached_at": "2026-02-08T22:50:32+00:00"}