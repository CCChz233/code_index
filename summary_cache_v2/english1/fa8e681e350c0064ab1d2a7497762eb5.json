{"summary": "Implements the squared ReLU activation (max(0, x)²) as a callable module for neural network layers, applying the operation element‑wise to input tensors.", "business_intent": "Offer a ready‑to‑use non‑linear activation that can be integrated into deep learning models to leverage the performance benefits reported for the squared ReLU variant in research and production applications.", "keywords": ["relu squared", "activation function", "neural network", "deep learning", "non-linear"], "summary_hash": "ac2c91003ad3", "cached_at": "2026-02-09T06:23:46+00:00"}