{"summary": "The module provides dataset classes for handling video‑to‑text data in speech and computer‑vision tasks. It includes a tar‑based dataset that streams video files from tar archives using a JSON manifest, a manifest processor that parses and filters video entries, and two concrete dataset implementations that read line‑by‑line JSON manifests to load video files and their transcripts. These datasets support duration‑based filtering, optional silence trimming, channel selection, text tokenization (BPE, WordPiece, or character‑level), BOS/EOS token insertion, padding, and optional return of sample IDs. They are designed for use with PyTorch and NeMo, facilitating efficient data loading for training and inference of multimodal speech‑recognition models.", "business_intent": "To enable efficient loading, preprocessing, and tokenization of video‑to‑text data for training multimodal speech‑recognition models within the NeMo framework, supporting distributed training and flexible data augmentation.", "keywords": ["dataset", "video", "speech recognition", "manifest", "BPE", "character tokenization", "duration filtering", "silence trimming", "channel selection", "distributed training"], "summary_hash": "843c707602da", "cached_at": "2026-02-08T10:59:26+00:00"}