{"summary": "Collates and preprocesses batches for sequence-to-sequence models, converting raw examples into tokenized tensors and preparing decoder inputs with appropriate shifting.", "business_intent": "Facilitate efficient training and inference of seq2seq architectures by providing ready‑to‑use batched data structures.", "keywords": ["seq2seq", "data collator", "batch preparation", "token encoding", "decoder shift", "T5 compatibility", "training pipeline", "tensor conversion"], "summary_hash": "a4e9cbd1f936", "cached_at": "2026-02-09T06:04:50+00:00"}