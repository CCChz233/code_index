{"summary": "Implements a configurable encoder‑decoder sequence‑to‑sequence model that supports multiple lattice‑based loss functions, allowing dynamic vocabulary updates and providing utilities for forward inference and model discovery.", "business_intent": "Help developers create and experiment with advanced seq2seq architectures for tasks like machine translation, speech recognition, or text generation, offering flexible loss configurations and easy switching between model variants.", "keywords": ["encoder‑decoder", "sequence‑to‑sequence", "lattice loss", "vocabulary management", "model registry", "neural network", "deep learning", "NLP", "speech recognition", "machine translation"], "summary_hash": "7cb9d9c59c06", "cached_at": "2026-02-08T09:19:22+00:00"}