{"summary": "Implements a rotary positional embedding with linear scaling of rotation angles, extending the base PhiRotaryEmbedding and maintaining cached cosine and sine tables for fast computation.", "business_intent": "Enable transformer models to use scalable, efficient positional encodings that improve performance on longer sequences while minimizing runtime overhead.", "keywords": ["rotary embedding", "linear scaling", "positional encoding", "transformer", "cosine cache", "sine cache", "phi model"], "summary_hash": "b2b2aa110836", "cached_at": "2026-02-09T08:33:02+00:00"}