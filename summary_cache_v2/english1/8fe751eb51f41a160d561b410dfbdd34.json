{"summary": "Implements a masked language modeling (MLM) head tailored for the NEZHA transformer architecture, converting encoder hidden states into token prediction logits.", "business_intent": "Enable pre‑training and fine‑tuning of NEZHA‑based language models on MLM tasks, facilitating token‑level prediction for downstream NLP applications.", "keywords": ["masked language modeling", "NEZHA", "transformer head", "token prediction", "pretraining", "fine‑tuning", "NLP"], "summary_hash": "ea82ea3eb424", "cached_at": "2026-02-09T08:15:48+00:00"}