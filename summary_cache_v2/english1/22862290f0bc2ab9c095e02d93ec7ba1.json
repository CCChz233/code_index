{"summary": "Provides a high‑level handler that orchestrates caching for large language model API calls. It abstracts over various cache backends (Redis, S3, Qdrant, in‑memory, disk) and offers asynchronous get/set operations. The handler determines the appropriate cache based on user configuration, processes different response types (completion, embedding, text completion, transcription, streaming), and integrates detailed logging and response assembly.", "business_intent": "Reduce latency and API costs by reusing prior LLM responses, ensuring efficient storage and retrieval across multiple caching solutions, and supporting both synchronous and asynchronous workflows for diverse LLM request types.", "keywords": ["caching", "LLM", "asynchronous", "Redis", "S3", "Qdrant", "in‑memory cache", "disk cache", "embedding", "completion", "transcription", "streaming", "performance optimization", "response handling", "wrapper"], "summary_hash": "bb50c78db772", "cached_at": "2026-02-08T07:46:50+00:00"}