{"summary": "Provides helper functionality to apply the Scaled Exponential Linear Unit (SELU) activation and compute its output specifications for neural network layers.", "business_intent": "Facilitates the use of SELU activation in selfâ€‘normalizing deep learning models, simplifying integration and ensuring correct output shape handling.", "keywords": ["SELU", "activation function", "neural network", "helper", "output specification", "deep learning"], "summary_hash": "257e92146a36", "cached_at": "2026-02-09T11:34:11+00:00"}