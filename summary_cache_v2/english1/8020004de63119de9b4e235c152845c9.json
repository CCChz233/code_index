{"summary": "Implements a LoRA (Low-Rank Adaptation) key‑value adapter that mirrors the structure of standard adapters but allows flexible input and output dimensions and omits the bottleneck activation, enabling lightweight, parameter‑efficient model fine‑tuning.", "business_intent": "Provide a modular component for efficiently adapting large neural models to new tasks or domains with minimal additional parameters, reducing training cost and deployment overhead.", "keywords": ["LoRA", "adapter", "low-rank", "parameter-efficient fine-tuning", "neural network", "key-value", "flexible dimensions", "no bottleneck activation"], "summary_hash": "bd5248400f70", "cached_at": "2026-02-08T09:51:18+00:00"}