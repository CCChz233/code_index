{"summary": "A processor that implements sliced multi-head attention for a text encoder, augmenting the attention mechanism with additional learnable key and value matrices while dividing the computation into fixed-size slices for memory efficiency.", "business_intent": "Enable scalable and memory‑efficient attention processing in large language or diffusion models by providing a modular component that adds trainable key/value projections and supports slice‑based computation.", "keywords": ["sliced attention", "learnable key value", "text encoder", "processor", "slice size", "memory efficiency", "transformer", "attention head dimension"], "summary_hash": "6702ca8b537e", "cached_at": "2026-02-09T04:06:58+00:00"}