{"summary": "Implements a metric that calculates ROUGE scores to quantify the similarity between a generated response and its reference text within the RAG evaluation framework, handling initialization and per‑turn scoring for single‑turn samples.", "business_intent": "Provide an automated way to assess the quality of generated answers in retrieval‑augmented generation systems by measuring lexical overlap with ground‑truth references.", "keywords": ["ROUGE", "text similarity", "evaluation metric", "generated response", "reference text", "RAG", "single turn", "metric computation", "quality assessment"], "summary_hash": "8d05c3a4e840", "cached_at": "2026-02-08T22:49:59+00:00"}