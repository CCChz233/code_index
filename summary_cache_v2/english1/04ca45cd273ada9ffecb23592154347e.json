{"summary": "Provides a memory‑optimized implementation of the Swish activation function for neural network training, delivering roughly a 30 % reduction in GPU memory usage compared to the standard Swish layer.", "business_intent": "Enable more efficient model training by lowering memory consumption, allowing larger batch sizes or deeper architectures without additional hardware costs.", "keywords": ["Swish", "activation function", "memory efficient", "training optimization", "neural networks", "PyTorch", "forward pass", "backward pass"], "summary_hash": "f98d71cd5f78", "cached_at": "2026-02-08T11:48:56+00:00"}