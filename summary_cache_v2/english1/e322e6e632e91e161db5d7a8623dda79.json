{"summary": "Initializes and registers the fused multi‑head attention (FMHA) operation suite, exposing high‑performance forward and backward attention kernels and utilities for bias handling, attention merging, and memory‑efficient attention variants. It selects and dispatches the appropriate GPU implementation (e.g., cutlass, flash, triton) based on the runtime environment.", "business_intent": "Supply fast, GPU‑accelerated attention primitives for transformer models, enabling efficient training and inference by leveraging specialized kernels and memory‑efficient strategies.", "keywords": ["fused multi-head attention", "attention kernels", "GPU acceleration", "forward backward", "memory efficient", "kernel dispatch", "bias handling", "attention merging", "transformer optimization"], "summary_hash": "f57947e63e0b", "cached_at": "2026-02-08T23:32:53+00:00"}