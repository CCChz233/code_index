{"summary": "Encapsulates a single transformer decoder block used in the Whisper speech‑to‑text model, integrating self‑attention, cross‑attention, feed‑forward processing, normalization and dropout to transform encoder outputs into decoder representations.", "business_intent": "Provides a reusable, modular component for constructing and deploying Whisper‑based speech recognition systems, supporting both training and inference of the decoder portion of the model.", "keywords": ["decoder layer", "transformer", "self-attention", "cross-attention", "feed-forward network", "layer normalization", "dropout", "Whisper", "speech recognition"], "summary_hash": "cdf2389c4f3e", "cached_at": "2026-02-09T10:54:43+00:00"}