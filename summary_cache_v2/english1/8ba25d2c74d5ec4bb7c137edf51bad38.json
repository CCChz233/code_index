{"summary": "A distribution class that shards model variables across a configurable mesh of devices using a layout map, enabling model parallelism alongside data parallelism and reducing perâ€‘device memory usage.", "business_intent": "Provide a scalable way to train large neural networks on multiple accelerators by partitioning weights, lowering memory footprint, and improving parallel execution.", "keywords": ["model parallelism", "variable sharding", "device mesh", "layout map", "distributed training", "memory reduction", "tensor layout", "multi-device scaling"], "summary_hash": "4f5d53a4075f", "cached_at": "2026-02-09T11:28:51+00:00"}