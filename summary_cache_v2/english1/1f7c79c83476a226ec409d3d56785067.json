{"summary": "This package implements the Databricks backend for the litellm library, offering unified interfaces for chat completion and embedding calls to Databricks foundation models. It translates OpenAI‑style parameters, manages synchronous and asynchronous HTTP communication, streams partial results, formats responses into litellm’s standard structures, computes token‑based pricing, and defines specialized error handling for Databricks service failures.", "business_intent": "Provide developers with a seamless, cost‑aware, and robust way to integrate Databricks large language models into applications through the litellm abstraction layer, supporting both regular and streaming interactions while delivering clear error diagnostics.", "keywords": ["databricks", "large language model", "llm integration", "chat completion", "embeddings", "cost calculation", "token pricing", "exception handling", "streaming", "asynchronous", "API wrapper", "litellm"], "summary_hash": "031746fed8a7", "cached_at": "2026-02-08T08:08:59+00:00"}