{"summary": "A neural‑network layer that normalizes its inputs by centering and scaling them. During training it uses the current batch's mean and variance, while during inference it relies on exponentially‑averaged moving statistics. It optionally learns per‑channel scale (gamma) and offset (beta) parameters, supports configurable axis, momentum, epsilon, and can synchronize statistics across distributed devices.", "business_intent": "Stabilize and accelerate deep model training by mitigating internal covariate shift, enabling higher learning rates and deeper architectures, and providing consistent behavior during inference.", "keywords": ["batch normalization", "input normalization", "moving mean", "moving variance", "training mode", "inference mode", "scale parameter", "offset parameter", "momentum", "epsilon", "axis", "synchronization", "deep learning layer"], "summary_hash": "14670ea0728e", "cached_at": "2026-02-09T11:58:23+00:00"}