{"summary": "CamembertModel implements a configurable Transformer architecture that can function as a self‑attention encoder or, when decoder mode is activated, as a decoder with an added cross‑attention layer, supporting seq2seq usage by accepting encoder hidden states. It handles input embeddings and offers utilities such as head pruning.", "business_intent": "Provide a versatile French language model that can be employed for both understanding and generation tasks, allowing developers to reuse the same model for classification, translation, summarization, and other NLP applications by switching between encoder, decoder, or encoder‑decoder configurations.", "keywords": ["Transformer", "encoder", "decoder", "cross-attention", "self-attention", "Camembert", "French language model", "seq2seq", "embeddings", "configurable architecture"], "summary_hash": "dad1b7e833be", "cached_at": "2026-02-09T10:05:17+00:00"}