{"summary": "Encapsulates a cache entry for a transformer layer, storing the attention key and value tensors and offering simple helpers to initialize the entry and append additional data.", "business_intent": "Facilitate fast incremental inference by maintaining and updating cached key/value states for transformer attention, reducing recomputation across successive steps.", "keywords": ["transformer", "cache", "key", "value", "entry", "append", "initialize", "attention", "incremental inference", "stateful"], "summary_hash": "923a6331646f", "cached_at": "2026-02-08T13:17:47+00:00"}