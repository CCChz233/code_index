{"summary": "Constructs trainer configurations for Megatron pipeline‑parallel language models with gradient scaling disabled, streamlining setup for fine‑tuning scenarios such as PEFT.", "business_intent": "Provide a ready‑made builder that simplifies and standardizes the creation of training pipelines for large language models, reducing overhead for developers who need to run pipeline‑parallel training without gradient scaling and accelerate fine‑tuning workflows.", "keywords": ["Megatron", "pipeline parallel", "language model", "trainer builder", "gradient scaler off", "PEFT tuning", "configuration", "large model training"], "summary_hash": "205e1c850eb0", "cached_at": "2026-02-08T09:41:29+00:00"}