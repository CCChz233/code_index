{"summary": "Implements a scaled dot‑product attention processor tailored for the LuminaNextDiT model, leveraging PyTorch 2.0's built‑in attention kernel. The processor normalizes inputs and adds rotary positional embeddings to the query and key tensors before computing attention.", "business_intent": "Provides a high‑performance, plug‑in attention component that accelerates transformer‑based image generation pipelines while preserving positional information, enabling faster and more accurate inference for LuminaNextDiT deployments.", "keywords": ["scaled dot-product attention", "PyTorch 2.0", "rotary embedding", "normalization layer", "LuminaNextDiT", "transformer", "attention processor", "high performance inference"], "summary_hash": "06f112f27084", "cached_at": "2026-02-09T04:06:39+00:00"}