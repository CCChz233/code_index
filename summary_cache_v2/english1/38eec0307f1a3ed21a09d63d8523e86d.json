{"summary": "Defines modular bottleneck-style transformer encoder and decoder components that assemble various subâ€‘encoders (bridge, perceiver, pooling) and configure transformer layers, exposing model architecture and compatible settings for NeMo NLP pipelines.", "business_intent": "Enable efficient, configurable transformer architectures with a bottleneck representation to improve performance and flexibility in natural language processing models built with NVIDIA NeMo.", "keywords": ["transformer", "bottleneck", "encoder", "decoder", "NLP", "NeMo", "bridge encoder", "perceiver encoder", "pooling encoder", "neural types", "mask handling", "configurable architecture"], "summary_hash": "24336c56c3e1", "cached_at": "2026-02-08T11:23:00+00:00"}