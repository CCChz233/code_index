{"summary": "Encapsulates the logic for computing cross‑attention within the LEDITS model, providing an initialized processor that can be invoked to produce attention outputs from query, key, and value tensors.", "business_intent": "Offer a modular, reusable component that streamlines cross‑attention calculations for multimodal or transformer‑based applications, enhancing code clarity and runtime efficiency.", "keywords": ["cross-attention", "processor", "transformer", "neural network", "attention mechanism", "modular component", "inference", "multimodal AI"], "summary_hash": "b12d00f1c597", "cached_at": "2026-02-09T04:18:51+00:00"}