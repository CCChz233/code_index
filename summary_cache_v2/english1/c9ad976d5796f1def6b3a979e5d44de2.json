{"summary": "A neural network module that applies the SiLU (Swish) activation after converting inputs to 32‑bit floating point precision.", "business_intent": "Enable reliable SiLU activation in mixed‑precision pipelines by performing the computation in FP32, improving numerical stability for training and inference.", "keywords": ["SiLU", "Swish", "activation", "float32", "upcast", "torch", "neural network", "mixed precision", "FP32", "stability"], "summary_hash": "fae25b3d3dd1", "cached_at": "2026-02-09T04:04:55+00:00"}