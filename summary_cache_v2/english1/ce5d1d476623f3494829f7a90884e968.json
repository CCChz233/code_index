{"summary": "A metric class that accumulates batch-wise Kullback-Leibler divergence between true and predicted probability distributions, applying clipping and optional sample weighting, and exposing methods to update, reset, and retrieve the aggregated result.", "business_intent": "Enable model developers to quantify the information loss between target and predicted distributions, facilitating performance monitoring and optimization during training and evaluation.", "keywords": ["Kullback-Leibler divergence", "KL divergence", "probability distribution", "metric", "model evaluation", "sample weighting", "stateful metric", "TensorFlow", "Keras", "statistical distance"], "summary_hash": "808f66d3c75e", "cached_at": "2026-02-09T11:43:52+00:00"}