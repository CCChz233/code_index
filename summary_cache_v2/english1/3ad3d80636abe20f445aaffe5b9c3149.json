{"summary": "Encapsulates all runtime mask data required by the Splash attention kernel, including next block indices, block mask classifications, and partial mask blocks, while minimizing TPU scalar memory usage through dtype reduction and supporting per‑head or per‑shard mask layouts.", "business_intent": "Enable high‑performance, memory‑efficient attention computation on TPUs by providing pre‑computed mask structures that guide block prefetching and causal masking logic.", "keywords": ["attention", "masking", "TPU", "memory optimization", "dtype reduction", "head shards", "prefetch", "block mask", "causal masking", "runtime data"], "summary_hash": "edc3db28fc8a", "cached_at": "2026-02-09T11:49:40+00:00"}