{"summary": "Implements a transformer decoder that iteratively refines query embeddings through masked cross‑attention limited to each query's predicted foreground region and self‑attention, producing detailed mask representations for segmentation models.", "business_intent": "Provide a high‑performance decoding module for vision transformers to generate precise masks, supporting image segmentation applications such as instance, semantic, and panoptic segmentation in computer‑vision pipelines.", "keywords": ["transformer decoder", "masked attention", "cross-attention", "self-attention", "query embeddings", "mask prediction", "foreground region", "image segmentation", "instance segmentation", "panoptic segmentation", "deep learning", "computer vision"], "summary_hash": "cb16ad236412", "cached_at": "2026-02-09T09:39:58+00:00"}