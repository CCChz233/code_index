{"summary": "Implements a modular component that generates relative positional encodings for Transformer‑XL layers based on model dimension and maximum sequence length, optionally scaling the input, and returns the encoding alongside the (unchanged) input tensor according to a configurable adapter strategy.", "business_intent": "Provide a reusable, configurable way to supply relative positional information to Transformer‑XL models, enhancing their ability to capture sequence order without directly altering the original input tensor.", "keywords": ["relative positional encoding", "Transformer-XL", "adapter", "embedding", "scaling", "sequence length", "model dimension", "positional bias", "modular component"], "summary_hash": "51450312893e", "cached_at": "2026-02-08T09:36:39+00:00"}