{"summary": "A plugin that enables mixedâ€‘precision training for NeMo models, configuring the appropriate precision mode, loss scaling, and device handling to accelerate training while conserving GPU memory.", "business_intent": "Increase training speed and reduce resource consumption for speech and language AI applications built with NeMo.", "keywords": ["NeMo", "mixed precision", "plugin", "training acceleration", "GPU memory optimization", "fp16", "bf16", "loss scaling", "PyTorch Lightning"], "summary_hash": "0fea226e15c3", "cached_at": "2026-02-08T08:24:23+00:00"}