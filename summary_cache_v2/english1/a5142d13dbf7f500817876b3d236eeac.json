{"summary": "Provides a Nystromformer-based masked language model that predicts masked tokens in input sequences using efficient low-rank attention mechanisms.", "business_intent": "Facilitate scalable natural language processing tasks such as text completion, understanding, and downstream fine‑tuning by offering a high‑performance masked language model.", "keywords": ["Nystromformer", "masked language modeling", "transformer", "efficient attention", "NLP", "language model", "token prediction"], "summary_hash": "82033a85a85a", "cached_at": "2026-02-09T07:16:23+00:00"}