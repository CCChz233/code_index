{"summary": "A demonstration script that trains and validates a Transformer-based next‑word prediction model on the WikiText‑2 corpus using Lightning Fabric, handling data loading, model setup, and a simple training loop with support for CPU, GPU, and multi‑GPU execution.", "business_intent": "Showcase how to leverage Lightning Fabric for efficient, scalable training of transformer language models, providing a reference implementation for developers to adopt distributed training practices in natural language processing tasks.", "keywords": ["Transformer", "language model", "next-word prediction", "WikiText-2", "Lightning Fabric", "PyTorch", "training loop", "data loading", "distributed training", "GPU", "CPU"], "summary_hash": "2e01abfd9b05", "cached_at": "2026-02-08T09:12:24+00:00"}