{"summary": "This module implements a CPU‑based Recurrent Neural Network Transducer (RNNT) utility, providing classes to initialize model parameters, manage metadata and probability configurations, and perform forward and backward passes with a custom LogSoftmax gradient. It integrates with PyTorch autograd and leverages Numba for performance, enabling RNNT loss computation and inference on CPU environments.", "business_intent": "Allow speech recognition systems to train and evaluate RNNT models on CPUs, reducing reliance on GPU resources and expanding deployment flexibility for applications where GPU acceleration is unavailable or cost‑prohibitive.", "keywords": ["RNNT", "CPU", "speech recognition", "loss computation", "log softmax gradient", "metadata management", "probability structures", "inference", "training", "Numba", "PyTorch autograd"], "summary_hash": "3331c7f4613e", "cached_at": "2026-02-08T11:17:02+00:00"}