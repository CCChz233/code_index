{"summary": "A layer that applies the Exponential Linear Unit (ELU) activation to its inputs, using a configurable alpha to scale the negative exponential region and passing positive values unchanged.", "business_intent": "Enable neural network models to benefit from a smooth, non-linear activation that mitigates vanishing gradients and accelerates convergence, especially in deep learning architectures.", "keywords": ["ELU", "activation function", "exponential linear unit", "alpha parameter", "neural network", "deep learning", "non-linear transformation", "Keras layer"], "summary_hash": "4cc93cac46b4", "cached_at": "2026-02-09T12:03:14+00:00"}