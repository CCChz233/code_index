{"summary": "Defines a forward operation for fast multi‑head attention using a split‑K strategy, encapsulating kernel configuration, application helpers, and diagnostics for supported shapes and parameters.", "business_intent": "Enable high‑performance, scalable attention kernels on GPUs by leveraging split‑K parallelism, improving throughput for large‑scale transformer models within the xformers library.", "keywords": ["multi-head attention", "split-K", "GPU", "CUDA", "performance", "kernel configuration", "operator registration", "validation", "transformer", "xformers"], "summary_hash": "7c818ce250ae", "cached_at": "2026-02-08T23:33:07+00:00"}