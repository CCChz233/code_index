{"summary": "The module implements blendable dataset utilities for Megatron‑based language modeling, offering classes that combine multiple data sources and compute sample indices on‑the‑fly to reduce memory consumption during training.", "business_intent": "Enable scalable, memory‑efficient data loading and mixing for large‑scale language model training pipelines, improving resource utilization and flexibility in dataset composition.", "keywords": ["blendable dataset", "memory efficient", "dynamic indexing", "language modeling", "Megatron", "data mixing", "PyTorch", "NVIDIA NeMo", "training pipeline"], "summary_hash": "4b74ef6bf18e", "cached_at": "2026-02-08T11:31:22+00:00"}