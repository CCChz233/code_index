{"summary": "Encapsulates the results of a CLIP vision transformer, providing the final hidden state sequence, optional pooled image embeddings, and optionally the full set of intermediate hidden states and attention matrices.", "business_intent": "Supply a structured representation of visual features and internal model dynamics for downstream multimodal tasks such as image encoding, similarity scoring, retrieval, or fineâ€‘tuning of CLIP-based systems.", "keywords": ["CLIP", "vision model", "output", "image embeddings", "pooled representation", "last hidden state", "hidden states", "attention weights", "torch", "multimodal"], "summary_hash": "e1be190512ce", "cached_at": "2026-02-09T11:20:37+00:00"}