{"summary": "Configures and executes training/evaluation of an encoder‑decoder neural machine translation model with a bottleneck layer using NVIDIA NeMo, Hydra, and PyTorch Lightning, including data preprocessing and distributed strategy setup.", "business_intent": "Provides a ready‑to‑run example for researchers and engineers to experiment with bottleneck NMT architectures, enabling reproducible training and easy integration into larger translation pipelines.", "keywords": ["neural machine translation", "encoder-decoder", "bottleneck", "NVIDIA NeMo", "Hydra", "PyTorch Lightning", "distributed training", "data preprocessing", "model configuration", "example script"], "summary_hash": "3be9bcd37d62", "cached_at": "2026-02-08T10:42:24+00:00"}