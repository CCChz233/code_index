{"summary": "A lightweight data holder that defines the default hyperparameter values for the Adadelta optimizer, encapsulating settings such as learning rate, rho, epsilon, and weight decay without inheriting from the NeMo Config system.", "business_intent": "Enable developers to quickly instantiate an Adadelta optimizer with sensible defaults, streamlining the configuration step in machineâ€‘learning training workflows.", "keywords": ["Adadelta", "optimizer", "default hyperparameters", "learning rate", "rho", "epsilon", "weight decay", "PyTorch", "training configuration", "machine learning"], "summary_hash": "27097186999a", "cached_at": "2026-02-08T10:15:41+00:00"}