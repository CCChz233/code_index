{"summary": "Provides a dataset interface that reads tokenized parallel translation data from tarred pickle files, using a metadata manifest to determine batch count and shard locations. Supports flexible file path specifications with brace expansion, configurable sharding strategies for distributed training, optional shuffling, language direction swapping, and token ID prepending. Integrates source and target tokenizers and presents an iterable collection whose length reflects total token count.", "business_intent": "Facilitate scalable, high‑throughput loading of pre‑tokenized translation corpora for training large‑scale neural machine translation models, especially in multi‑process or multi‑node setups.", "keywords": ["translation dataset", "tarred files", "tokenized", "BPE tokenizer", "metadata manifest", "brace expansion", "shard strategy", "scatter", "replicate", "distributed data parallel", "shuffling", "reverse language direction", "prepend token", "iterator", "length"], "summary_hash": "a8716944c3e2", "cached_at": "2026-02-08T11:26:52+00:00"}