{"summary": "A plugin that automatically wraps every batch‑normalization layer in a model with logic that synchronises the layer’s statistics across multiple processes, enabling consistent behaviour during distributed training. When the model runs on a single device the plugin is inert.", "business_intent": "Provide seamless, out‑of‑the‑box support for synchronized batch‑norm in multi‑GPU or multi‑process deep‑learning workloads, improving model convergence and accuracy in distributed environments.", "keywords": ["batch normalization", "synchronization", "multiprocessing", "distributed training", "model plugin", "multi‑device", "parallel execution"], "summary_hash": "97da5661d840", "cached_at": "2026-02-08T08:11:24+00:00"}