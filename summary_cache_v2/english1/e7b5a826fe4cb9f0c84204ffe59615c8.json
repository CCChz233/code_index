{"summary": "Implements the post‑self‑attention output transformation for the NEZHA transformer model, applying a linear projection, dropout, residual addition, and layer‑normalization to the attention results.", "business_intent": "Provide a reusable component that finalizes the self‑attention computation in transformer‑based language models, preparing the data for the next encoder layer.", "keywords": ["NEZHA", "self‑attention", "output layer", "linear projection", "dropout", "layer normalization", "residual connection", "transformer", "neural network"], "summary_hash": "4a5bcd3e7de9", "cached_at": "2026-02-09T08:15:24+00:00"}