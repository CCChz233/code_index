{"summary": "Container class that encapsulates the outputs of a Vision Transformer masked autoencoder during pre‑training, including the reconstruction loss, predicted pixel logits, the binary mask of corrupted patches, the mapping to original patch indices, and optionally the hidden representations and attention matrices from each layer.", "business_intent": "Facilitate downstream processing of pre‑training results by providing a unified structure for loss calculation, reconstruction evaluation, and model introspection, supporting both training loops and analysis of internal model dynamics.", "keywords": ["loss", "reconstruction logits", "mask", "ids_restore", "hidden_states", "attentions", "TensorFlow", "Vision Transformer", "masked autoencoder", "pretraining output"], "summary_hash": "ae2123401962", "cached_at": "2026-02-09T11:44:08+00:00"}