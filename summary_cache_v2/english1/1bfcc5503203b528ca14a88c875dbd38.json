{"summary": "Creates a causal attention mask tailored for paged block‑diagonal attention, incorporating offset handling and padding for key/value tensors.", "business_intent": "Facilitates efficient long‑sequence transformer inference by generating masks that respect page boundaries, causal order, and padded keys, enabling scalable attention computation.", "keywords": ["attention mask", "paged attention", "block diagonal", "causal masking", "offset handling", "padded keys", "transformer", "long sequences", "efficiency"], "summary_hash": "0a0d184addb2", "cached_at": "2026-02-08T23:24:00+00:00"}