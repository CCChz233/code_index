{"summary": "The module implements a Litellm backend for Amazon SageMaker‑hosted large language models. It defines configuration handling, custom error types, a decoder for AWS event‑stream payloads, and a client class that builds authenticated requests, formats prompts, and supports synchronous, asynchronous, and streaming text generation as well as embedding extraction.", "business_intent": "Allow applications to seamlessly integrate with SageMaker inference endpoints for LLM tasks, providing a unified, high‑level API that abstracts authentication, request construction, response parsing, and error management, thereby simplifying the use of hosted models for generation, streaming, and embedding services.", "keywords": ["Amazon SageMaker", "large language model", "LLM inference", "streaming responses", "asynchronous API", "configuration management", "custom error handling", "event stream decoding", "text generation", "embeddings", "AWS authentication"], "summary_hash": "5564d3a7ebfc", "cached_at": "2026-02-08T07:54:12+00:00"}