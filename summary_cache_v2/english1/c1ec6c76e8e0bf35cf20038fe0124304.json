{"summary": "Provides a collection of PyTorch modules that implement transformer‑style operations for graph neural networks in DGL, including attention layers that incorporate graph‑derived biases, encoders for node degree, shortest‑path, and spatial (including 3‑D) information, and a full Graphormer layer that combines these components with feed‑forward and normalization blocks.", "business_intent": "Enable developers and researchers to build high‑performance graph‑based models—especially for chemistry, biology, and network analysis—by offering ready‑to‑use transformer components that capture structural, positional, and geometric relationships within graphs.", "keywords": ["graph transformer", "multi‑head attention", "graph bias", "degree encoding", "path encoding", "spatial encoding", "3D coordinates", "DGL", "PyTorch", "Graphormer", "molecular graph"], "summary_hash": "ffb2b7767999", "cached_at": "2026-02-09T00:42:52+00:00"}