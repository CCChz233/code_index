{"summary": "Implements a linear attention module that conforms to the expected interface of a generic attention block, enabling plug‑and‑play usage within transformer‑style architectures.", "business_intent": "Offer an efficient, drop‑in replacement for standard attention layers to accelerate model training and inference while maintaining compatibility with existing codebases.", "keywords": ["linear attention", "transformer", "neural network", "efficiency", "compatible interface", "deep learning", "attention block"], "summary_hash": "b0790c48fbba", "cached_at": "2026-02-08T08:55:48+00:00"}