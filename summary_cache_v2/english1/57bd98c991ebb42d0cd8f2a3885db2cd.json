{"summary": "A structured container that holds all relevant outputs from a sequence-to-sequence model, including decoder and encoder hidden states, attention matrices, cross‑attention data, and cached key/value tensors for fast incremental decoding.", "business_intent": "Enable downstream applications such as translation, summarization, or chat generation to retrieve model internals and leverage cached computations for low‑latency, high‑throughput inference.", "keywords": ["seq2seq", "model output", "hidden states", "attention", "cross‑attention", "past key values", "cache", "decoder", "encoder", "tensor", "generation", "inference acceleration"], "summary_hash": "25c03476821d", "cached_at": "2026-02-09T06:28:39+00:00"}