{"summary": "The package provides a comprehensive suite of evaluation metrics for Retrieval‑Augmented Generation (RAG) systems. It defines generic metric abstractions and concrete implementations that assess answer correctness, relevance, faithfulness, factuality, similarity, precision/recall of retrieved context, language quality (BLEU, ROUGE), rubric‑based scoring, tool‑call accuracy, multimodal consistency, noise robustness, and goal achievement across both single‑turn and multi‑turn interactions. Metrics leverage language‑model prompts, embedding similarity, NLI, and cross‑encoders, and include utilities for aggregation and scoring.", "business_intent": "Enable developers and data scientists to quantitatively evaluate and improve the performance of AI‑driven retrieval and generation pipelines, ensuring generated content is accurate, relevant, faithful to source material, and aligned with business goals.", "keywords": ["RAG evaluation", "metrics", "answer correctness", "relevance", "faithfulness", "factual correctness", "similarity", "precision", "recall", "BLEU", "ROUGE", "rubric scoring", "tool call accuracy", "multimodal evaluation", "noise sensitivity", "goal accuracy", "LLM prompts", "embedding similarity", "single‑turn", "multi‑turn"], "summary_hash": "1d23fd85b52a", "cached_at": "2026-02-08T22:52:41+00:00"}