{"summary": "Implements a block‑diagonal attention bias that restricts each query block to attend only to the corresponding key block, enabling efficient handling of batches with sequences of different lengths.", "business_intent": "Facilitate memory‑efficient multi‑head attention by providing a flexible mask that supports causal, local, and custom block structures, and allows splitting of attention outputs back into original sequence tensors.", "keywords": ["attention mask", "block diagonal", "memory efficient", "variable length sequences", "causal mask", "local attention", "tensor splitting", "PyTorch"], "summary_hash": "6ea83953e4b6", "cached_at": "2026-02-08T23:23:05+00:00"}