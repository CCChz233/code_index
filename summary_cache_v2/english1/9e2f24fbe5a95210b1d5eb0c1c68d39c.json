{"summary": "A configuration holder that defines a fixed sparsity pattern for transformer architectures, inheriting generic sparsity settings and adding specialized handling for static global and local attention layouts.", "business_intent": "Enables developers to configure and apply a predetermined sparse attention structure, reducing computational load and memory usage in large-scale generative models.", "keywords": ["fixed sparsity", "transformer", "configuration", "global layout", "local layout", "sparse attention", "model efficiency", "memory reduction", "computational optimization"], "summary_hash": "ababae8ecc2f", "cached_at": "2026-02-08T23:19:55+00:00"}