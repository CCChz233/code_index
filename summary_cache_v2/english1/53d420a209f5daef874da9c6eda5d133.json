{"summary": "Provides an attention layer that uses orthogonal landmark vectors and clustering (k‑means and spherical k‑means) to approximate full attention efficiently.", "business_intent": "Accelerate and scale transformer‑based models for language or vision tasks by reducing the computational cost of attention while preserving representation quality.", "keywords": ["attention", "orthogonal landmarks", "clustering", "k-means", "spherical k-means", "efficient transformer", "feature approximation", "scalable neural networks"], "summary_hash": "21f833295619", "cached_at": "2026-02-08T23:20:27+00:00"}