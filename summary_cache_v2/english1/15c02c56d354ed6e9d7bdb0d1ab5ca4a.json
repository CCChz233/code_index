{"summary": "A wrapper that adapts any data sampler for distributed training by partitioning its index list across the available processes, while delegating all randomness and shuffling to the original sampler.", "business_intent": "Enable seamless data sharding in multi‑GPU or multi‑node training pipelines so each worker receives a distinct subset of samples without modifying the sampler's inherent behavior.", "keywords": ["distributed training", "sampler", "sharding", "data loading", "Lightning", "parallelism", "index partitioning"], "summary_hash": "ae2455fa9b76", "cached_at": "2026-02-08T08:26:36+00:00"}