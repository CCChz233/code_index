{"summary": "Implements the joint computation of a Hybrid Autoregressive Transducer, merging encoder and prediction representations through a configurable feed‑forward network, applying a selectable activation, optional dropout and log‑softmax to produce token logits. Includes options for memory‑saving tensor clearing and fused loss and word‑error‑rate calculation over sub‑batches.", "business_intent": "Enable efficient training and inference of speech‑to‑text models based on the HAT architecture by providing a flexible, memory‑aware joint network that can simultaneously compute logits, loss and WER, reducing runtime and GPU memory usage.", "keywords": ["Hybrid Autoregressive Transducer", "joint network", "feed‑forward", "activation", "dropout", "log‑softmax", "memory saving", "fused loss", "word error rate", "speech recognition", "encoder", "prediction network", "vocabulary"], "summary_hash": "82127aa557f7", "cached_at": "2026-02-08T11:07:49+00:00"}