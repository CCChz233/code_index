{"summary": "Implements the attention layer used in Longformer models, handling both local sliding‑window attention and optional global attention while maintaining the ability to prune unnecessary attention heads.", "business_intent": "Enable efficient processing of very long textual inputs in natural‑language tasks, reducing computational cost and memory usage for large‑scale transformer deployments.", "keywords": ["attention", "Longformer", "sliding window", "global attention", "head pruning", "transformer", "NLP", "efficiency"], "summary_hash": "a06a87ebef11", "cached_at": "2026-02-09T11:12:16+00:00"}