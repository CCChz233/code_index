{"summary": "Provides a multi‑head attention module that substitutes the standard dense attention with a sliding‑window sparse pattern, following Longformer and sparse transformer concepts, to efficiently compute attention over very long sequences.", "business_intent": "Allow transformer‑based models to scale to long input texts while keeping memory and compute requirements low, supporting tasks such as long‑document understanding, language modeling, and sequence generation.", "keywords": ["multi-head attention", "sliding window", "sparse attention", "Longformer", "transformer", "efficient scaling", "long sequences", "memory reduction", "MistralAttention"], "summary_hash": "0572a58ba117", "cached_at": "2026-02-09T08:12:50+00:00"}