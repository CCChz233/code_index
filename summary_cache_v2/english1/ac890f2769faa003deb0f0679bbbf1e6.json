{"summary": "A wrapper that integrates two 2‑dimensional transformer blocks, allowing them to operate jointly on image‑like tensors. It configures attention heads, dimensions, layers, dropout, cross‑attention, positional embeddings, vector embeddings and optional adaptive normalization, and routes inputs through the combined transformers to produce mixed‑mode outputs.", "business_intent": "Enable a versatile transformer backbone for image generation or diffusion pipelines that need to handle both continuous and discrete latent representations, supporting advanced features such as cross‑attention and adaptive layer‑norm for improved inference quality.", "keywords": ["dual transformer", "2D transformer", "image processing", "attention", "cross‑attention", "adaptive layer norm", "diffusion models", "latent embeddings", "dropout", "positional embeddings"], "summary_hash": "6d3b968bfdac", "cached_at": "2026-02-09T04:39:09+00:00"}