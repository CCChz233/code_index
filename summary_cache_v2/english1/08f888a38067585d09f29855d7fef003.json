{"summary": "Implements a hook for the LiteLLM proxy that inspects incoming prompts for potential injection attacks, generates suspicious keyword lists, optionally runs moderation checks, and aborts or sanitizes requests before they reach the language model.", "business_intent": "Protect the LLM service and downstream applications from malicious prompt manipulation, ensuring compliance and security while maintaining reliable API performance.", "keywords": ["prompt injection", "LLM security", "moderation", "pre-call hook", "suspicious keywords", "FastAPI", "caching", "logging", "custom logger"], "summary_hash": "4b92b8038070", "cached_at": "2026-02-08T07:50:51+00:00"}