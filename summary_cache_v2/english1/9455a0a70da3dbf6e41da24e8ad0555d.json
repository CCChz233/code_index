{"summary": "DistEmbedding manages trainable node or edge embeddings that are partitioned and stored across a cluster, providing sparse, mini‑batch‑driven updates via DGL’s distributed optimizers, enabling efficient large‑scale graph model training.", "business_intent": "Facilitate scalable training of graph models on massive graphs by offering a distributed, sharded embedding layer that integrates with DGL’s optimizer framework and supports sparse updates.", "keywords": ["distributed embeddings", "graph neural networks", "sharding", "sparse updates", "partition policy", "DGL", "scalable training", "node embeddings", "edge embeddings", "distributed optimizer"], "summary_hash": "578fb337d0e8", "cached_at": "2026-02-08T23:57:10+00:00"}