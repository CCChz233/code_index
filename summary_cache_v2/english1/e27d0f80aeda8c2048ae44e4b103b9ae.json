{"summary": "Provides a loss module that penalizes attention tensors for deviating from binary values, encouraging the network to produce near‑binary attention maps during training.", "business_intent": "Enhance model interpretability and computational efficiency by enforcing binary attention, facilitating faster inference and hardware‑friendly implementations.", "keywords": ["loss function", "attention", "binarization", "neural network", "training regularization", "binary mask", "deep learning"], "summary_hash": "3a23ba0be53d", "cached_at": "2026-02-08T08:43:29+00:00"}