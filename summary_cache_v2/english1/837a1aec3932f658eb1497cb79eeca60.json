{"summary": "This module implements a text‑to‑image diffusion pipeline that enhances Stable Diffusion with an attention‑based control technique. It encodes prompts using a CLIP text model, iteratively denoises latent representations with a UNet and scheduler, applies a custom attention manipulation to steer generation, decodes the latents via a VAE, and optionally runs a safety filter on the output. Supporting utilities handle attention recording, Gaussian smoothing of tensors, and scaling of LoRA layers.", "business_intent": "Enable creators and enterprises to generate high‑quality, controllable images from textual descriptions, offering fine‑grained attention steering for artistic, marketing, and design workflows while ensuring content safety.", "keywords": ["stable diffusion", "text-to-image generation", "attention control", "attend-and-excite", "diffusion pipeline", "CLIP encoding", "UNet denoiser", "latent decoding", "VAE", "safety checker", "Gaussian smoothing", "LoRA scaling"], "summary_hash": "30d4e3cb04bc", "cached_at": "2026-02-09T05:24:23+00:00"}