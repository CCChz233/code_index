{"summary": "Implements a cache backed by a Qdrant vector database that stores prompts and their LLM responses as embeddings, allowing retrieval of semantically similar entries through both synchronous and asynchronous interfaces.", "business_intent": "Reduce latency and cost of LLM calls by reusing previously generated completions when new requests are semantically close to cached ones.", "keywords": ["Qdrant", "vector store", "semantic cache", "embeddings", "similarity search", "synchronous", "asynchronous", "LLM caching", "performance optimization"], "summary_hash": "356c7398c98d", "cached_at": "2026-02-08T07:46:20+00:00"}