{"summary": "Utility class that generates and transforms attention masks for transformer models, converting 2‑dimensional (batch, query length) masks into 4‑dimensional tensors suitable for attention score multiplication, while supporting causal (uni‑directional) masking and optional sliding‑window constraints.", "business_intent": "Enable correct masking of attention mechanisms in deep‑learning pipelines, ensuring models respect sequence order and locality requirements during training and inference.", "keywords": ["attention mask", "transformer", "causal mask", "sliding window", "4D mask", "mask conversion", "PyTorch", "sequence modeling"], "summary_hash": "0ae415d15ac4", "cached_at": "2026-02-09T06:27:00+00:00"}