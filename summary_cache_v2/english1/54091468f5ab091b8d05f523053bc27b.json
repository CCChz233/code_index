{"summary": "A model wrapper that adapts a GPTBigCode pretrained transformer for token-level classification tasks, handling input processing and producing per-token logits.", "business_intent": "Enable developers to apply GPTBigCode to sequence labeling problems such as named-entity recognition or code token tagging, facilitating fine-tuning and inference in downstream applications.", "keywords": ["GPTBigCode", "token classification", "sequence labeling", "pretrained transformer", "NLP", "fine-tuning", "per-token logits"], "summary_hash": "534c866c7876", "cached_at": "2026-02-09T10:51:02+00:00"}