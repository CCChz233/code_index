{"summary": "Encapsulates a lightweight helper that performs an HTTP GET request to fetch a site's robots.txt file, providing a reusable interface for retrieving and handling robots.txt content.", "business_intent": "Allow web crawling or SEO applications to programmatically obtain robots.txt directives and enforce compliant crawling behavior.", "keywords": ["robots.txt", "HTTP GET", "web crawler", "request helper", "site permissions", "compliance", "URL fetching"], "summary_hash": "62655f7c2369", "cached_at": "2026-02-09T07:39:13+00:00"}