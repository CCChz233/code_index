{"summary": "A test suite that verifies the tokenization logic for the LED (Longformer Encoder‑Decoder) model, checking handling of special tokens, global attention masks, batch preparation, length limits, empty targets and pre‑tokenized inputs.", "business_intent": "Guarantee correct tokenization behavior to support robust NLP applications using the LED model and prevent regressions in token handling across releases.", "keywords": ["LED", "tokenization", "special tokens", "global attention", "batch preparation", "max length", "pretokenized inputs", "unit testing", "NLP", "model validation"], "summary_hash": "2a822ea59eb1", "cached_at": "2026-02-09T05:40:23+00:00"}