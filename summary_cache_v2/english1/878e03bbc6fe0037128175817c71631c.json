{"summary": "Collates and dynamically pads variable‑length speech inputs for a sequence‑to‑sequence model, applying a processor to convert raw audio, inserting the decoder start token, and optionally generating attention masks for each batch.", "business_intent": "Enables streamlined batch preparation for training or evaluating speech‑to‑text models by ensuring inputs are uniformly padded and correctly formatted for the model's encoder‑decoder architecture.", "keywords": ["data collator", "dynamic padding", "speech input", "seq2seq", "WhisperProcessor", "decoder start token", "attention mask", "batch preparation"], "summary_hash": "480cd8fa5e78", "cached_at": "2026-02-09T06:18:11+00:00"}