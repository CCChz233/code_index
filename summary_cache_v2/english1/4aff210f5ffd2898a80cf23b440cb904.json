{"summary": "Implements a wrapper around a PyTorch optimizer that applies the Noam learning‑rate schedule, scaling the learning rate based on model dimension, a warm‑up period, and the current training step.", "business_intent": "Enable stable and efficient training of transformer‑style neural networks by automatically adjusting the learning rate according to the proven Noam schedule.", "keywords": ["Noam schedule", "learning rate decay", "warm-up", "transformer", "PyTorch optimizer", "training step scaling", "deep learning"], "summary_hash": "e8ccef0bc946", "cached_at": "2026-02-09T00:31:07+00:00"}