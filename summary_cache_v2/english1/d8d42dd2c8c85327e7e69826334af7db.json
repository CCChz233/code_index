{"summary": "Test suite that sets up models, tokenizers, and datasets to validate the functionality of the Contrastive Preference Optimization (CPO) trainer, including its behavior with and without LoRA adaptation.", "business_intent": "Guarantee the correctness and stability of the CPO training workflow for language models, enabling reliable preference-based fine-tuning and supporting future development and integration of PEFT techniques.", "keywords": ["CPO", "trainer", "testing", "LoRA", "PEFT", "transformers", "dataset", "torch", "parameterized", "unittest", "preference optimization"], "summary_hash": "df72e28049d7", "cached_at": "2026-02-09T05:57:58+00:00"}