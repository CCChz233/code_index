{"summary": "Encapsulates a neural network layer that combines a linear transformation with batch normalization, offering a ready-to-use building block for multi‑layer perceptron architectures.", "business_intent": "Enable developers to quickly assemble deep learning models with stabilized training and improved convergence by providing a pre‑configured MLP layer with batch normalization.", "keywords": ["neural network", "MLP", "batch normalization", "layer", "deep learning", "model component", "training stability"], "summary_hash": "91d410e1d652", "cached_at": "2026-02-09T08:38:36+00:00"}