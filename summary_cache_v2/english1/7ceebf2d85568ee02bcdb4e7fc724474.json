{"summary": "A Keras optimizer wrapper that automatically scales the loss during training to prevent numeric underflow in mixed‑precision (float16) computations, dynamically adjusting the scaling factor based on gradient finiteness.", "business_intent": "Provide stable, hands‑free mixed‑precision training by managing loss scaling, thereby improving numerical stability and reducing manual configuration for deep‑learning models.", "keywords": ["loss scaling", "dynamic scaling", "mixed precision", "float16", "optimizer wrapper", "gradient underflow", "numerical stability", "Keras", "training step", "scale adjustment"], "summary_hash": "906f5f22bbfa", "cached_at": "2026-02-09T11:26:55+00:00"}