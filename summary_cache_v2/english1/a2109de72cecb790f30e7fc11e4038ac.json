{"summary": "Implements a SentencePiece‑based tokenizer that converts raw text and associated layout bounding boxes into token IDs, attention masks and token‑type information, handling special tokens, padding, truncation and box encoding for document‑understanding models.", "business_intent": "Prepare textual and spatial layout data for transformer models used in document analysis, enabling downstream tasks such as classification, extraction, and question answering on scanned or digital documents.", "keywords": ["SentencePiece", "tokenization", "bounding boxes", "layout encoding", "special tokens", "padding", "truncation", "document understanding", "preprocessing", "transformer"], "summary_hash": "e4cb97edb62d", "cached_at": "2026-02-09T11:02:34+00:00"}