{"summary": "Provides a combined global and local attention mechanism for encoder modules, generating representations that capture both long-range and short-range dependencies within input sequences.", "business_intent": "Improve the encoder's contextual understanding in sequence-to-sequence models such as translation or summarization, leading to higher quality outputs by efficiently modeling hierarchical relationships.", "keywords": ["global attention", "local attention", "encoder", "representation", "sequence modeling", "transformer", "contextual awareness"], "summary_hash": "c197db629ea2", "cached_at": "2026-02-09T10:12:31+00:00"}