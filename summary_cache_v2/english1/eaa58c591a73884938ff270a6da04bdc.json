{"summary": "Implements a parametric rectified linear unit activation layer that learns a slope coefficient for negative inputs, with optional sharing of these coefficients across designated axes.", "business_intent": "Provides a trainable activation function allowing neural networks to adapt the negative slope, improving model flexibility and performance.", "keywords": ["parametric relu", "activation layer", "learnable alpha", "shared axes", "initializer", "regularizer", "constraint", "deep learning", "neural network"], "summary_hash": "b98d80929af2", "cached_at": "2026-02-09T12:02:55+00:00"}