{"summary": "Prepares and batches sequence-to-sequence examples by encoding tokens, applying padding, and adjusting decoder inputs for encoderâ€‘decoder architectures.", "business_intent": "Streamlines batch creation for training and inference of seq2seq models (e.g., T5), ensuring data is correctly formatted for the model pipeline.", "keywords": ["seq2seq", "data collator", "batch preparation", "token encoding", "padding", "decoder shift", "T5", "transformer", "NLP"], "summary_hash": "a4e9cbd1f936", "cached_at": "2026-02-09T05:58:00+00:00"}