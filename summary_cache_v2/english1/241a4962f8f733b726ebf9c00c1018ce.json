{"summary": "Encapsulates the outputs of a Vision Transformer masked autoencoder during pre‑training, including the reconstruction loss, predicted pixel logits, masking information, and optional internal states and attention maps.", "business_intent": "Facilitate downstream processing of pre‑training results by delivering all necessary tensors—loss for optimization, logits for reconstruction evaluation, mask and index restoration for patch handling, and optional hidden states and attentions for model analysis or debugging.", "keywords": ["loss", "reconstruction", "logits", "mask", "ids_restore", "hidden_states", "attentions", "Vision Transformer", "masked autoencoder", "pretraining", "output container"], "summary_hash": "3085615ff2d4", "cached_at": "2026-02-09T11:43:24+00:00"}