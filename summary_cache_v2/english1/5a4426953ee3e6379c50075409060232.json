{"summary": "The module implements a hierarchical loss mechanism for sentence‑transformer models that operate on embeddings of several nested dimensions. It provides decorators that cache the model’s forward pass and adapt existing cached loss functions to work with each reduced embedding size, shrinking pre‑computed vectors on‑the‑fly and aggregating the resulting losses.", "business_intent": "Enable efficient training of multi‑scale sentence embedding models by reusing a single forward computation for all target dimensions, thereby reducing computational cost while supporting hierarchical loss objectives.", "keywords": ["sentence transformer", "hierarchical loss", "multi‑scale embeddings", "embedding reduction", "caching", "training efficiency", "nested dimensions", "loss aggregation"], "summary_hash": "da71b7bf33f9", "cached_at": "2026-02-08T13:54:02+00:00"}