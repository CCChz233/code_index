{"summary": "Implements a lightweight module that forwards the hidden states from an intermediate BERT encoder layer straight to the classification head, enabling direct cross‑entropy loss computation without traversing the remaining layers.", "business_intent": "Accelerates experimentation and inference by allowing early‑exit or probing of intermediate BERT representations for sequence classification tasks, reducing computational overhead and supporting model analysis.", "keywords": ["BERT", "intermediate layer", "shortcut", "early exit", "sequence classification", "cross entropy", "neural network module", "transformer", "PyTorch"], "summary_hash": "642f49eeec2a", "cached_at": "2026-02-09T06:12:18+00:00"}