{"summary": "Evaluates a sentence embedding model by computing similarity scores (cosine, Euclidean, Manhattan) between paired sentence embeddings and measuring how well these scores correlate with reference similarity labels using Pearson and Spearman rank correlations, exposing a primary metric for model comparison.", "business_intent": "Offer a reusable benchmark to quantify the semantic similarity performance of embedding models, supporting model selection, tuning, and reporting in natural language processing workflows.", "keywords": ["embedding similarity", "model evaluation", "Spearman correlation", "Pearson correlation", "cosine similarity", "Euclidean distance", "Manhattan distance", "sentence transformer", "benchmark", "metric", "CSV logging", "semantic similarity"], "summary_hash": "a79b028b34b6", "cached_at": "2026-02-08T13:47:30+00:00"}