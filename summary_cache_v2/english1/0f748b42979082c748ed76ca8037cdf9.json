{"summary": "Provides an abstract indexed dataset and concrete implementations for sequence‑to‑sequence NLP tasks, using memory‑mapped files to enable fast random access and low‑memory loading of token ID or raw text data, and integrates with tokenizers for large training corpora.", "business_intent": "Facilitate efficient, scalable training and evaluation of large‑scale seq2seq models such as translation or summarization within the NeMo framework by minimizing memory footprint and speeding data I/O.", "keywords": ["sequence-to-sequence", "dataset", "memory-mapped", "random access", "low memory", "tokenizer integration", "NLP", "NeMo", "binarized tokens", "raw text"], "summary_hash": "81c28719f583", "cached_at": "2026-02-08T12:09:22+00:00"}