{"summary": "A configurable BERT-based transformer that can operate either as a pure encoder using self‑attention or as a decoder by inserting cross‑attention layers, following the original \"Attention is All You Need\" architecture. It manages internal statistics, logging, and hyper‑parameters such as patience and regression thresholds.", "business_intent": "Offer a versatile language model component that supports both encoding and decoding workflows for NLP applications, enabling developers to build tasks like translation, summarization, or conditional generation with a single model class.", "keywords": ["BERT", "transformer", "encoder", "decoder", "cross-attention", "self-attention", "configurable", "statistics", "logging", "patience", "regression threshold"], "summary_hash": "cd9b37731c73", "cached_at": "2026-02-09T06:11:43+00:00"}