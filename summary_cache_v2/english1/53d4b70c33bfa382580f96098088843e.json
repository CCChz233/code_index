{"summary": "A configuration container that defines all architectural and hyper‑parameter settings for the image encoder component of the FLAVA multimodal model, including transformer dimensions, attention heads, feed‑forward size, dropout rates, layer‑norm epsilon, image resolution, patch size, channel count, bias options, mask token usage, and codebook vocabulary.", "business_intent": "Allow developers to easily customize, instantiate, and share pretrained FLAVA image encoder models for vision and multimodal tasks, supporting masked image modeling and seamless integration with the FLAVA codebook.", "keywords": ["FLAVA", "image encoder", "configuration", "transformer", "vision model", "hidden size", "layers", "attention heads", "patch size", "image size", "dropout", "layer normalization", "bias", "mask token", "codebook vocabulary", "pretrained"], "summary_hash": "f8cbb628e032", "cached_at": "2026-02-09T10:15:51+00:00"}