{"summary": "Defines a base neural module that implements a Megatron-style transformer decoder, specifying the expected input and output neural types and providing common functionality for decoder components within the NeMo NLP framework.", "business_intent": "To give developers a reusable foundation for building or extending transformer decoder modules used in large language models, streamlining integration, training, and inference of decoder architectures in NLP applications.", "keywords": ["decoder", "Megatron", "NeMo", "transformer", "NLP", "neural module", "language model", "deep learning", "input metadata", "output metadata"], "summary_hash": "5fe1f78ddbe4", "cached_at": "2026-02-08T11:23:11+00:00"}