{"summary": "Provides a dataset implementation for BERT-style pretraining within the Megatron framework. It loads tokenized text from a memory‑mapped indexed source, constructs masked language modeling and segment‑type inputs, applies padding, and converts data to NumPy arrays for consumption by PyTorch models.", "business_intent": "Facilitate large‑scale BERT training by supplying an efficient, ready‑to‑use data pipeline that handles masking, segment identification, and batching, thereby accelerating development of NLP models for downstream applications.", "keywords": ["BERT", "dataset", "masked language modeling", "tokenization", "Megatron", "PyTorch", "memory-mapped", "indexing", "padding", "NumPy", "segment IDs", "language modeling"], "summary_hash": "e051b598b462", "cached_at": "2026-02-08T11:30:13+00:00"}