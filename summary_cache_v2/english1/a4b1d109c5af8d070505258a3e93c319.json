{"summary": "Core TensorFlow layer implementing the Transformer-XL architecture, managing segment-level recurrence, memory caching, and multi‑head attention for long‑range sequence modeling.", "business_intent": "Enable developers to construct efficient, long‑context language or sequence models by providing a ready‑to‑use Transformer‑XL main layer component.", "keywords": ["transformer-xl", "tensorflow", "recurrent memory", "segment recurrence", "multi-head attention", "sequence modeling", "language model", "neural network layer"], "summary_hash": "a262412bc3f9", "cached_at": "2026-02-09T07:44:32+00:00"}