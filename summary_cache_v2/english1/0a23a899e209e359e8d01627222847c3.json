{"summary": "The decoder package supplies a registry and a set of builder classes that translate model‑specific configurations and weight tensors into TensorRT‑compatible transformer decoder layers. It supports multiple architectures (e.g., Falcon, Gemma, GPT, GPT‑J, LLaMA) and handles attention, feed‑forward, normalization, activation, and optional quantization to produce ready‑to‑use decoder components for high‑throughput inference.", "business_intent": "Enable developers to export large language models to TensorRT for accelerated inference, providing a unified interface that abstracts architecture‑specific details and delivers optimized decoder layers suitable for production‑grade deployment.", "keywords": ["TensorRT", "LLM", "decoder", "transformer", "model export", "quantization", "attention", "feed‑forward", "normalization", "Falcon", "Gemma", "GPT", "GPT‑J", "LLaMA", "builder", "configuration", "high‑performance inference"], "summary_hash": "e361906470a4", "cached_at": "2026-02-08T12:12:06+00:00"}