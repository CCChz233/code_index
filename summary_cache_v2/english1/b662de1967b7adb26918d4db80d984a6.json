{"summary": "The module implements Megatron‑compatible transformer encoder and decoder components specialized for retrieval tasks. It integrates parallel transformer layers, rotary position embeddings, and custom 3‑D attention mask construction, while providing mechanisms for memory handling and checkpointable state serialization.", "business_intent": "Enable scalable, high‑performance retrieval models that encode and decode query and document representations for large‑scale search and information‑retrieval applications.", "keywords": ["Megatron", "transformer", "encoder", "decoder", "retrieval", "attention mask", "rotary embedding", "parallelism", "checkpointing", "memory management", "PyTorch", "einops"], "summary_hash": "c22106e7a8d7", "cached_at": "2026-02-08T11:24:37+00:00"}