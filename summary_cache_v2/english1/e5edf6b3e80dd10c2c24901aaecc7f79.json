{"summary": "A memory‑efficient dataset that streams token‑level punctuation and capitalization labels from tar archives for BERT‑style models. It reads a metadata JSON describing batch counts and tar file locations, loads label vocabularies, uses a tokenizer to obtain special token IDs, builds samples on‑the‑fly, optionally ignores sub‑word or special tokens for loss computation, shuffles a configurable number of batches in memory, and distributes shards across multiple processes using scatter or replicate strategies. It also provides utilities to copy label vocabularies and collate batches for training.", "business_intent": "Enable large‑scale training of punctuation and capitalization prediction models without loading the entire corpus into RAM, while supporting distributed data parallelism and flexible sharding.", "keywords": ["punctuation", "capitalization", "token classification", "tarred dataset", "memory-efficient", "distributed training", "sharding", "tokenizer", "label vocabularies", "data streaming", "collate function"], "summary_hash": "01997e084b56", "cached_at": "2026-02-08T11:28:12+00:00"}