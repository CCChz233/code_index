{"summary": "Provides utilities to load, attach, enable, disable, and manage parameter‑efficient fine‑tuning adapters (such as LoRA, IA3, AdaLora) within transformer‑based models, allowing multiple adapters to be activated or deactivated and exposing their state for training or inference.", "business_intent": "Simplify the integration and lifecycle management of PEFT adapters so developers can efficiently apply, train, and switch lightweight fine‑tuning modules on large language models without modifying the core model architecture.", "keywords": ["PEFT", "adapter management", "LoRA", "IA3", "AdaLora", "transformer", "fine‑tuning", "state dict", "enable/disable", "multiple adapters"], "summary_hash": "c5cc63f4d68a", "cached_at": "2026-02-09T07:56:03+00:00"}