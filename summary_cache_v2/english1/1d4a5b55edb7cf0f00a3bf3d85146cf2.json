{"summary": "Generates and manages relative positional encodings for local (sliding‑window or chunked) attention, following the Transformer‑XL approach, with optional scaling and dropout, and provides utilities to extend and apply these encodings during model forward passes.", "business_intent": "Facilitate efficient streaming or chunk‑based transformer architectures by supplying accurate relative position information, enhancing performance in sequence modeling tasks such as speech recognition, language modeling, and other applications that require limited‑window attention.", "keywords": ["relative positional encoding", "sliding window attention", "chunked attention", "Transformer-XL", "positional embeddings", "dropout", "scaling", "sequence length", "embedding dimension"], "summary_hash": "a17d73b8f6e6", "cached_at": "2026-02-08T09:28:48+00:00"}