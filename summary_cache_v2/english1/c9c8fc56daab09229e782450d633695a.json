{"summary": "Provides a dedicated masked language modeling head for DeBERTa‑v2 models, converting encoder hidden states into vocabulary logits for token prediction.", "business_intent": "Facilitates NLP applications that require masked token prediction, such as pre‑training, fine‑tuning for text understanding, and downstream language‑model based services.", "keywords": ["DeBERTa", "masked language modeling", "MLM head", "transformer", "NLP", "token prediction", "forward pass", "PyTorch"], "summary_hash": "90c04ff4d90f", "cached_at": "2026-02-09T11:53:02+00:00"}