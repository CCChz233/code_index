{"summary": "Implements a rotary positional embedding mechanism with linear scaling for stable language models, extending the base rotary embedding functionality.", "business_intent": "Enable transformer-based language models to apply scalable, stable rotary positional encodings, improving model performance on longer sequences.", "keywords": ["rotary embedding", "linear scaling", "positional encoding", "stable language model", "transformer", "cosine sine cache", "embedding layer", "neural network", "sequence length", "model stability"], "summary_hash": "eb1ef56be46e", "cached_at": "2026-02-09T09:24:11+00:00"}