{"summary": "Encapsulates the result of an attention operation in the LXMERT model, holding attention probabilities and the transformed hidden states while providing a lightweight forward helper to expose the output.", "business_intent": "Enables downstream multimodal language‑vision applications to access and propagate attention information from the LXMERT transformer, supporting tasks such as visual question answering, image captioning, and cross‑modal reasoning.", "keywords": ["LXMERT", "attention", "output", "transformer", "multimodal", "forward helper", "tensor", "visual-language"], "summary_hash": "9c0561488bcd", "cached_at": "2026-02-09T09:27:58+00:00"}