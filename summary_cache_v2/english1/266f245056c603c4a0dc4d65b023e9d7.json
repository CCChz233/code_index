{"summary": "Container class that encapsulates the tensors produced by a hierarchical transformer model, including the final sequence embeddings, a pooled representation of the classification token, and optional per‑layer hidden states and attention matrices from both shallow and deep encoder stacks.", "business_intent": "Enable downstream applications such as text classification, sentence embedding extraction, or model introspection by delivering a unified, easy‑to‑access output structure that contains both pooled and detailed layer‑wise information.", "keywords": ["transformer", "pooled output", "hidden states", "attention weights", "shallow encoder", "deep encoder", "sequence representation", "PyTorch", "model output", "classification token"], "summary_hash": "7042824066fb", "cached_at": "2026-02-09T08:40:29+00:00"}