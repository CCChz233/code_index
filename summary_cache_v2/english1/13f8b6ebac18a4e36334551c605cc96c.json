{"summary": "Provides the language modeling head for a RoBERTa model in TensorFlow, projecting hidden states to vocabulary logits using output embeddings and an optional bias.", "business_intent": "Enables masked language modeling for pre‑training or fine‑tuning RoBERTa models, allowing token prediction at masked positions.", "keywords": ["RoBERTa", "masked language modeling", "TensorFlow", "language model head", "output embeddings", "bias", "logits", "pretraining", "fine‑tuning"], "summary_hash": "0cf747006fb2", "cached_at": "2026-02-09T11:42:00+00:00"}