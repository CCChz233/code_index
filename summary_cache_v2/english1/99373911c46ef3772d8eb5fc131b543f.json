{"summary": "Implements a leaky rectified linear unit activation layer that applies a small linear scaling to negative inputs while passing positive inputs unchanged.", "business_intent": "Provides a neural‑network activation function that preserves gradient flow for negative activations, improving model training stability and performance compared to standard ReLU.", "keywords": ["activation", "leaky ReLU", "negative slope", "gradient flow", "neural network", "deep learning", "non‑linear", "layer"], "summary_hash": "84e39d4a3407", "cached_at": "2026-02-09T12:03:24+00:00"}