{"summary": "A compact two‑layer MLP that runs in tensor‑parallel mode to encode prompts, producing virtual token embeddings for parameter‑efficient tuning of large language models.", "business_intent": "Enable scalable and fast prompt encoding for p‑tuning, reducing the parameter footprint while maintaining performance in large‑scale transformer deployments.", "keywords": ["tensor parallel", "MLP", "prompt encoder", "virtual token embeddings", "p-tuning", "lightweight", "model parallelism", "efficient tuning"], "summary_hash": "efce24612e7c", "cached_at": "2026-02-08T09:43:57+00:00"}