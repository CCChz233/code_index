{"summary": "Test module that validates the CUDA kernels used in the RNN‑Transducer loss implementation, covering standard, multiblank, and TDT variants. It checks forward (alpha), backward (beta), and gradient computations, including edge cases such as value clamping and fast‑emit regularization, using NumPy and PyTorch reference implementations.", "business_intent": "Guarantee the correctness and numerical stability of GPU‑accelerated RNNT loss calculations, which are critical for training high‑performance speech‑recognition models.", "keywords": ["RNNT", "CUDA", "GPU", "kernel", "forward probability", "backward probability", "gradient", "multiblank", "TDT", "test", "Numba", "PyTorch", "speech recognition"], "summary_hash": "f6f2b10c40eb", "cached_at": "2026-02-08T10:33:28+00:00"}