{"summary": "Test suite that compares training a ConvNet model using Lightning Fabric's DistributedDataParallel against native PyTorch DistributedDataParallel, checking that losses, model states, timing, and CUDA memory usage match under deterministic conditions.", "business_intent": "Validate that Fabric's DDP implementation provides identical behavior to PyTorch's DDP, giving users confidence in switching to or using Fabric for distributed training.", "keywords": ["parity", "distributed data parallel", "Fabric", "PyTorch", "ConvNet", "CUDA", "deterministic", "testing", "state dict equality", "timing", "memory usage"], "summary_hash": "696ee0d9e527", "cached_at": "2026-02-08T08:32:09+00:00"}