{"summary": "Manages the lifecycle and execution of a TensorRT‑optimized inference engine, handling resource allocation, input preparation, and inference calls.", "business_intent": "Provide a lightweight, production‑ready wrapper that abstracts TensorRT engine setup and execution to enable fast deep‑learning inference in performance‑critical applications.", "keywords": ["TensorRT", "inference engine", "buffer allocation", "input handling", "GPU acceleration", "model execution", "optimization", "wrapper", "deep learning", "deployment"], "summary_hash": "291c9fcf8208", "cached_at": "2026-02-08T11:05:11+00:00"}