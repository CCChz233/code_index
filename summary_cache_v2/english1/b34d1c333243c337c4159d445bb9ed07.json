{"summary": "A fast tokenizer implementation for the LED (Longformer Encoder-Decoder) model, enabling efficient conversion of raw text into token IDs suitable for processing long sequences.", "business_intent": "Facilitate high‑performance tokenization of lengthy documents for encoder‑decoder NLP tasks such as summarization or translation.", "keywords": ["LED", "tokenizer", "fast", "long documents", "encoder-decoder", "NLP", "tokenization", "HuggingFace"], "summary_hash": "3c9128a4eb4f", "cached_at": "2026-02-09T06:34:41+00:00"}