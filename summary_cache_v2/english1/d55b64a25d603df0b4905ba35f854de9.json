{"summary": "Implements a single transformer decoder block for the Marian sequence-to-sequence model using Flax, integrating self‑attention, encoder‑decoder attention, feed‑forward processing, layer normalization and dropout.", "business_intent": "Provides a reusable component for building and training Marian‑based neural machine translation systems, enabling efficient decoding of target sequences.", "keywords": ["Flax", "Marian", "decoder layer", "transformer", "self‑attention", "cross‑attention", "feed‑forward", "layer normalization", "dropout", "neural machine translation"], "summary_hash": "ba0e7a27908e", "cached_at": "2026-02-09T11:27:33+00:00"}