{"summary": "The script reads raw text‑normalization data files, optionally preprocesses them, tokenizes the text using a transformer tokenizer, groups the examples into batches, and writes those batches into tar archives that can be streamed efficiently during model training with NeMo.", "business_intent": "Prepare a large, efficiently loadable dataset for training a duplex text‑normalization model, enabling faster data access and reduced I/O overhead in production or research pipelines.", "keywords": ["dataset creation", "tar archive", "text normalization", "tokenization", "batching", "parallel processing", "NeMo", "speech recognition"], "summary_hash": "968446fed8bb", "cached_at": "2026-02-08T10:45:55+00:00"}