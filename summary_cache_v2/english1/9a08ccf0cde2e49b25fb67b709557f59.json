{"summary": "Provides a PyTorch optimizer wrapper that dynamically adjusts the learning rate using the Noam schedule, which scales with model size, warm‑up steps, and the current training step.", "business_intent": "Enable stable and efficient training of transformer models by automating the Noam learning‑rate schedule, reducing manual tuning and improving convergence.", "keywords": ["optimizer", "learning rate schedule", "Noam", "warm-up", "transformer", "PyTorch", "model dimension", "training step"], "summary_hash": "ed19ecf26a33", "cached_at": "2026-02-09T00:57:28+00:00"}