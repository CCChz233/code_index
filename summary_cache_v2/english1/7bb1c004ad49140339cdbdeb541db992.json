{"summary": "Implements an optimized batched greedy decoding routine for RNN‑Transducer models that iteratively determines the next non‑blank token while minimizing network calls, leveraging CUDA graph execution to achieve high‑throughput sequence inference. Includes structures for mutable decoding state and management of multiple CUDA graph objects.", "business_intent": "Accelerate speech‑recognition inference by reducing latency and increasing throughput of greedy decoding in production‑grade ASR systems, especially on NVIDIA GPUs.", "keywords": ["ASR", "RNN‑Transducer", "greedy decoding", "CUDA graphs", "batched inference", "loop label computation", "performance optimization", "GPU acceleration", "state management"], "summary_hash": "2a1032831d22", "cached_at": "2026-02-08T11:15:35+00:00"}