{"summary": "Implements Triton GPU kernels and helper utilities for split‑K execution of fast multi‑head attention, including forward computation, reduction, dequantization, autotuning, and cache management.", "business_intent": "Boost transformer attention performance on GPUs by parallelising the K dimension with split‑K kernels, providing configurable and autotuned implementations for high‑throughput inference and training.", "keywords": ["Triton", "split-K", "fast multi-head attention", "GPU kernels", "autotuning", "reduction", "dequantization", "configuration", "caching"], "summary_hash": "5ba32ec9754c", "cached_at": "2026-02-08T23:33:22+00:00"}