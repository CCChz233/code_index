{"summary": "A Flax module that implements a Performer-based masked language model, handling inputs, attention masks, and optional labels to generate prediction logits (and loss) for masked tokens.", "business_intent": "To deliver an efficient, scalable masked language modeling solution using Performer attention for training and inference in NLP applications.", "keywords": ["Flax", "Performer", "masked language modeling", "NLP", "transformer", "efficient attention", "JAX", "pretraining", "inference", "language model"], "summary_hash": "fcde862ea6c9", "cached_at": "2026-02-09T06:00:56+00:00"}