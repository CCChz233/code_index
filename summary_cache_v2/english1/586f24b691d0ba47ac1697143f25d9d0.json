{"summary": "A helper utility that evaluates how relevant a language model's output is by prompting another LLM to act as a judge, returning a relevance score or judgment.", "business_intent": "Enable automated relevance assessment of generated text for model evaluation, quality control, and content moderation workflows.", "keywords": ["relevance evaluation", "LLM judge", "automatic assessment", "language model output", "text quality", "model benchmarking", "AI evaluation utility"], "summary_hash": "296b813d1d30", "cached_at": "2026-02-09T06:44:09+00:00"}