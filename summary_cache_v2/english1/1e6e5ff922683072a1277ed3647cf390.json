{"summary": "Implements 4‑bit weight quantization for diffusion models using bitsandbytes, converting transformer layers to low‑bit linear modules during model loading, handling on‑device quantization, dequantization, validation, and state‑dict serialization.", "business_intent": "Enable memory‑efficient deployment and faster inference of large diffusion models by reducing weight precision to 4 bits while maintaining compatibility with existing loading and saving workflows.", "keywords": ["4-bit quantization", "bitsandbytes", "diffusion models", "memory optimization", "low‑bit inference", "model loading", "weight conversion", "serialization", "GPU acceleration", "torch dtype"], "summary_hash": "510693213229", "cached_at": "2026-02-09T04:09:12+00:00"}