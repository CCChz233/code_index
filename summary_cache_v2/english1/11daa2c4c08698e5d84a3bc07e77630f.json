{"summary": "Implements a masked language model based on the Yoso architecture, offering a forward computation and getter/setter for the output embedding layer.", "business_intent": "Enable prediction of masked tokens in text sequences for tasks such as text completion, information retrieval, and pretraining of language representations.", "keywords": ["masked language modeling", "Yoso architecture", "output embeddings", "forward pass", "NLP", "token prediction", "pretraining"], "summary_hash": "2561d002891a", "cached_at": "2026-02-09T09:57:51+00:00"}