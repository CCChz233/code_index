{"summary": "The tokenizer package supplies several text preprocessing classes used by sentence‑transformer models. It includes a basic whitespace splitter, a word‑level tokenizer that builds and persists a token‑to‑index dictionary, and a phrase tokenizer that merges known multi‑word expressions into single tokens using a configurable delimiter.", "business_intent": "Provide flexible and efficient tokenization mechanisms to prepare raw text for embedding generation, supporting custom vocabularies, phrase detection, and easy persistence for downstream NLP applications.", "keywords": ["tokenization", "phrase merging", "whitespace split", "vocabulary management", "token-to-index", "text preprocessing", "sentence-transformers", "embeddings"], "summary_hash": "3363808017a9", "cached_at": "2026-02-08T13:59:36+00:00"}