{"summary": "TensorFlow Keras layer implementing the Longformer self‑attention mechanism, offering efficient sparse attention for very long sequences and supporting head pruning.", "business_intent": "Provide a scalable attention component for transformer‑based NLP models, reducing computational and memory costs when processing long text inputs.", "keywords": ["Longformer", "attention", "TensorFlow", "Keras", "transformer", "sparse attention", "sliding window", "global attention", "head pruning", "NLP"], "summary_hash": "242e3ad98cb2", "cached_at": "2026-02-09T11:13:45+00:00"}