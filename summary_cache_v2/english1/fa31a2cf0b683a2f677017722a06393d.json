{"summary": "Provides a KERPLE relative position embedding implementation for Megatron transformer models, computing linear bias terms for attention scores based on forward and backward token distances and supporting both autoregressive decoding and joint encoder scenarios.", "business_intent": "Enhance transformer models with flexible positional bias to improve language modeling, generation, and comprehension performance, especially in largeâ€‘scale Megatron deployments.", "keywords": ["KERPLE", "relative position embedding", "attention bias", "Megatron", "transformer", "autoregressive", "encoder", "positional encoding", "linear bias", "token distance"], "summary_hash": "088a210ca818", "cached_at": "2026-02-08T11:25:46+00:00"}