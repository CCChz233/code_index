{"summary": "A structured container that holds a transformer model's output tensors, including the final hidden states, a pooled representation of the sequence, optional hidden layer activations, self‑attention maps, cross‑attention maps, and cached key/value tensors for fast decoding.", "business_intent": "Standardize the delivery of model results so downstream applications (e.g., classification, sequence generation, or analysis) can easily access pooled embeddings, attention information, and cached states for efficient inference and further processing.", "keywords": ["transformer", "model output", "pooled representation", "hidden states", "attention weights", "cross‑attention", "cache", "past key values", "sequence modeling"], "summary_hash": "469947123fe8", "cached_at": "2026-02-09T06:28:14+00:00"}