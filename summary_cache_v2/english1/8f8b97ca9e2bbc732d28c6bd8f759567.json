{"summary": "Implements a compositional attention layer that separates the attention process into a search phase (softmax over query‑key products across heads) and a retrieval phase (aggregation of values guided by a set of learned rules). The layer supports configurable dimensions for model, attention, keys, values, and a selection space, and can apply bias, dropout, causal masking, and optional non‑linear scoring for the retrieval step.", "business_intent": "Offer a more expressive and memory‑efficient attention mechanism for Transformer‑style models by disentangling search and retrieval, enabling richer inter‑head interactions while reducing the number of required heads and overall resource consumption.", "keywords": ["compositional attention", "search and retrieval", "multi‑head attention", "transformer", "rule‑based retrieval", "causal masking", "dropout", "non‑linear scoring", "dimensionality"], "summary_hash": "a90b831ee1b5", "cached_at": "2026-02-08T23:20:24+00:00"}