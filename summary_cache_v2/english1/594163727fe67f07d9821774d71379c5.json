{"summary": "A transformer‑based text model that can function as a self‑attention encoder or, when configured, as a decoder with an additional cross‑attention layer, supporting encoder‑decoder setups for sequence‑to‑sequence tasks.", "business_intent": "Provide a versatile component for building and fine‑tuning NLP systems that need contextual text embeddings or full encoder‑decoder pipelines, such as translation, summarization, and other downstream language applications.", "keywords": ["transformer", "self-attention", "cross-attention", "encoder", "decoder", "seq2seq", "configurable", "text representation", "pretrained model"], "summary_hash": "7a0160501c96", "cached_at": "2026-02-09T09:18:07+00:00"}