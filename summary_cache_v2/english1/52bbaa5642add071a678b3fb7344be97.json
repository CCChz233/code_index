{"summary": "Provides a local attention module that generates a mask limiting attention to a configurable neighborhood around each position and applies this mask during the forward computation.", "business_intent": "Allow neural network models, especially transformers, to concentrate on nearby tokens, reducing computational cost and memory usage while maintaining performance on tasks that benefit from localized context.", "keywords": ["local attention", "attention mask", "neural network", "transformer", "forward pass", "efficiency", "context window"], "summary_hash": "76728ef1a346", "cached_at": "2026-02-08T23:19:45+00:00"}