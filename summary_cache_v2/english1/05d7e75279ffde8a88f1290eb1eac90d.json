{"summary": "The module defines a hierarchy of Megatron‑based GPT models enhanced with lightweight adapter modules. It provides a base adapter model that wraps a Megatron GPT, handling forward passes, loss computation, optimizer setup, checkpointing, and training hooks. Two concrete subclasses implement specific adapter strategies: one inserts generic linear adapters into each transformer layer for task‑specific fine‑tuning, and another integrates IA3‑style infused adapters that scale key, value, and feed‑forward activations. Only the adapter parameters are trained and saved, allowing the base GPT to remain frozen and be reused across tasks.", "business_intent": "Enable efficient, low‑cost fine‑tuning of large GPT language models for downstream NLP applications by leveraging parameter‑efficient adapters, reducing training time and storage while preserving the ability to deploy the original model unchanged for inference.", "keywords": ["Megatron", "GPT", "adapter modules", "parameter‑efficient fine‑tuning", "language modeling", "IA3", "lightweight adapters", "PyTorch Lightning", "NLP", "model checkpointing", "parallel adapters"], "summary_hash": "d06e1ccac327", "cached_at": "2026-02-08T11:34:29+00:00"}