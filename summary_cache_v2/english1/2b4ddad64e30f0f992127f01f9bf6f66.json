{"summary": "A configuration holder that defines all architectural hyperparameters for an MBart multilingual sequence‑to‑sequence transformer, including vocabulary size, model dimensions, encoder/decoder layers, attention heads, feed‑forward sizes, activation functions, dropout rates, and other training options.", "business_intent": "Enables developers to instantiate, customize, and reproduce MBart models with specific settings for multilingual translation or generation tasks, simplifying model creation and fine‑tuning.", "keywords": ["MBart", "configuration", "transformer", "multilingual", "encoder", "decoder", "hyperparameters", "pretrained", "model architecture", "dropout", "attention heads"], "summary_hash": "b0f8ed5168c4", "cached_at": "2026-02-09T11:05:09+00:00"}