{"summary": "Implements sparse adaptive optimizers for node embedding layers in DGL’s PyTorch backend, maintaining per‑embedding gradient statistics, adapting learning rates, and updating only embeddings with non‑zero gradients. Includes support for CPU pinned memory and NCCL‑based distributed synchronization.", "business_intent": "Enables efficient and scalable training of large graph neural networks with massive embedding tables by reducing memory usage and communication overhead, accelerating convergence and supporting distributed training environments.", "keywords": ["sparse optimizer", "adaptive learning rate", "node embeddings", "DGL", "PyTorch", "gradient statistics", "CPU pinned memory", "NCCL", "distributed training", "graph neural networks"], "summary_hash": "247745553bc4", "cached_at": "2026-02-09T01:00:32+00:00"}