{"summary": "Provides a PyTorch-compatible distributed sparse embedding layer that partitions node or edge embeddings across multiple machines, supports mini‑batch forward passes, and works with DGL’s distributed optimizers for efficient large‑scale graph neural network training.", "business_intent": "Enable scalable and memory‑efficient training of graph neural networks on massive graphs by distributing embedding storage and computation across a cluster.", "keywords": ["distributed embedding", "sparse", "mini-batch", "DGL", "PyTorch", "graph neural networks", "scalability", "parameter server", "optimizer integration"], "summary_hash": "f05bbebe2b83", "cached_at": "2026-02-09T01:00:27+00:00"}