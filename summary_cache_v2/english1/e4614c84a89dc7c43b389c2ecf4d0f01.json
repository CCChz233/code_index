{"summary": "Implements a cross‑entropy loss for sequence models that ignores padding tokens, incorporates label‑smoothing regularization, and optionally limits the computation to the last k tokens, with configurable per‑token reduction.", "business_intent": "Supply a flexible loss function for training and evaluating neural sequence models such as machine translation or language modeling, enhancing stability through label smoothing and allowing efficient evaluation by focusing on recent tokens.", "keywords": ["cross entropy", "label smoothing", "padding exclusion", "sequence loss", "last k tokens", "per-token reduction", "neural machine translation", "language modeling", "loss function"], "summary_hash": "807d6bc13717", "cached_at": "2026-02-08T08:28:20+00:00"}