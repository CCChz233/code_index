{"summary": "Provides a Lightning Fabric strategy that orchestrates model‑parallel training by constructing a two‑dimensional device mesh that merges fully‑sharded data parallelism with tensor parallelism. It handles distributed environment setup, rank allocation, gradient‑synchronization control, and checkpoint input/output for models spread across multiple GPUs or nodes.", "business_intent": "Allow developers to efficiently train very large neural networks on multi‑GPU or multi‑node clusters using combined data and tensor parallel techniques, while abstracting away the complexities of distributed initialization, synchronization, and checkpoint management.", "keywords": ["model parallelism", "fully sharded data parallel", "tensor parallelism", "device mesh", "distributed training", "Lightning Fabric", "checkpointing", "gradient synchronization", "parallel strategy"], "summary_hash": "057fc4abb7ec", "cached_at": "2026-02-08T09:02:36+00:00"}