{"summary": "A processor that computes scaled dot‑product self‑attention, using PyTorch 2.0's native implementation when available.", "business_intent": "Accelerate transformer‑based model training and inference by providing an efficient, plug‑in attention component.", "keywords": ["scaled dot-product", "self-attention", "processor", "PyTorch", "transformer", "neural network", "deep learning"], "summary_hash": "3acdb9aa5841", "cached_at": "2026-02-09T03:30:08+00:00"}