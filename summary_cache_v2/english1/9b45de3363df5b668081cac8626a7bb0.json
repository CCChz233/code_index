{"summary": "Provides a functional relative position encoding mechanism for transformer attention, generating positional bias tensors based on token distances.", "business_intent": "Improve sequence models' understanding of token order and distance, enabling more accurate language processing tasks such as translation, summarization, and classification.", "keywords": ["relative position encoding", "transformer", "attention bias", "positional encoding", "sequence modeling", "NLP", "functional encoding"], "summary_hash": "2ba586d1dc5b", "cached_at": "2026-02-09T08:15:15+00:00"}