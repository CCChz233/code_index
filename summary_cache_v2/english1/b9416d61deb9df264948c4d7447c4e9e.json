{"summary": "A configurable transformer model derived from RoBERTa that can operate as a pure encoder using self‑attention or as a decoder by inserting cross‑attention layers. The mode is selected via configuration flags, allowing the model to serve both standard encoding tasks and sequence‑to‑sequence applications where encoder hidden states are supplied.", "business_intent": "Offer a reusable language‑model component that supports both encoding and decoding, enabling developers to build NLP solutions such as classification, translation, summarization, or any task requiring a flexible encoder‑decoder architecture.", "keywords": ["transformer", "encoder", "decoder", "cross‑attention", "seq2seq", "RoBERTa", "self‑attention", "language model", "head pruning", "embeddings"], "summary_hash": "368e5850254c", "cached_at": "2026-02-09T11:24:39+00:00"}