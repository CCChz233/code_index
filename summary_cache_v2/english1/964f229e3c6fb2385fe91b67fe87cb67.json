{"summary": "Manages a block-diagonal padded mask tailored for paged attention, handling the mapping between page tables and the flattened key/value tensors used in transformer attention layers.", "business_intent": "Enable efficient, scalable attention computation on long sequences by structuring keys and values into paged blocks, reducing memory overhead and supporting flexible tensor layouts.", "keywords": ["attention", "mask", "paged", "block diagonal", "keys", "values", "transformer", "padding", "pagination", "tensor conversion", "scalable"], "summary_hash": "67f87a6c4e8a", "cached_at": "2026-02-08T23:23:25+00:00"}