{"summary": "Provides reference implementations and utilities to test split‑K attention kernels against a baseline, using PyTorch and xformers FMHA operations.", "business_intent": "Validate the correctness and numerical stability of split‑K attention implementations within the library.", "keywords": ["split-k", "attention", "reference implementation", "testing", "PyTorch", "xformers", "FMHA", "numerical correctness"], "summary_hash": "0acb4198d3f6", "cached_at": "2026-02-08T23:26:36+00:00"}