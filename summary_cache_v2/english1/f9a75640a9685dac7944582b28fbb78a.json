{"summary": "A configuration container that holds all hyper‑parameters and architectural settings required to build a CPMAnt transformer model, such as vocabulary size, hidden dimensions, attention heads, feed‑forward size, layer count, dropout, positional bias, normalization epsilon, initialization scale, prompt and segment specifications, and caching behavior.", "business_intent": "Allow developers and researchers to easily define, customize, and reproduce the CPMAnt model architecture and its runtime behavior for natural‑language processing tasks.", "keywords": ["configuration", "transformer", "hyperparameters", "vocab size", "hidden size", "attention heads", "feed‑forward dimension", "layer count", "dropout", "position bias", "layer normalization", "initialization", "prompt", "segment", "caching"], "summary_hash": "938f3b2dfc9f", "cached_at": "2026-02-09T11:55:28+00:00"}