{"summary": "Implements a PyTorch module that performs graph attention network version‑2 (GATv2) convolution, computing attention scores between nodes and their neighbors, applying softmax and optional dropout, and aggregating multi‑head messages to update node embeddings. Supports homogeneous and bipartite graphs, residual connections, activation functions, bias, weight sharing, and configurable handling of zero‑in‑degree nodes.", "business_intent": "Provide a reusable, high‑performance GATv2 layer for building graph neural network models within the DGL‑PyTorch ecosystem, enabling developers to incorporate advanced attention‑based message passing into their applications.", "keywords": ["GATv2", "graph attention", "convolution", "PyTorch", "DGL", "multi‑head", "message passing", "graph neural network", "residual connection", "dropout"], "summary_hash": "21d7d011204e", "cached_at": "2026-02-09T00:46:08+00:00"}