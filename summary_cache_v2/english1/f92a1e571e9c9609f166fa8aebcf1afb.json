{"summary": "A neural network linear module that integrates Low‑Rank Adaptation (LoRA) capabilities, allowing LoRA weights to be attached, merged into the base weight matrix for efficient inference, detached for training, and used during forward propagation.", "business_intent": "Facilitate parameter‑efficient fine‑tuning and deployment of models by offering a drop‑in linear layer that can dynamically apply, fuse, or remove LoRA adaptations.", "keywords": ["linear layer", "LoRA", "low-rank adaptation", "weight fusion", "weight unfusion", "parameter-efficient fine-tuning", "neural network", "model compression", "inference optimization", "PyTorch"], "summary_hash": "e40289b20cc2", "cached_at": "2026-02-09T04:00:51+00:00"}