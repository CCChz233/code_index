{"summary": "Provides a forward multi‑head attention (MHA) operation implemented with CUTLASS kernels, offering broad configurability across data types and GPU generations, including legacy devices lacking Tensor Cores.", "business_intent": "Enable high‑performance, flexible MHA computation for transformer models on a wide range of NVIDIA GPUs, ensuring compatibility and optimal use of hardware features.", "keywords": ["multi-head attention", "CUTLASS", "GPU compatibility", "Tensor Core fallback", "xFormers", "CUDA", "SM60", "P100", "float32", "kernel optimization"], "summary_hash": "0565d8a2c206", "cached_at": "2026-02-08T23:24:24+00:00"}