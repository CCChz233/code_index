{"summary": "Implements a multi‑head attention layer that incorporates relative positional bias, handles projection of queries, keys and values, and provides utilities for reshaping tensors and pruning attention heads.", "business_intent": "Offer a configurable attention component for transformer‑based models that improves performance with relative position encoding and enables model size reduction through head pruning.", "keywords": ["attention", "relative position", "bias", "multi‑head", "projection", "head pruning", "transformer", "tensor reshaping"], "summary_hash": "223fecb4544e", "cached_at": "2026-02-09T11:03:00+00:00"}