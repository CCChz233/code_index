{"summary": "This module provides a pooling-based encoder that reduces the length of hidden representations in transformer models for automatic speech recognition, offering configurable pooling strategies and initialization settings.", "business_intent": "To improve efficiency and scalability of ASR transformer models by compressing sequence representations, enabling faster inference and lower memory consumption while maintaining performance.", "keywords": ["pooling", "encoder", "transformer", "ASR", "sequence reduction", "hidden representations", "configurable", "initialization", "torch", "NVIDIA NeMo"], "summary_hash": "0830bd9295db", "cached_at": "2026-02-08T11:18:14+00:00"}