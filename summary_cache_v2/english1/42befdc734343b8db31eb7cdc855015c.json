{"summary": "Provides a callback that captures a full‑iteration CUDA graph during model training, managing the graph lifecycle across fit, epoch, batch, and checkpoint events while intentionally omitting the data loader and learning‑rate scheduler from the captured graph.", "business_intent": "Accelerate deep‑learning training by using CUDA graphs to reduce per‑iteration kernel launch overhead, offering a reusable component that can be plugged into a training framework for higher GPU efficiency.", "keywords": ["CUDA graph", "training acceleration", "callback", "GPU performance", "iteration capture", "checkpoint integration", "setup", "teardown", "epoch handling", "batch handling"], "summary_hash": "7cf840d5cbd2", "cached_at": "2026-02-08T08:23:54+00:00"}