{"summary": "A neural network layer that randomly zeroes a fraction of its inputs during training, scaling the remaining activations to preserve overall magnitude.", "business_intent": "Provides regularization to reduce overfitting, leading to models that generalize better on unseen data and improve predictive performance in production.", "keywords": ["dropout", "regularization", "training mode", "inference mode", "random masking", "activation scaling", "noise shape", "seed", "neural network layer", "overfitting prevention"], "summary_hash": "8ac3b91fc115", "cached_at": "2026-02-09T12:01:22+00:00"}