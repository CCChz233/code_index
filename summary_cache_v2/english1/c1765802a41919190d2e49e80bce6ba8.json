{"summary": "Encapsulates the additional output layers used during the pre‑training phase of a Nezha transformer model, handling the computation of task‑specific predictions.", "business_intent": "Enable the model to learn language representations by providing the necessary heads for masked token prediction and sentence relationship tasks during pre‑training.", "keywords": ["Nezha", "pre‑training heads", "transformer", "masked language modeling", "sentence relationship", "forward computation", "neural network", "NLP"], "summary_hash": "f7bad97a816d", "cached_at": "2026-02-09T08:15:53+00:00"}