{"summary": "Implements a token-level masked language modeling head for encoder-decoder models (e.g., T5), managing model-parallel vocabulary partitioning and optional distributed output logits.", "business_intent": "Provide a scalable, parallelizable token prediction layer to support efficient training and inference of large-scale encoder-decoder models on masked language modeling tasks.", "keywords": ["masked language modeling", "token head", "encoder-decoder", "model parallel", "parallel output", "T5", "Megatron", "logits"], "summary_hash": "7caef28a054a", "cached_at": "2026-02-08T09:49:15+00:00"}