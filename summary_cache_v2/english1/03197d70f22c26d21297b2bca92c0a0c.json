{"summary": "OnlineDPOTrainer orchestrates the online Direct Preference Optimization fine‑tuning workflow for causal language models. It coordinates the main model, optional reference and reward models, a pairwise judge, configuration settings, data collation, dataset handling, tokenization, and optional PEFT adapters. The class drives data loading, token processing, training steps that incorporate preference signals, periodic evaluation, logging, checkpoint management, and automatic generation of a model card for the resulting model.", "business_intent": "Offer a ready‑to‑use solution that enables organizations and researchers to align and improve large language models through preference‑based online training, streamlining integration of reward signals, evaluation pipelines, and deployment‑ready artifacts.", "keywords": ["online DPO", "trainer", "fine‑tuning", "language model", "preference optimization", "reward model", "reference model", "pairwise judge", "data collator", "evaluation", "logging", "checkpointing", "model card", "PEFT", "transformers"], "summary_hash": "eee0561892e3", "cached_at": "2026-02-09T05:59:26+00:00"}