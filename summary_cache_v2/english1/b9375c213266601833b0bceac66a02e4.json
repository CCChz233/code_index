{"summary": "Implements a backend that integrates Intel Neural Compressor into the Optimum Benchmark framework. It handles loading pretrained language models, creating lightweight placeholder models, applying post‑training quantization, and executing inference forward passes and text generation, including cache pre‑fill handling.", "business_intent": "Enable faster and more efficient inference of transformer language models by leveraging Intel Neural Compressor quantization within a benchmark suite, helping users evaluate and deploy optimized AI workloads on Intel hardware.", "keywords": ["Intel Neural Compressor", "quantization", "language model", "inference backend", "model loading", "text generation", "cache prefill", "performance optimization", "PyTorch", "transformers", "Optimum Benchmark"], "summary_hash": "7658940dc536", "cached_at": "2026-02-09T02:30:41+00:00"}