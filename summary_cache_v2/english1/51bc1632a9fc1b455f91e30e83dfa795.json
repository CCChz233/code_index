{"summary": "Implements a hard sigmoid activation function for neural network layers, providing a fast approximation of the sigmoid with simple forward computation and output specification.", "business_intent": "Provide an efficient, low‑overhead activation mechanism to accelerate inference and training of deep learning models, particularly on resource‑constrained hardware.", "keywords": ["hard sigmoid", "activation function", "neural network", "deep learning", "inference optimization", "lightweight computation", "output specification"], "summary_hash": "a596f031fb75", "cached_at": "2026-02-09T11:34:06+00:00"}