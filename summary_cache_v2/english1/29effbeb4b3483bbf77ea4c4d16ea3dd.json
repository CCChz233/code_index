{"summary": "Encapsulates the tensors produced by a Mixture-of-Experts transformer model, including the final hidden representations, optional per‑layer hidden states, attention weight matrices, and the raw routing probabilities used by expert selectors.", "business_intent": "Standardize the delivery of MoE model results so that downstream components can retrieve sequence embeddings, layer‑wise diagnostics, attention patterns, and expert routing information for tasks such as fine‑tuning, inference, and performance analysis.", "keywords": ["MoE", "model output", "hidden states", "attention weights", "router probabilities", "tensor", "PyTorch", "sequence", "layer", "Mixture of Experts"], "summary_hash": "1e148818984c", "cached_at": "2026-02-09T06:28:24+00:00"}