{"summary": "...", "business_intent": "...", "keywords": ["self-attention", "text", "BLIP", "attention map", "gradients", "forward pass", "tensor transpose", "debugging", "interpretability"], "summary_hash": "2b7bc55c6c58", "cached_at": "2026-02-09T10:08:36+00:00"}