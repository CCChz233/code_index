{"summary": "Defines a base encoder‑decoder model class for self‑supervised speech representation learning within the NeMo ASR collection. It encapsulates data handling, forward pass, loss computation, and training/validation orchestration, providing a reusable foundation for building self‑supervised speech models.", "business_intent": "Facilitate the development and training of self‑supervised speech models that can leverage large amounts of unlabeled audio to improve downstream automatic speech recognition systems.", "keywords": ["self-supervised learning", "speech representation", "encoder-decoder", "ASR", "NeMo", "data preparation", "training loop", "loss computation", "audio preprocessing", "neural types", "PyTorch Lightning"], "summary_hash": "07a464507562", "cached_at": "2026-02-08T11:10:22+00:00"}