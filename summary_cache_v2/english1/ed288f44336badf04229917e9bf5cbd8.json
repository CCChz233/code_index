{"summary": "Implements rotary positional embeddings for GPT-NeoX, handling cosine/sine cache creation and applying the embeddings to input tensors during the forward pass.", "business_intent": "Provide an efficient positional encoding mechanism for transformer models to enhance attention computations.", "keywords": ["rotary embedding", "positional encoding", "cosine cache", "sine cache", "GPT-NeoX", "transformer", "attention", "forward pass", "tensor"], "summary_hash": "06ba51b79b73", "cached_at": "2026-02-09T08:27:54+00:00"}