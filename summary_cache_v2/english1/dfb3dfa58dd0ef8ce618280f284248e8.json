{"summary": "Encapsulates a transformer decoder module, exposing its embedding layer, decoder stack, configuration parameters (hidden size, vocabulary size, maximum sequence length) and providing a forward computation as well as utilities for model export and example inputs.", "business_intent": "Enable developers to integrate a ready‑to‑use transformer decoder for tasks such as text generation, translation, or summarization within machine‑learning pipelines.", "keywords": ["transformer", "decoder", "neural network", "embedding", "forward pass", "model export", "vocabulary size", "sequence length", "natural language generation"], "summary_hash": "a0c71c64df95", "cached_at": "2026-02-08T09:46:54+00:00"}