{"summary": "This module implements the core Parquet I/O capabilities for Dask DataFrames. It defines wrapper classes that load Parquet files, retrieve specific partitions, and manage column projection. It also provides utilities for selecting the Parquet engine, handling metadata, applying filters, and integrating these operations into Dask's highâ€‘level graph for parallel execution.", "business_intent": "To enable efficient, scalable reading and writing of Parquet datasets within Dask, allowing users to process large columnar data in parallel with support for column selection, filtering, and metadata management.", "keywords": ["parquet", "dask", "dataframe", "parallel I/O", "column projection", "filtering", "metadata", "engine abstraction", "high-level graph", "partitioning", "read", "write"], "summary_hash": "2710e39afba7", "cached_at": "2026-02-08T23:26:30+00:00"}