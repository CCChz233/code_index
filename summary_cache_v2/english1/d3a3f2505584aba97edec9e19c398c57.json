{"summary": "Generates a block‑diagonal attention bias that accommodates padded key/value sequences. Queries are split into blocks of given lengths, while keys/values are padded to a uniform block size, ensuring each query can only attend to keys within its own block and to positions that are actually used.", "business_intent": "Enable efficient, memory‑friendly attention masking for transformer models that require block‑wise attention with variable‑length key/value segments, improving performance and correctness in padded sequence scenarios.", "keywords": ["attention mask", "block diagonal", "padding", "keys", "values", "queries", "transformer", "FMHA", "sequence lengths", "mask generation"], "summary_hash": "249362b9716b", "cached_at": "2026-02-08T23:23:16+00:00"}