{"summary": "Implements a pretrained MBart transformer model adapted for sequence classification tasks, providing methods to process input token sequences, apply the MBart encoder, and produce classification logits for each input.", "business_intent": "Facilitates multilingual text classification applications such as sentiment analysis, topic detection, or intent recognition across diverse languages.", "keywords": ["MBart", "sequence classification", "multilingual", "transformer", "pretrained model", "logits", "NLP", "fine-tuning", "text classification"], "summary_hash": "e14092936efd", "cached_at": "2026-02-09T07:11:44+00:00"}