{"summary": "The module implements rubric‑based evaluation utilities for language model outputs. It defines two metric classes that automatically construct evaluation prompts and compute scores according to predefined rubrics, supporting both single‑turn and multi‑turn interactions, with one class using a reference answer and the other operating reference‑free.", "business_intent": "Provide a ready‑to‑use framework for measuring the quality of LLM responses in conversational AI products, allowing developers and researchers to benchmark and improve model performance using domain‑specific rubrics.", "keywords": ["rubric evaluation", "LLM metrics", "reference-based scoring", "reference-free scoring", "single-turn", "multi-turn", "prompt generation", "conversation assessment"], "summary_hash": "019e5802e493", "cached_at": "2026-02-08T22:49:56+00:00"}