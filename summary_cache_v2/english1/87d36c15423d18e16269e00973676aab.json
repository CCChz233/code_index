{"summary": "Encapsulates the global execution context for a distributed neural network training run, exposing configuration details such as parallelism dimensions, device allocation, checkpoint handling, experiment paths, and model metadata.", "business_intent": "Provide a centralized, queryable source of runtime state that enables coordinated operation of data, model, pipeline, and tensor parallel components across multiple processes and devices, facilitating reproducible training, checkpointing, and resource management.", "keywords": ["distributed training", "parallelism", "checkpoint", "model metadata", "MPI", "rank", "size", "device ID", "experiment directory", "logging", "random seed", "FP8", "tensor parallel", "pipeline parallel", "data parallel", "expert parallel", "versioning"], "summary_hash": "4ac24f840e8d", "cached_at": "2026-02-08T08:20:10+00:00"}