{"summary": "A neural layer that computes self‑attention weights over a sequence of frame‑level features and aggregates them into a fixed‑dimensional embedding, implementing the attention‑based pooling described in the Self‑Attention Encoding and Pooling for Speaker Recognition paper.", "business_intent": "Provide speaker recognition systems with a robust, attention‑driven pooling mechanism to generate utterance‑level embeddings from variable‑length audio inputs.", "keywords": ["self‑attention", "attention pooling", "speaker recognition", "utterance embedding", "neural network layer", "sequence aggregation", "fixed‑dimensional representation"], "summary_hash": "47a6f3f7ee32", "cached_at": "2026-02-08T09:00:47+00:00"}