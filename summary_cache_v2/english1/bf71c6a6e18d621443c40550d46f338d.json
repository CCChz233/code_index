{"summary": "A neural module that computes cross‑attention between spatial feature positions and token‑wise text embeddings, allowing each spatial location to incorporate contextual information from the textual modality.", "business_intent": "Facilitate multimodal integration of visual and textual data to enhance tasks such as image captioning, visual question answering, and other vision‑language applications.", "keywords": ["cross-attention", "spatial features", "text embeddings", "multimodal fusion", "vision-language", "attention block", "transformer"], "summary_hash": "bc041f96bb97", "cached_at": "2026-02-08T09:01:33+00:00"}