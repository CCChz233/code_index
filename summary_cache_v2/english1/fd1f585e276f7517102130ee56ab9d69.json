{"summary": "Implements rotary positional embeddings for a stable language model, handling cosine/sine cache creation and applying the embeddings during the forward pass.", "business_intent": "Provide an efficient and numerically stable positional encoding mechanism for transformer-based language models to enhance training stability and inference performance.", "keywords": ["rotary embedding", "positional encoding", "cosine sine cache", "transformer", "language model", "stable LM"], "summary_hash": "5cbd16571b7d", "cached_at": "2026-02-09T09:24:08+00:00"}