{"summary": "This test suite validates the integration of Lightning Fabric's model‑parallel strategies. It exercises feed‑forward modules under various parallel configurations (tensor parallel, FSDP2, combined sharding), checks device mesh setup, gradient clipping, checkpoint saving/loading (full and sharded), and a simple training loop to ensure correct behavior of distributed state handling and module placement.", "business_intent": "Guarantee that Lightning Fabric's advanced distributed training features work reliably for end‑users, reducing regression risk and providing confidence that model‑parallel, tensor‑parallel, and checkpointing functionalities are correctly implemented.", "keywords": ["model parallel", "tensor parallel", "FSDP2", "distributed training", "checkpointing", "gradient clipping", "device mesh", "sharded state dict", "PyTorch", "Lightning Fabric", "integration tests"], "summary_hash": "bacd99465c12", "cached_at": "2026-02-08T08:44:40+00:00"}