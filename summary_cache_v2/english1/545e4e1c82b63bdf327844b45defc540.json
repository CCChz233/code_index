{"summary": "A collection of helper utilities for constructing, splitting, and managing indexed language modeling datasets used by Megatron models in NeMo. It provides functions to build datasets of various types (e.g., BERT, T5, UL2), create masked‑language‑model predictions, handle token and token‑type creation, blend multiple datasets, truncate and pad sequences, and retrieve dataset components such as samples mapping or indexed data.", "business_intent": "Enable scalable preprocessing and preparation of large‑scale language modeling corpora for training Megatron‑style transformer models, supporting multiple model architectures and facilitating efficient data loading, masking, and splitting for downstream NLP tasks.", "keywords": ["dataset construction", "language modeling", "Megatron", "masked language modeling", "train validation test split", "indexed dataset", "tokenization", "dataset blending", "memory management", "BERT", "T5", "UL2", "BART", "ICT"], "summary_hash": "47dc4ce3140d", "cached_at": "2026-02-08T11:31:33+00:00"}