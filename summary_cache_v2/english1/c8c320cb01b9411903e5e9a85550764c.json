{"summary": "A test module that validates the correct retrieval of prompt caching metadata for various language model providers, ensuring the library reports caching support accurately.", "business_intent": "Guarantee reliable detection of prompt caching capabilities across different LLM services to enable optimized prompt reuse and cost-effective usage.", "keywords": ["prompt caching", "Anthropic", "OpenAI", "Deepseek", "unit testing", "LLM", "caching metadata", "verification"], "summary_hash": "bc409f59e53c", "cached_at": "2026-02-08T07:23:56+00:00"}