{"summary": "Encapsulates an attention module that computes attention outputs and supports pruning of attention heads.", "business_intent": "Provide a configurable attention component for neural network models, enabling efficient forward computation and model size reduction through head pruning.", "keywords": ["attention", "forward pass", "head pruning", "neural network", "model compression", "transformer"], "summary_hash": "78d6909ee71a", "cached_at": "2026-02-09T08:09:14+00:00"}