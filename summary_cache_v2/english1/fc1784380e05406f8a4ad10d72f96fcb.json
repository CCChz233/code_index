{"summary": "Implements the Swin Transformer self‑attention mechanism as a TensorFlow/Keras layer, managing weight creation, input reshaping, score computation, and output projection.", "business_intent": "Provide a reusable component for integrating Swin‑Transformer style self‑attention into computer‑vision models, enabling hierarchical feature extraction and improved model performance.", "keywords": ["self-attention", "Swin Transformer", "TensorFlow", "Keras", "vision", "transformer", "attention scores", "layer", "deep learning"], "summary_hash": "61d2703ef3ce", "cached_at": "2026-02-09T09:31:06+00:00"}