{"summary": "Provides the host‑side context that manages resources, configuration, and communication for TensorRT‑based large language model inference.", "business_intent": "Enable high‑performance LLM inference on GPUs by encapsulating the host‑side state and operations required by TensorRT.", "keywords": ["TensorRT", "LLM", "inference", "host context", "GPU", "resource management", "deep learning", "model execution"], "summary_hash": "52a9b7593105", "cached_at": "2026-02-08T10:12:40+00:00"}