{"summary": "Implements the masked language modeling head for a BERT model, projecting hidden states to vocabulary logits with configurable layer‑norm, weight initialization, and optional model‑parallel output handling.", "business_intent": "Provide BERT models with the capability to predict masked tokens during pre‑training or fine‑tuning, supporting large vocabularies and distributed training scenarios.", "keywords": ["masked language modeling", "BERT head", "vocabulary projection", "layer normalization", "model parallelism", "parallel output", "weight initialization"], "summary_hash": "b69f03e230af", "cached_at": "2026-02-08T10:12:01+00:00"}