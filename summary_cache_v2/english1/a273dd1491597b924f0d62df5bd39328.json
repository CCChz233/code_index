{"summary": "Provides a multilingual transformer model that predicts a label for each token in an input sequence by adding a token‑level classification head to the XLM‑Roberta architecture.", "business_intent": "Allow applications to perform token‑level NLP tasks such as named‑entity recognition, part‑of‑speech tagging, or other sequence labeling across many languages using a pretrained XLM‑Roberta model.", "keywords": ["XLM-Roberta", "token classification", "multilingual", "transformer", "named entity recognition", "sequence labeling", "pretrained model", "deep learning"], "summary_hash": "9291d198b04e", "cached_at": "2026-02-09T07:33:27+00:00"}