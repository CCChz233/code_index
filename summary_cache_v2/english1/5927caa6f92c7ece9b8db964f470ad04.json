{"summary": "The module offers a utility to apply rotary positional embeddings to tensors that include padding, optionally using Triton for accelerated computation and integrating with blockâ€‘diagonal causal masks for attention.", "business_intent": "Enable efficient and correct positional encoding in transformer models that process padded batches, improving performance and accuracy for downstream NLP or vision tasks.", "keywords": ["rotary positional embedding", "ROPE", "padding support", "transformer", "attention bias", "block diagonal causal mask", "triton acceleration", "tensor manipulation", "sequence length handling"], "summary_hash": "1f362165029a", "cached_at": "2026-02-08T23:29:52+00:00"}