{"summary": "Encapsulates a single BERT training or inference instance, linking it to its originating editing task and storing the associated feature set, while offering utilities to retrieve token‑level labels and to pad sequences to a uniform length.", "business_intent": "Facilitate the preparation and handling of data samples for BERT models in editing‑task workflows, enabling consistent input formatting and label realization during model training and prediction.", "keywords": ["BERT", "example", "training", "inference", "token labels", "padding", "editing task", "features", "NLP", "sequence length"], "summary_hash": "140c602d5df1", "cached_at": "2026-02-08T09:58:42+00:00"}