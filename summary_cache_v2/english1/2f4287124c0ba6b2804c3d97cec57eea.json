{"summary": "Implements a Lightning Fabric strategy that orchestrates fully sharded data parallel (FSDP) training across multiple XLA devices. It wraps modules with FSDP, manages device placement, communication, activation checkpointing, and provides flexible checkpointing modes (full, sharded, sequential). An internal utility controls gradient synchronization during the backward pass.", "business_intent": "Enable scalable and memory‑efficient training of large models on TPUs or other XLA hardware by leveraging FSDP within the Lightning ecosystem.", "keywords": ["XLA", "FSDP", "Fully Sharded Data Parallel", "Lightning Fabric", "distributed training", "TPU", "activation checkpointing", "checkpointing", "gradient synchronization", "multi‑device parallelism"], "summary_hash": "0b21f66cdad3", "cached_at": "2026-02-08T09:02:06+00:00"}