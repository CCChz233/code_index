{"summary": "Provides KERPLE relative position embeddings that introduce linear bias terms into attention scores, supporting both forward (auto‑regressive) and backward (bidirectional) distance calculations for decoder and encoder layers.", "business_intent": "Enable transformer‑based NLP models to incorporate efficient, bias‑driven positional information, improving generation quality and encoder representations while maintaining low computational overhead.", "keywords": ["relative position embedding", "linear bias", "attention", "auto‑regressive decoder", "bidirectional encoder", "KERPLE", "transformer", "positional encoding"], "summary_hash": "f4316ab32a0d", "cached_at": "2026-02-08T09:48:18+00:00"}