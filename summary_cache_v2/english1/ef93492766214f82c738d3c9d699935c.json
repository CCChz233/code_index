{"summary": "Provides utilities to configure a PyTorch distributed runtime and to wrap functions for parallel execution across multiple processes or GPUs.", "business_intent": "Simplify the deployment of scalable, distributed machine learning tasks, reducing the effort required to set up and manage parallel training or inference.", "keywords": ["PyTorch", "distributed computing", "parallel execution", "decorator pattern", "multi-GPU", "environment configuration", "scalable training"], "summary_hash": "3e805071ae0f", "cached_at": "2026-02-08T08:32:20+00:00"}