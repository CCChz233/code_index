{"summary": "Implements a custom attention processor that injects image prompt features into the cross‑attention mechanism of a diffusion model, scaling and reshaping the image embeddings based on configurable hidden size, token length, and optional memory‑efficient xformers support.", "business_intent": "Provide an efficient way to condition generative models on visual inputs by adapting attention layers to incorporate image embeddings, improving image‑guided generation while managing memory and computational load.", "keywords": ["attention processor", "IP‑Adapter", "image prompt", "cross attention", "hidden size", "scale factor", "token context length", "memory efficient", "xformers", "diffusion model"], "summary_hash": "8e028f6ab7dc", "cached_at": "2026-02-09T03:30:30+00:00"}