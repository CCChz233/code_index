{"summary": "Implements a custom autograd operation that forwards the input unchanged while supplying a user‑defined gradient transformation, enabling straight‑through estimation for non‑differentiable functions.", "business_intent": "Facilitates training of models that use discrete or quantized operators (e.g., sign, binarization) by providing a gradient proxy, improving convergence in low‑precision or binary neural network deployments.", "keywords": ["straight-through estimation", "custom autograd", "gradient mapping", "identity forward", "gradient clipping", "hardtanh", "quantization", "binary activation", "PyTorch"], "summary_hash": "84a81a7f8a67", "cached_at": "2026-02-09T11:43:57+00:00"}