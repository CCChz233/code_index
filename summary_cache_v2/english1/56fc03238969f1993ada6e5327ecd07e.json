{"summary": "Implements a layer‑normalization operation that normalizes each sample in a batch across designated feature axes, computes per‑sample mean and variance, applies epsilon for numerical stability, and optionally scales and shifts the normalized output with trainable gamma and beta parameters, with an alternative RMS‑based scaling mode.", "business_intent": "Offer a per‑example normalization layer to improve training stability and speed in deep neural networks by reducing internal covariate shift, while providing configurable scaling, centering, and efficient RMS scaling options.", "keywords": ["layer normalization", "per-sample normalization", "trainable scale", "trainable shift", "gamma", "beta", "epsilon", "RMS scaling", "deep learning", "neural network regularization", "Keras layer"], "summary_hash": "07e73a652bbd", "cached_at": "2026-02-09T11:58:13+00:00"}