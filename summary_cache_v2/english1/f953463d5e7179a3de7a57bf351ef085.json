{"summary": "Provides a lightweight text preprocessing component that splits raw strings into basic tokens by handling punctuation, case conversion, accent removal, and optional Chinese character segmentation while respecting a whitelist of tokens that must stay intact.", "business_intent": "Prepares raw textual data for natural language processing models by performing essential tokenization steps required before applying more sophisticated tokenizers such as WordPiece or BPE.", "keywords": ["tokenization", "lowercasing", "punctuation splitting", "accent stripping", "Chinese character handling", "preserve tokens", "text preprocessing", "NLP"], "summary_hash": "7e11efc30d95", "cached_at": "2026-02-09T09:26:28+00:00"}