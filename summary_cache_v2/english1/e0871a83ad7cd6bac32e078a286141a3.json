{"summary": "Provides a lazily generated causal attention mask that blocks attention to future tokens in sequence models, with an adjustable offset to shift the triangular mask up or down.", "business_intent": "Enables efficient autoregressive processing in transformer‑based NLP or time‑series models by supplying on‑demand masks that enforce causality, reducing memory usage and preventing invalid softmax operations.", "keywords": ["causal mask", "attention", "transformer", "lazy evaluation", "offset", "autoregressive", "mask generation", "future token blocking"], "summary_hash": "9ce06f68154b", "cached_at": "2026-02-09T11:49:12+00:00"}