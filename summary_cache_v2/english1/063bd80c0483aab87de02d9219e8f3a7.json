{"summary": "Implements a mixed‑precision plugin for PyTorch Lightning that leverages PyTorch's native automatic mixed precision (AMP). It manages gradient scaling, optimizer unscaling, and precision handling for both training and inference, providing a seamless interface for using lower‑precision arithmetic.", "business_intent": "Accelerate model training and reduce GPU memory consumption by automatically using float16/bfloat16 where safe, while preserving numerical stability and model accuracy, thereby simplifying AMP adoption for Lightning users.", "keywords": ["mixed precision", "AMP", "gradient scaling", "optimizer unscaling", "PyTorch Lightning", "training acceleration", "memory optimization", "precision plugin"], "summary_hash": "591eb3c0429b", "cached_at": "2026-02-08T09:00:09+00:00"}