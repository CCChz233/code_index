{"summary": "Manages attention decoding with split int4 quantized key/value caches, handling initialization and state for lowâ€‘bit transformer inference.", "business_intent": "Accelerate transformer model inference by reducing memory usage and bandwidth through int4 quantization of the KV cache.", "keywords": ["attention", "decoding", "int4 quantization", "key-value cache", "split", "transformer", "inference optimization", "low-bit", "memory efficiency"], "summary_hash": "d67aee1bf9de", "cached_at": "2026-02-08T23:15:44+00:00"}