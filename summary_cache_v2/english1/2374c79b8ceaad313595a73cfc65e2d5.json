{"summary": "Loads a tab‑separated file containing concept IDs and synonym strings, tokenizes each concept with a HuggingFace tokenizer up to a maximum token length, and yields tokenized examples suitable for training an entity‑linking encoder or for constructing a nearest‑neighbor index.", "business_intent": "Enable efficient preparation of training data and index building for entity linking systems that map textual mentions to canonical concepts.", "keywords": ["entity linking", "dataset", "tokenizer", "concept synonyms", "training data", "nearest neighbor index", "tokenization", "max sequence length", "data loading", "retrieval"], "summary_hash": "772c67de8527", "cached_at": "2026-02-08T09:53:58+00:00"}