{"summary": "Provides an abstraction and concrete implementation for communicating with a Triton inference server to generate text from large language models, offering both batch and streaming query capabilities.", "business_intent": "Enable applications to integrate LLM-powered text generation services hosted on Triton, supporting high‑throughput batch requests and low‑latency streaming responses for NLP workloads.", "keywords": ["LLM", "text generation", "Triton inference server", "client", "batch processing", "streaming", "NLP", "deployment", "inference"], "summary_hash": "6027eedab8ad", "cached_at": "2026-02-08T12:11:52+00:00"}