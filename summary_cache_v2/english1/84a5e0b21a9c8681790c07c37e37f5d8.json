{"summary": "Provides a context manager that temporarily disables gradient synchronization for Fully Sharded Data Parallel (FSDP) modules, restoring the original sync state upon exit.", "business_intent": "Enable efficient gradient accumulation in distributed training by turning off automatic gradient all-reduce during a scoped block, reducing communication overhead and allowing manual synchronization later.", "keywords": ["FSDP", "gradient synchronization", "no-sync", "context manager", "distributed training", "performance optimization"], "summary_hash": "1854cfcefb48", "cached_at": "2026-02-08T08:26:02+00:00"}