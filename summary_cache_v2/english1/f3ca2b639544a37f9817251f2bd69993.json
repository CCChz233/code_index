{"summary": "The module implements the decoder side of Transformer architectures, providing a full decoder that processes target sequences, attends to encoder outputs, and maintains recurrent memory, as well as a configurable decoder layer that performs masked self‑attention, encoder‑decoder attention, and a position‑wise feed‑forward network with flexible normalization and activation options.", "business_intent": "To supply reusable, configurable decoder building blocks for NLP models such as machine translation, text generation, and speech‑to‑text within the NeMo framework, enabling developers to construct and train Transformer‑based sequence generation systems efficiently.", "keywords": ["Transformer", "decoder", "self-attention", "encoder-decoder attention", "multi-head attention", "feed-forward network", "layer normalization", "dropout", "activation function", "pre‑layernorm", "post‑layernorm", "recurrent memory", "PyTorch", "NeMo"], "summary_hash": "ab9d9c72e446", "cached_at": "2026-02-08T11:22:36+00:00"}