{"summary": "Implements a Performer attention layer for Flax models, delivering linear‑time self‑attention with reduced memory and computational cost.", "business_intent": "Provide an efficient, scalable attention component for large‑scale transformer architectures used in natural language processing, recommendation systems, and other sequence modeling applications.", "keywords": ["Performer", "linear attention", "Flax", "JAX", "transformer layer", "efficient attention", "scalable sequence modeling", "neural network"], "summary_hash": "c1c6540fa235", "cached_at": "2026-02-09T06:00:36+00:00"}