{"summary": "This module implements a high‑level client for Amazon Bedrock language models, handling request construction for the /converse and /invoke endpoints, translating OpenAI‑style parameters to native formats, managing streaming event‑stream decoding, and processing responses for chat completions, text completions, and embeddings. It also provides configuration support for Cohere models, a mock iterator for testing, and utilities for response shape handling.", "business_intent": "Allow the Litellm framework to seamlessly integrate with Amazon Bedrock LLM services, offering unified APIs for chat, completion, streaming, and embedding use‑cases while abstracting AWS‑specific details.", "keywords": ["Amazon Bedrock", "LLM client", "chat completion", "streaming", "invoke API", "converse API", "event stream decoder", "Cohere configuration", "embeddings", "mock response iterator", "async handling"], "summary_hash": "8d8b117efe78", "cached_at": "2026-02-08T08:00:58+00:00"}