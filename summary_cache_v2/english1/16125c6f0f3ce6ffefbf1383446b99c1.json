{"summary": "Provides modules that convert input images into sequences of patch tokens for transformer models. The implementation extracts non‑overlapping patches, optionally pads inputs to meet window size requirements, projects each patch to a fixed token dimension via a convolutional layer, applies optional layer normalization, and can add sinusoidal or learned positional embeddings to the token stream.", "business_intent": "Facilitate the integration of Swin‑Transformer style architectures into medical imaging pipelines by offering ready‑to‑use patch embedding layers that handle tokenization, padding, projection, normalization, and positional encoding, thereby simplifying model development for segmentation, classification, and detection tasks.", "keywords": ["patch embedding", "Swin Transformer", "tokenization", "positional encoding", "convolutional projection", "layer normalization", "automatic padding", "medical imaging", "deep learning", "MONAI"], "summary_hash": "a2a548158c7f", "cached_at": "2026-02-08T13:30:06+00:00"}