{"summary": "Provides a class that builds and manages attention masks for transformer attention mechanisms, offering utilities to create causal, padding, and custom masks compatible with PyTorch tensors.", "business_intent": "Facilitate flexible generation and handling of attention masks to control token interactions in transformer models, ensuring correct attention calculations across various masking scenarios.", "keywords": ["attention mask", "transformer", "PyTorch", "causal mask", "padding mask", "mask generation", "tensor"], "summary_hash": "34b938f531c2", "cached_at": "2026-02-08T23:30:59+00:00"}