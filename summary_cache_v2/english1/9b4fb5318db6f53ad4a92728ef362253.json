{"summary": "A neural network module that adds a masked language modeling head to a RoBERTa backbone, handling the projection of hidden states to vocabulary logits and providing a forward computation for MLM predictions.", "business_intent": "Facilitate the fine‑tuning or pre‑training of RoBERTa models on masked language modeling tasks, enabling downstream NLP applications that require contextual token prediction.", "keywords": ["RoBERTa", "masked language modeling", "MLM head", "transformer", "NLP", "language model", "fine‑tuning", "vocabulary projection", "deep learning"], "summary_hash": "4aec701ae287", "cached_at": "2026-02-09T11:08:44+00:00"}