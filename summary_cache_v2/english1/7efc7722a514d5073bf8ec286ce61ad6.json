{"summary": "Implements a scaled dot-product attention processor that integrates query/key normalization, rotary positional embeddings, and perturbed attention guidance for the HunyuanDiT model, leveraging PyTorch 2.0 optimizations.", "business_intent": "Provide a high‑performance, guided attention mechanism for transformer‑based generative models, enhancing output quality and control via perturbation‑based attention guidance.", "keywords": ["scaled dot-product attention", "rotary embedding", "normalization layer", "perturbed attention guidance", "PyTorch 2.0", "HunyuanDiT", "attention processor"], "summary_hash": "b4861d8e37f9", "cached_at": "2026-02-09T04:06:33+00:00"}