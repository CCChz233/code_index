{"summary": "Implements high‑performance forward and backward multi‑head attention kernels using the Composable Kernel library, handling diverse attention bias masks and providing utilities for alignment, sequence‑length extraction, and tensor‑bias handling.", "business_intent": "Accelerate transformer workloads by offering optimized GPU attention operations that support custom masking and bias configurations, thereby improving training and inference efficiency.", "keywords": ["multi-head attention", "forward kernel", "backward kernel", "Composable Kernel", "GPU acceleration", "attention bias", "mask handling", "alignment checks", "tensor bias", "sequence length"], "summary_hash": "8e7e8457b045", "cached_at": "2026-02-08T23:33:01+00:00"}