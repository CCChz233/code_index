{"summary": "A neural module that maps BERT hidden representations to vocabulary logits, enabling the model to predict masked tokens in a sequence.", "business_intent": "Supports masked language modeling tasks for applications like text completion, error detection, and preâ€‘training of language models.", "keywords": ["BERT", "masked language modeling", "logits", "embeddings", "token prediction", "NLP", "language model head", "transformer"], "summary_hash": "01e9379ff0f2", "cached_at": "2026-02-08T13:18:35+00:00"}