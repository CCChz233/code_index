{"summary": "A lightweight, trainable adapter that inserts a single hidden‑layer feed‑forward network with optional LayerNorm and dropout into a larger model. It mirrors the input dimension, applies an activation, and initializes its output layer to zeros so that, when disabled, it leaves the original model unchanged.", "business_intent": "Provide a parameter‑efficient way to fine‑tune or specialize large pretrained models for new tasks or domains by adding minimal, controllable transformations that can be turned on or off without impacting the base model.", "keywords": ["adapter", "linear feedforward", "layer normalization", "activation function", "dropout", "zero‑initialized output layer", "parameter‑efficient fine‑tuning", "pre/post normalization", "residual composition", "hidden dimension"], "summary_hash": "ccc3749f7030", "cached_at": "2026-02-08T08:24:31+00:00"}