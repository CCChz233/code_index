{"summary": "Implements a gated activation function that combines a linear transformation with a GELU nonlinearity for use in neural network layers.", "business_intent": "Provide an efficient, high‑performance activation component for deep‑learning models, especially transformer‑style architectures.", "keywords": ["activation", "gated linear unit", "GELU", "neural network", "forward pass", "deep learning"], "summary_hash": "22c38a75c506", "cached_at": "2026-02-08T08:50:53+00:00"}