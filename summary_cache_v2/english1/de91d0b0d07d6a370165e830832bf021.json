{"summary": "Provides a PyTorch‑based implementation of a graph attention convolution operator for DGL, aggregating neighbor node features with learnable attention coefficients. Supports multi‑head attention, optional dropout, residual connections, activation functions, bias terms, and works with both homogeneous and unidirectional bipartite graphs while safely handling nodes with zero in‑degree.", "business_intent": "Enable developers and researchers to construct graph neural network models that leverage attention mechanisms for tasks such as node classification, link prediction, and graph representation learning, accelerating the development of advanced graph‑based AI solutions.", "keywords": ["graph attention network", "GAT", "convolution", "PyTorch", "DGL", "multi‑head attention", "dropout", "residual connection", "activation", "bias", "homogeneous graph", "bipartite graph", "zero in‑degree handling", "edge softmax"], "summary_hash": "cd670e200210", "cached_at": "2026-02-09T00:46:28+00:00"}