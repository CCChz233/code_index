{"summary": "Implements a specialized attention processor that integrates image prompt features into a model's cross‑attention mechanism, handling scaling and token length configuration for IP‑Adapter usage.", "business_intent": "Enable efficient and configurable incorporation of image‑based prompts into diffusion or transformer models, improving performance and memory usage for IP‑Adapter applications.", "keywords": ["attention processor", "IP-Adapter", "image prompt", "cross-attention", "hidden size", "scale factor", "token context length", "memory efficient", "xformers"], "summary_hash": "8e028f6ab7dc", "cached_at": "2026-02-09T03:31:30+00:00"}