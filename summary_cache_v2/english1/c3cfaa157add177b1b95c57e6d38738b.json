{"summary": "Implements a multi‑layer transformer decoder specialized for table data. It repeatedly updates query embeddings through self‑attention and cross‑attention with encoder outputs, optionally adding object queries and positional embeddings, and can return intermediate layer activations when auxiliary loss is enabled.", "business_intent": "Provide a decoding mechanism that extracts and predicts table structures (cells, rows, columns) from encoded visual features, supporting enhanced training via auxiliary supervision.", "keywords": ["transformer decoder", "self-attention", "cross-attention", "query embeddings", "object queries", "position embeddings", "auxiliary loss", "table transformer", "multi‑layer", "intermediate activations"], "summary_hash": "dded21beab5c", "cached_at": "2026-02-09T10:11:49+00:00"}