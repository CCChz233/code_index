{"summary": "A thin wrapper around a Marian causal language model decoder that ensures pretrained checkpoints are loaded correctly when the decoder is used together with an EncoderDecoderModel, providing a forward method that delegates to the underlying decoder.", "business_intent": "Facilitate seamless integration of Marian decoder checkpoints into encoderâ€‘decoder architectures, simplifying model loading and reducing configuration errors.", "keywords": ["Marian", "decoder wrapper", "pretrained checkpoint", "causal language model", "EncoderDecoderModel", "model loading", "forward pass"], "summary_hash": "9e7d29d2ef15", "cached_at": "2026-02-09T11:28:44+00:00"}