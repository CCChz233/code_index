{"summary": "A utility model that constructs 4‑dimensional attention masks required by transformer‑style layers, converting input specifications into a mask tensor ready for use in attention calculations.", "business_intent": "Enable downstream neural network components to apply correct attention constraints by providing pre‑computed 4D masks, simplifying model integration and improving inference reliability.", "keywords": ["attention mask", "4D", "transformer", "preprocessing", "mask generation", "tensor", "neural network"], "summary_hash": "5b1bac9cc7a1", "cached_at": "2026-02-09T04:17:24+00:00"}