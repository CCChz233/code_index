{"summary": "The module supplies builder utilities for assembling TensorRT‑LLM compatible language models. It creates decoder layers, embeddings, layer‑norms, and optional quantization, wraps them into a language‑model head or generic model, and provides helpers for preparing inputs, executing forward passes, and printing diagnostic information.", "business_intent": "Facilitate the export and high‑performance GPU inference of large language models from NVIDIA NeMo by generating TensorRT‑LLM ready architectures with optional quantization and Lora support.", "keywords": ["TensorRT", "LLM", "model export", "decoder", "quantization", "language model head", "builder", "inference", "NeMo", "transformer", "layernorm", "embedding", "attention"], "summary_hash": "95bb992a92b3", "cached_at": "2026-02-08T11:39:06+00:00"}