{"summary": "Defines the pretraining head modules for a lightweight BERT variant, encapsulating the layers needed to compute masked token predictions and sentence relationship scores, and provides a forward routine to process hidden states from the encoder.", "business_intent": "Enable efficient training of a compact BERT model on language modeling objectives, facilitating downstream NLP applications that require a small-footprint transformer.", "keywords": ["MobileBERT", "pretraining heads", "masked language modeling", "sentence prediction", "neural network", "transformer", "forward pass", "model initialization"], "summary_hash": "33746b2b412f", "cached_at": "2026-02-09T11:37:29+00:00"}