{"summary": "Implements a linear transformation whose weight matrix is split column‑wise across multiple devices, handling local weight slices, optional bias, and the necessary communication to produce the full output.", "business_intent": "Facilitate scalable training and inference of large neural networks by distributing the heavy linear layer computation across GPUs/TPUs, reducing per‑device memory and compute load.", "keywords": ["column parallelism", "linear layer", "model parallelism", "weight partitioning", "distributed training", "GPU scaling", "tensor parallel", "deep learning"], "summary_hash": "1d4bf7004052", "cached_at": "2026-02-08T23:18:02+00:00"}