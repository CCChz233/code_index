{"summary": "Encapsulates configurable tile dimensions used by FlashAttention kernels, allowing users to specify block sizes that influence execution speed while keeping numerical results unchanged.", "business_intent": "Provide a structured way to set, retrieve, and validate block size parameters for forward and backward passes of attention kernels, ensuring optimal GPU performance.", "keywords": ["FlashAttention", "tile sizes", "block configuration", "GPU performance", "kernel tuning", "parameter validation", "attention computation"], "summary_hash": "85fff6be3545", "cached_at": "2026-02-09T11:48:39+00:00"}