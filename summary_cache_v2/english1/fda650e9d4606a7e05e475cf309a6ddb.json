{"summary": "Provides functionality to calculate per-head attention bias tensors for graph-based transformer models, integrating graph structural information into the attention mechanism.", "business_intent": "Enables more accurate graph representation learning by incorporating structural bias into multi-head attention, supporting applications such as molecular property prediction, node classification, and other graph analytics tasks.", "keywords": ["attention bias", "graph transformer", "multi-head attention", "graph structure", "neural network", "Graphormer", "bias computation"], "summary_hash": "384cff4791a1", "cached_at": "2026-02-09T10:18:58+00:00"}