{"summary": "A dynamically expanding cache that stores per‑layer key and value tensors for transformer‑based generative models, automatically growing as new tokens are produced and exposing list‑like access and iteration.", "business_intent": "Enable fast, memory‑efficient attention reuse in generative AI pipelines by maintaining and updating a flexible key/value cache that adapts to varying sequence lengths and integrates with legacy cache formats.", "keywords": ["dynamic cache", "transformer", "key-value storage", "generative models", "token generation", "attention optimization", "sequence length handling", "legacy conversion", "cache reordering"], "summary_hash": "d8c5f00e1f65", "cached_at": "2026-02-09T06:21:12+00:00"}