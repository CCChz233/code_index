{"summary": "Implements a relative‑position multi‑head attention module that combines the Transformer‑XL style attention with Longformer’s sliding‑window local attention and optional global tokens, supporting caching and dropout for processing very long sequences.", "business_intent": "Enable scalable transformer‑based NLP services (e.g., document classification, summarization, search) by providing an efficient attention layer that handles long inputs with reduced memory and compute costs.", "keywords": ["multi-head attention", "relative positional bias", "sliding window", "local attention", "global attention", "Transformer-XL", "Longformer", "caching", "dropout", "long sequence processing"], "summary_hash": "66fe934c3456", "cached_at": "2026-02-08T09:28:34+00:00"}