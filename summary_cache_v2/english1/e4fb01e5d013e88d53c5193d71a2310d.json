{"summary": "We need to produce JSON with summary, business_intent, keywords list.\n\nWe have file-level metadata: path, docstring empty, imports list, globals none, class summaries for GPTSFTDataset and GPTSFTPackedDataset (the second summary truncated with \"...\").\n\nWe need to infer responsibilities: dataset wrapper for supervised fine-tuning of GPT-style language models, handling tokenization, prompt templating, truncation, loss mask, attention mask, collate function.\n\nBusiness intent: enable training of GPT models with supervised fine-tuning using custom datasets, facilitating efficient data loading and preprocessing for large-scale language model training.\n\nKeywords: dataset, GPT, fine-tuning, tokenization, prompt, loss mask, attention mask, collate, Megatron, language modeling, PyTorch, NeMo, text memmap, online sample mapping, etc.\n\nNow produce JSON.\n\nMake sure no function names verbatim (but class names maybe okay? Not required to avoid). But we can mention dataset wrapper.\n\nLet's craft summary: \"Provides dataset classes that load and preprocess text data for supervised fine‑tuning of GPT‑style language models within the NeMo Megatron framework. The classes handle tokenization, optional prompt templates, truncation to a maximum length, creation of loss and attention masks, and supply a collate function for batching.\"\n\nBusiness intent: \"Facilitate efficient preparation of training data for GPT fine‑tuning jobs, enabling researchers and engineers to quickly integrate custom text corpora into NeMo Megatron pipelines.\"\n\nKeywords list.\n\nLet's output JSON.assistantfinal{\n  \"summary\": \"Implements dataset classes that load, validate, and preprocess text examples for supervised fine‑tuning of GPT‑style language models in the NeMo Megatron framework. The classes manage tokenization, optional prompt templating, truncation, generation of loss and attention masks, and provide a collate function for creating batched inputs.\",\n  \"business_intent\": \"Enable rapid and scalable preparation of training data for GPT fine‑tuning workflows, allowing developers to integrate custom text corpora into NeMo Megatron pipelines with efficient loading and preprocessing.\",\n  \"keywords\": [\n    \"dataset\",\n    \"GPT\",\n    \"fine-tuning\",\n    \"supervised learning\",\n    \"tokenization\",\n    \"prompt template\",\n    \"truncation\",\n    \"loss mask\",\n    \"attention mask\",\n    \"collate function\",\n    \"Megatron\",\n    \"NeMo\",\n    \"language modeling\",\n    \"PyTorch\",\n    \"", "business_intent": "We need to produce JSON with summary, business_intent, keywords list.\n\nWe have file-level metadata: path, docstring empty, imports list, globals none, class summaries for GPTSFTDataset and GPTSFTPackedDataset (the second summary truncated with \"...\").\n\nWe need to infer responsibilities: dataset wrapper for supervised fine-tuning of GPT-style language models, handling tokenization, prompt templating, truncation, loss mask, attention mask, collate function.\n\nBusiness intent: enable training of GPT models with supervised fine-tuning using custom datasets, facilitating efficient data loading and preprocessing for large-scale language model training.\n\nKeywords: dataset, GPT, fine-tuning, tokenization, prompt, loss mask, attention mask, collate, Megatron, language modeling, PyTorch, NeMo, text memmap, online sample mapping, etc.\n\nNow produce JSON.\n\nMake sure no function names verbatim (but class names maybe okay? Not required to avoid). But we can mention dataset wrapper.\n\nLet's craft summary: \"Provides dataset classes that load and preprocess text data for supervised fine‑tuning of GPT‑style language models within the NeMo Megatron framework. The classes handle tokenization, optional prompt templates, truncation to a maximum length, creation of loss and attention masks, and supply a collate function for batching.\"\n\nBusiness intent: \"Facilitate efficient preparation of training data for GPT fine‑tuning jobs, enabling researchers and engineers to quickly integrate custom text corpora into NeMo Megatron pipelines.\"\n\nKeywords list.\n\nLet's output JSON.assistantfinal{\n  \"summary\": \"Implements dataset classes that load, validate, and preprocess text examples for supervised fine‑tuning of GPT‑style language models in the NeMo Megatron framework. The classes manage tokenization, optional prompt templating, truncation, generation of loss and attention masks, and provide a collate function for creating batched inputs.\",\n  \"business_intent\": \"Enable rapid and scalable preparation of training data for GPT fine‑tuning workflows, allowing developers to integrate custom text corpora into NeMo Megatron pipelines with efficient loading and preprocessing.\",\n  \"keywords\": [\n    \"dataset\",\n    \"GPT\",\n    \"fine-tuning\",\n    \"supervised learning\",\n    \"tokenization\",\n    \"prompt template\",\n    \"truncation\",\n    \"loss mask\",\n    \"attention mask\",\n    \"collate function\",\n    \"Megatron\",\n    \"NeMo\",\n    \"language modeling\",\n    \"PyTorch\",\n    \"", "keywords": [], "summary_hash": "181bfea6a750", "cached_at": "2026-02-08T11:31:39+00:00"}