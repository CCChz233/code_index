{"summary": "Encapsulates a causal attention mask that blocks future positions in sequence models, ensuring each token can only attend to itself and preceding tokens.", "business_intent": "Support autoregressive inference in language or time‑series models by providing a ready‑to‑use mask that enforces proper causal ordering.", "keywords": ["causal mask", "attention", "transformer", "autoregressive", "sequence modeling", "mask retrieval"], "summary_hash": "5ec339b02339", "cached_at": "2026-02-09T11:59:49+00:00"}