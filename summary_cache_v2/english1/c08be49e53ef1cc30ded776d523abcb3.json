{"summary": "Implements a neural‑network layer that adds an activity‑based regularization term to the model’s loss while forwarding its input unchanged.", "business_intent": "Enable models to discourage large activations, promoting sparsity and better generalization during training.", "keywords": ["activity regularization", "neural network layer", "loss penalty", "sparsity", "overfitting mitigation", "training regularizer", "TensorFlow", "Keras"], "summary_hash": "1bb6fa762986", "cached_at": "2026-02-09T11:18:02+00:00"}