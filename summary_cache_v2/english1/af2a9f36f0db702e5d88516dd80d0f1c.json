{"summary": "Provides a handler that connects to Azure OpenAI's realtime endpoint, constructs the appropriate service URL, and manages asynchronous websocket communication to stream chat interactions through the LiteLLM proxy.", "business_intent": "Enable LiteLLM users to utilize Azure OpenAI's real‑time conversational capabilities with low‑latency streaming over websockets.", "keywords": ["Azure OpenAI", "realtime API", "websocket", "asynchronous communication", "chat streaming", "LiteLLM proxy", "service URL construction", "low latency", "conversational AI"], "summary_hash": "28ec47c844be", "cached_at": "2026-02-08T08:11:48+00:00"}