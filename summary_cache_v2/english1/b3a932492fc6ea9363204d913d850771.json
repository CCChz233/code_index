{"summary": "Implements core sparse linear algebra primitives for CSR‑formatted tensors, including forward and backward computations for sparse softmax, sampled dense‑dense matrix multiplication (SDDMM) and sparse‑dense matrix multiplication (SPMM). These utilities convert between CSR and COO representations and provide gradient support for deep‑learning models.", "business_intent": "Enable high‑performance sparse operations in neural network layers, particularly transformer‑based architectures, to reduce memory and compute costs while preserving differentiability.", "keywords": ["CSR", "sparse softmax", "SDDMM", "SPMM", "gradient computation", "PyTorch", "efficient attention", "sparse tensors", "deep learning"], "summary_hash": "a0f21941befd", "cached_at": "2026-02-08T23:28:01+00:00"}