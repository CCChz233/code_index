{"summary": "A model class that adapts a GPT-2 transformer for token-level classification, providing pretrained weight loading and a forward pass that yields per-token logits for sequence labeling tasks such as named entity recognition or part-of-speech tagging.", "business_intent": "Allow developers to apply GPT-2 to token classification problems, enabling accurate information extraction and custom NLP pipelines without building a transformer from scratch.", "keywords": ["GPT-2", "token classification", "sequence labeling", "pretrained transformer", "NLP", "entity extraction", "fine-tuning", "token-level prediction"], "summary_hash": "5add0c4fc13a", "cached_at": "2026-02-09T07:05:32+00:00"}