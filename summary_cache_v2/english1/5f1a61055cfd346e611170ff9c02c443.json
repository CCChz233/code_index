{"summary": "This module implements the integration layer between Litellm and Google Vertex AI / Google AI Studio Gemini models. It defines configuration objects that translate Gemini‑specific generation settings into OpenAI‑style parameters, handles request construction, response parsing, streaming iteration, and error handling. The core wrapper class exposes synchronous, asynchronous, and streaming generation interfaces, supporting features such as function calling, image handling, safety settings, and token usage tracking.", "business_intent": "Provide Litellm users with a seamless, OpenAI‑compatible interface to generate text (and multimodal) completions using Google Vertex AI and Google AI Studio Gemini models, enabling flexible deployment, streaming responses, and robust error handling across sync and async workflows.", "keywords": ["Vertex AI", "Google AI Studio", "Gemini", "LLM wrapper", "configuration mapping", "streaming responses", "asynchronous calls", "function calling", "image processing", "safety settings", "token usage", "error handling", "OpenAI compatibility"], "summary_hash": "565120b04169", "cached_at": "2026-02-08T07:58:42+00:00"}