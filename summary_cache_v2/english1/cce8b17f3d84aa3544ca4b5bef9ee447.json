{"summary": "A component that maps a hidden state vector to a log‑probability distribution over the vocabulary, implementing a linear projection (with optional weight sharing with the embedding layer) followed by a log‑softmax, used for next‑token generation in sequence models.", "business_intent": "Provide token probability scores for language generation or next‑word prediction tasks, facilitating decoders to sample or select the next token.", "keywords": ["next token generation", "log softmax", "weight sharing", "embedding", "linear projection", "language model", "vocabulary distribution"], "summary_hash": "f8e3aca86f69", "cached_at": "2026-02-08T23:29:34+00:00"}