{"summary": "Implements the decoder component of the DETR model, stacking configurable decoder layers that iteratively refine object query embeddings via self‑attention and cross‑attention with encoder outputs, handling query position embeddings and optionally returning intermediate layer activations for auxiliary loss.", "business_intent": "Provide the core transformer decoding mechanism for end‑to‑end object detection, transforming learned queries into final detection predictions while supporting auxiliary supervision during training.", "keywords": ["DETR", "transformer decoder", "self-attention", "cross-attention", "object queries", "query position embeddings", "auxiliary loss", "layer stacking", "feature refinement"], "summary_hash": "5f159045d2d9", "cached_at": "2026-02-09T09:22:51+00:00"}