{"summary": "Applies absolute positional encoding to an input tensor, scaling and adding the encoding directly without a residual connection.", "business_intent": "Injects positional information into sequence data for models like Transformers, enabling downstream layers to recognize token order.", "keywords": ["positional encoding", "absolute", "adapter", "sequence length", "scaling factor", "no residual", "tensor transformation", "transformer"], "summary_hash": "34ec33212e80", "cached_at": "2026-02-08T09:36:34+00:00"}