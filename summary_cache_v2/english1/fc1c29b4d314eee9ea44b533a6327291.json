{"summary": "Implements a gated recurrent unit layer that processes sequential data, dynamically choosing between a cuDNN‑optimized GPU kernel or a native TensorFlow implementation based on runtime conditions. It maintains internal weights for input and recurrent transformations, supports configurable activations, dropout, bias handling, statefulness, sequence output options, and reset‑gate conventions.", "business_intent": "Enable developers to build efficient, scalable models for time‑dependent tasks such as language modeling, speech recognition, and forecasting by providing a flexible, high‑performance recurrent layer that automatically leverages available hardware acceleration.", "keywords": ["GRU", "gated recurrent unit", "recurrent neural network", "sequence modeling", "cuDNN", "GPU acceleration", "TensorFlow", "dropout", "stateful", "unroll", "reset gate", "activation", "bias"], "summary_hash": "a0775cce828e", "cached_at": "2026-02-09T11:59:48+00:00"}