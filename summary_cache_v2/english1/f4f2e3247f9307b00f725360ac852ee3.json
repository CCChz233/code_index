{"summary": "Provides a PyTorch-compatible dataset that prepares text samples for BERT pretraining, handling tokenization, masking of token IDs, sequence length alignment, and optional truncation of sentence pairs, while exposing document retrieval functionality.", "business_intent": "Facilitates efficient creation of training data for BERT language models on proprietary or domain-specific corpora, supporting masked language modeling and nextâ€‘sentence prediction tasks to improve downstream NLP performance.", "keywords": ["BERT", "pretraining", "dataset", "tokenization", "masking", "sequence truncation", "next sentence prediction", "NLP", "machine learning", "data loading"], "summary_hash": "c683f98a27de", "cached_at": "2026-02-08T09:56:10+00:00"}