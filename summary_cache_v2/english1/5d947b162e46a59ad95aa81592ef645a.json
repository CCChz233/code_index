{"summary": "Implements a Megatron‑compatible dataset that creates training samples for T5 models using a prefix language‑model objective combined with span‑corruption patterns, enabling the model to be fine‑tuned for selective forgetting of token spans.", "business_intent": "Supports privacy‑compliant model updates by providing data for unlearning or removal of specific information from pretrained T5 language models.", "keywords": ["T5", "dataset", "language modeling", "prefix LM", "span corruption", "unlearning", "Megatron", "GPTDataset", "length distribution", "numpy"], "summary_hash": "db6858bafae5", "cached_at": "2026-02-08T11:31:00+00:00"}