{"summary": "Provides the core evaluation engine for the function‑call leaderboard, coordinating configuration loading, checker selection (e.g., AST, executable, multi‑turn, relevance), execution of test cases, score aggregation, and leaderboard updates. Includes supporting utilities for response validation, accuracy calculation, data preprocessing, result logging, cost and latency measurement, and CSV generation of leaderboard data.", "business_intent": "Enable systematic benchmarking of function‑calling AI models by automating test execution, performance measurement, and public ranking, facilitating research comparison and model improvement.", "keywords": ["evaluation", "function calling", "leaderboard", "benchmarking", "checker", "AST", "executable", "multi‑turn", "relevance", "scoring", "configuration", "validation", "accuracy", "preprocessing", "logging", "cost tracking", "latency", "CSV", "model comparison"], "summary_hash": "6e966a412690", "cached_at": "2026-02-08T12:46:34+00:00"}