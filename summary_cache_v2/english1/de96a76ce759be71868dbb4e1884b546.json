{"summary": "Encapsulates the default hyperparameter settings for the AdamW optimizer, offering a lightweight container for configuring learning rate, betas, epsilon, weight decay, and related options without relying on a NeMo Config object.", "business_intent": "Provide a reusable, standardized way to specify AdamW optimizer parameters across training pipelines, ensuring consistency and simplifying experiment configuration.", "keywords": ["AdamW", "optimizer", "hyperparameters", "configuration", "defaults", "learning rate", "betas", "epsilon", "weight decay", "PyTorch"], "summary_hash": "694bfe53b995", "cached_at": "2026-02-08T10:15:37+00:00"}