{"summary": "Implements a TensorFlow loss module for BERT-style pretraining that jointly optimizes masked language modeling and next‑sentence prediction, automatically skipping any positions marked with a sentinel label (-100).", "business_intent": "Facilitates efficient training of mobile‑optimized BERT models by providing a combined NSP/MLM loss function suitable for large‑scale language model pretraining pipelines.", "keywords": ["TensorFlow", "BERT", "pretraining loss", "masked language modeling", "next sentence prediction", "ignore label -100", "mobile BERT", "language model training"], "summary_hash": "d0a031618895", "cached_at": "2026-02-09T11:35:05+00:00"}