{"summary": "Provides a bias-aware GELU activation that automatically selects a TorchScript-compatible implementation for inference while employing an autograd-enabled version during training to ensure precise gradient computation.", "business_intent": "Facilitate reliable and highâ€‘performance activation handling in Bloom models across both training and production environments, supporting deployment and scalability.", "keywords": ["GELU", "bias", "activation", "TorchScript", "autograd", "training", "inference", "gradient accuracy", "model deployment", "Megatron", "DeepSpeed"], "summary_hash": "14a3681a20c1", "cached_at": "2026-02-09T11:31:21+00:00"}