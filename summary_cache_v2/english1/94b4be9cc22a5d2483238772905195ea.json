{"summary": "Implements a handler that intercepts Vertex AI streaming responses in the LiteLLM proxy, assembles the full response from incremental chunks, forwards the data to the client, and creates structured logging payloads describing the content generation, request metadata and model information.", "business_intent": "Provide observability for Vertex AI pass‑through endpoints by capturing detailed logs of streamed responses while preserving the original streaming behavior for downstream consumers.", "keywords": ["Vertex AI", "streaming", "logging", "proxy", "pass‑through", "response assembly", "content generation", "model identifier", "HTTP", "async"], "summary_hash": "aa83b9a90136", "cached_at": "2026-02-08T07:51:53+00:00"}