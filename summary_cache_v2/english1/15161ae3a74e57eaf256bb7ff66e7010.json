{"summary": "A transformer-based model class that leverages a Megatron-BERT architecture to perform token-level classification tasks such as named entity recognition or part-of-speech tagging.", "business_intent": "Enable enterprises and developers to apply a highâ€‘capacity, pretrained Megatron-BERT model for accurate and scalable token classification in natural language processing applications.", "keywords": ["Megatron-BERT", "token classification", "transformer", "NLP", "pretrained model", "sequence labeling", "deep learning"], "summary_hash": "e8f805284057", "cached_at": "2026-02-09T07:12:31+00:00"}