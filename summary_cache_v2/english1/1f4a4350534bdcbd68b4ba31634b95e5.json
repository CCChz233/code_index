{"summary": "Provides a block-diagonal attention mask where each query block can only attend to keys within the same block, while the key/value sequences may contain gaps, enabling sparse attention patterns for efficient transformer computation.", "business_intent": "Facilitates high-performance multi-head attention by generating and handling masks that enforce block-wise locality with irregular key/value layouts, reducing computational cost in large-scale models.", "keywords": ["block diagonal mask", "gappy keys", "sparse attention", "transformer", "FMHA", "mask materialization", "paged mask", "sequence lengths"], "summary_hash": "e8be7b6f80a4", "cached_at": "2026-02-08T23:23:29+00:00"}