{"summary": "Defines a stochastic attention module that creates random attention masks and applies them to query, key, and value tensors during the forward pass, integrating with the xformers attention framework and supporting optional sparsification.", "business_intent": "Offer a lightweight, randomised attention mechanism for transformer architectures to reduce computational load and introduce regularisation through stochastic masking.", "keywords": ["random attention", "stochastic mask", "transformer", "scaled dot product", "sparsify", "attention pattern", "PyTorch", "xformers"], "summary_hash": "b448f606e7a0", "cached_at": "2026-02-08T23:31:48+00:00"}