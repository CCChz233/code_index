{"summary": "Implements a single transformer decoder block for the Whisper speech‑to‑text model using Flax. The setup method configures attention and feed‑forward sub‑modules, while the call method defines the forward computation of the layer.", "business_intent": "Provide a reusable, high‑performance decoder layer component for building Whisper models in Flax, enabling efficient speech recognition and transcription pipelines.", "keywords": ["Flax", "Whisper", "decoder layer", "transformer", "self‑attention", "cross‑attention", "feed‑forward", "JAX", "neural network", "speech recognition"], "summary_hash": "e20e8fb6e649", "cached_at": "2026-02-09T10:53:45+00:00"}