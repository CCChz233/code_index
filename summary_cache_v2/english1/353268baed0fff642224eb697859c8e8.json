{"summary": "Provides dataset utilities for token‑level classification tasks, handling tokenization, feature extraction, label encoding, padding, caching, and preparation of both training and inference examples for BERT‑based models.", "business_intent": "To streamline the creation and loading of token classification data (e.g., NER) for training and deploying BERT models within the NeMo framework, reducing preprocessing overhead and supporting efficient inference.", "keywords": ["token classification", "dataset", "BERT", "tokenization", "feature extraction", "label encoding", "padding", "caching", "NLP", "NeMo", "inference"], "summary_hash": "3c49ec5a93a7", "cached_at": "2026-02-08T11:28:00+00:00"}