{"summary": "Implements a BERT-based embedding component that transforms tokenized text into contextual hidden representations, returning tensors shaped as sequence length × batch size × hidden dimension.", "business_intent": "Supply high‑quality contextual word embeddings for downstream NLP applications such as semantic search, text classification, or model fine‑tuning.", "keywords": ["BERT", "embedding", "NeMo", "language model", "contextual representations", "sequence", "hidden size", "NLP", "transformer", "inference"], "summary_hash": "e66dcc87c135", "cached_at": "2026-02-08T10:10:05+00:00"}