{"summary": "Implements the attention layer used in DINO v2 vision transformers, handling query, key, and value projections and providing a forward computation, with the ability to prune attention heads for model compression.", "business_intent": "Provide an efficient self‑attention component for vision transformer models and enable reduction of model size and inference cost by pruning unnecessary attention heads.", "keywords": ["attention", "vision transformer", "DINO v2", "head pruning", "neural network", "model compression", "self‑attention", "feature extraction"], "summary_hash": "4c9d0b465221", "cached_at": "2026-02-09T08:52:36+00:00"}