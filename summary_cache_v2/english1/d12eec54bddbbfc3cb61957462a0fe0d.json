{"summary": "Implements a dataset that loads and preprocesses text data for GPT‑style embedding models used in information retrieval, handling tokenization, sample mapping, attention‑mask generation, type conversion and efficient batch collation.", "business_intent": "Enable fast and scalable preparation of text inputs for training or inference of GPT embedding models within NeMo pipelines, supporting large‑scale retrieval applications.", "keywords": ["GPT embedding", "information retrieval", "dataset", "tokenization", "attention mask", "batch collation", "PyTorch", "NeMo", "text preprocessing", "sample mapping"], "summary_hash": "945ac633a7ed", "cached_at": "2026-02-08T11:28:37+00:00"}