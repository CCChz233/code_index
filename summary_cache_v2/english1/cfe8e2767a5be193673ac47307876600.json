{"summary": "Implements a non‑learnable positional encoding layer that generates sinusoidal embeddings for each position up to a configurable maximum sequence length, matching the model's hidden dimension.", "business_intent": "Provides a ready‑to‑use component for transformer‑style models to inject deterministic order information into token embeddings without adding trainable parameters.", "keywords": ["positional encoding", "sinusoidal", "fixed", "transformer", "embedding", "sequence length", "hidden size", "deterministic", "non‑learnable"], "summary_hash": "314aef28655b", "cached_at": "2026-02-08T09:47:11+00:00"}