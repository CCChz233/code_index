{"summary": "Provides an attention mechanism specialized for abstract syntax tree inputs, handling initialization, forward computation, and optional pruning of attention heads.", "business_intent": "Support neural models that process code structures by delivering efficient AST-focused attention and enabling model compression through head pruning.", "keywords": ["attention", "AST", "neural network", "transformer", "head pruning", "forward pass", "model compression"], "summary_hash": "6feee33f7596", "cached_at": "2026-02-09T11:06:20+00:00"}