{"summary": "Provides utilities to analyze a transformer model's attention heads and automatically identify heads that exhibit known functional patterns such as duplicate‑token, induction, and previous‑token behaviors. It computes similarity scores between head attention patterns and predefined detection templates, validates model dimensions, and exposes helpers to retrieve supported patterns and detection results.", "business_intent": "Enable researchers and engineers to quickly locate and study specialized attention heads in large language models for interpretability, debugging, and model‑editing tasks.", "keywords": ["transformer", "attention head", "pattern detection", "activation cache", "similarity score", "induction head", "duplicate token head", "previous token head", "model interpretability", "mechanistic analysis"], "summary_hash": "dfbb38c81ba8", "cached_at": "2026-02-08T13:21:27+00:00"}