{"summary": "Encapsulates the full set of tensors returned by a Table Transformer encoder‑decoder model, including final decoder hidden states, per‑layer hidden states and attention maps for both encoder and decoder, as well as an optional stack of layer‑normalized intermediate decoder activations for auxiliary loss computation.", "business_intent": "Enables downstream applications such as table‑to‑text generation, data extraction, or fine‑tuning by providing detailed model outputs needed for analysis, debugging, and training with auxiliary decoding objectives.", "keywords": ["table transformer", "encoder-decoder", "model output", "hidden states", "attention weights", "cross attention", "intermediate activations", "auxiliary loss", "layernorm", "PyTorch", "tensor"], "summary_hash": "76a71285798a", "cached_at": "2026-02-09T10:12:14+00:00"}