{"summary": "The module is an example script that shows how to train a NeMo speech‑to‑text model using BPE or word‑piece tokenization with K2 graph‑based sequence models. It includes guidance for preparing the tokenizer, optionally building a token‑level language model for MMI loss, and running the training with configurable loss types, graph topologies, and distributed settings.", "business_intent": "Enable researchers and engineers to quickly set up and benchmark end‑to‑end automatic speech recognition pipelines that leverage subword tokenization and advanced graph‑based criteria (CTC or MMI) within the NeMo framework.", "keywords": ["ASR", "speech-to-text", "BPE", "word‑piece", "tokenizer", "K2", "graph loss", "CTC", "MMI", "token‑level language model", "NeMo", "training script", "Hydra configuration", "distributed training"], "summary_hash": "b60887ffc422", "cached_at": "2026-02-08T10:40:41+00:00"}