{"summary": "Implements the post‑attention processing for an AST model, applying a linear projection and dropout to the attention output while delegating the residual addition to the surrounding layer.", "business_intent": "Provides a modular component for building AST‑based neural networks that separates residual handling from output transformation, supporting pre‑layer‑norm architectures.", "keywords": ["transformer", "self‑attention", "output processing", "layer normalization", "residual connection", "dropout", "neural network module", "ASTLayer"], "summary_hash": "89f086c23c3e", "cached_at": "2026-02-09T11:06:18+00:00"}