{"summary": "...", "business_intent": "...", "keywords": ["self-attention", "parallel", "transformer", "memory allocation", "checkpointing", "tensor transpose", "forward pass", "sequence modeling"], "summary_hash": "bfbb7ec3aef9", "cached_at": "2026-02-08T09:48:41+00:00"}