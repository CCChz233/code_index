{"summary": "Utility module that constructs TensorRT inference engines, handling configuration, model parsing, and serialization to enable highâ€‘performance GPU inference.", "business_intent": "Provide a streamlined way to generate optimized TensorRT engines for deep learning models, improving inference speed and efficiency within the NeMo ecosystem.", "keywords": ["TensorRT", "engine building", "inference optimization", "GPU", "CUDA", "model parsing", "serialization", "performance"], "summary_hash": "e775283d12c6", "cached_at": "2026-02-08T10:49:19+00:00"}