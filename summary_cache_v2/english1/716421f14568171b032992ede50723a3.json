{"summary": "Provides an implementation of the Novograd optimizer, an adaptive stochastic gradient method that maintains per‑layer moment estimates to adjust learning rates, supporting gradient averaging, weight decay, and the AMSGrad variant.", "business_intent": "Enables users of the MONAI framework to train deep learning models—particularly in medical imaging—more effectively by offering a specialized optimizer that can improve convergence speed and stability compared to standard SGD or Adam.", "keywords": ["Novograd", "optimizer", "adaptive gradient", "stochastic gradient descent", "weight decay", "AMSGrad", "PyTorch", "deep learning", "medical imaging", "MONAI"], "summary_hash": "629c4d4fe303", "cached_at": "2026-02-08T13:07:11+00:00"}