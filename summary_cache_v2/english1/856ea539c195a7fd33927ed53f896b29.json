{"summary": "Provides example training scripts that demonstrate how to fine‑tune Sentence‑Transformer bi‑encoder models on the MS MARCO passage ranking dataset, covering two loss strategies (multiple‑negatives ranking and margin‑based MSE). The scripts handle data loading, model setup, training loops, evaluation, and checkpointing to produce dense retrieval models.", "business_intent": "Enable developers and researchers to build and customize high‑performance dense retrieval models for search and question‑answering applications by leveraging the large MS MARCO dataset and advanced loss functions.", "keywords": ["MS MARCO", "bi‑encoder", "dense retrieval", "sentence transformer", "MultipleNegativesRankingLoss", "MarginMSELoss", "hard negatives", "passage ranking", "embedding training", "information retrieval"], "summary_hash": "6f1bc08f086a", "cached_at": "2026-02-08T14:01:04+00:00"}