{"summary": "Utility module offering helper routines for constructing and manipulating attention bias tensors, generating random sequence lengths, packing key/value caches, and providing reference attention implementations for transformer models.", "business_intent": "Facilitate the creation, testing, and benchmarking of custom attention bias configurations and reference attention calculations within transformer architectures.", "keywords": ["attention bias", "transformer", "utility functions", "random sequence generation", "key/value cache packing", "reference attention", "torch", "fmha"], "summary_hash": "8cd4bc7c586b", "cached_at": "2026-02-08T23:25:31+00:00"}