{"summary": "Encapsulates the attention bias mechanism for a LeViT vision transformer, managing bias tensor creation, retrieval, and application during forward computation, and allowing mode switching between training and inference.", "business_intent": "Enable efficient subsampled attention in vision transformer models to reduce computational load and memory consumption while maintaining performance.", "keywords": ["LeViT", "vision transformer", "attention bias", "subsampling", "forward pass", "training mode", "inference", "neural network optimization"], "summary_hash": "f86f98f8f1c4", "cached_at": "2026-02-09T08:38:43+00:00"}