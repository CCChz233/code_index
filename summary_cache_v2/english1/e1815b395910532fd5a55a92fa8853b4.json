{"summary": "Encapsulates the tensors produced by a table transformer decoder, including the final hidden states, optional per‑layer hidden states, self‑ and cross‑attention weights, and an optional stack of layer‑normalized intermediate activations for each decoder layer, which can be used for auxiliary decoding losses.", "business_intent": "Supply a standardized container for decoder outputs that downstream components can consume for tasks such as table understanding, generation, or training with auxiliary objectives.", "keywords": ["decoder output", "hidden states", "attention weights", "cross attention", "intermediate activations", "layernorm", "auxiliary loss", "table transformer", "model tensors"], "summary_hash": "2af91acb5b8f", "cached_at": "2026-02-09T10:12:10+00:00"}