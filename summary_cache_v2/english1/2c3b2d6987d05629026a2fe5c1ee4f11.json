{"summary": "Implements a learnable Laplacian positional encoding that converts the smallest non‑trivial Laplacian eigenvectors of a graph into fixed‑size node embeddings using either a multi‑head Transformer encoder or a permutation‑invariant DeepSet network, with optional batch normalization and a post‑processing MLP.", "business_intent": "Supply expressive, trainable positional features for graph neural network models—especially graph transformers—to enhance their ability to capture structural information and improve performance on downstream tasks such as node classification, link prediction, and graph classification.", "keywords": ["laplacian", "positional encoding", "graph neural network", "transformer", "deepset", "eigenvectors", "embedding", "batch normalization", "mlp", "graph transformer"], "summary_hash": "6154f55ad787", "cached_at": "2026-02-09T00:42:54+00:00"}