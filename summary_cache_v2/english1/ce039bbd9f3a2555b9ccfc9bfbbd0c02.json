{"summary": "A sampler that extends PyTorch's DistributedSampler to allow each process to receive a possibly different number of samples when the dataset size cannot be evenly split among the replicas, while preserving optional shuffling and other configuration options such as seed and drop_last.", "business_intent": "Enable reliable distributed training on datasets whose size is not evenly divisible by the number of workers, ensuring each rank gets an appropriate subset of data without forcing artificial padding or duplication.", "keywords": ["distributed training", "PyTorch", "sampler", "nonâ€‘even division", "rank", "replicas", "shuffling", "dataset indexing", "drop_last", "seed"], "summary_hash": "9df528cbafde", "cached_at": "2026-02-08T11:22:05+00:00"}