{"summary": "Computes the impact of systematically occluding regions of an input image on a neural network’s output probabilities, producing a sensitivity map that highlights important areas and an image indicating the most likely class for each occlusion scenario.", "business_intent": "Provides an interpretability tool for deep‑learning models, especially in medical imaging, allowing developers and clinicians to visualize and understand which image regions drive model predictions, thereby aiding debugging, trust, and regulatory compliance.", "keywords": ["occlusion sensitivity", "model interpretability", "visualization", "importance map", "deep learning", "image analysis", "probability change", "region importance", "PyTorch", "medical imaging"], "summary_hash": "08611d692fd6", "cached_at": "2026-02-08T13:08:15+00:00"}