{"summary": "The module supplies a high‑level Python wrapper around NCCL to create and manage communication groups across multiple GPUs, generate and share NCCL unique identifiers, and perform sparse all‑to‑all collective operations for efficient multi‑GPU data exchange.", "business_intent": "Enable scalable, high‑performance distributed training of graph neural networks by simplifying NCCL‑based GPU communication within the DGL framework.", "keywords": ["NCCL", "communication groups", "multi‑GPU", "sparse all‑to‑all", "unique identifier", "distributed training", "CUDA", "DGL", "collective operations"], "summary_hash": "3672b2162a06", "cached_at": "2026-02-09T00:59:08+00:00"}