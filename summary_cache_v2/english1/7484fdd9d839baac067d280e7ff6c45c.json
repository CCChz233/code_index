{"summary": "Provides a NumPy‑based implementation of the RNN‑Transducer (RNNT) loss, including forward and backward algorithms, log‑softmax activation handling, and optional FastEmit regularization, wrapped as a PyTorch autograd Function for integration with speech recognition models.", "business_intent": "Enables training and evaluation of automatic speech recognition (ASR) systems using the RNNT objective, offering a reference CPU implementation that can be used for debugging, benchmarking, or as a fallback when GPU‑accelerated versions are unavailable.", "keywords": ["RNNT loss", "RNN‑Transducer", "speech recognition", "ASR", "NumPy", "PyTorch autograd", "FastEmit", "log‑softmax gradient", "forward‑backward algorithm", "sequence modeling"], "summary_hash": "92c1c992c0ff", "cached_at": "2026-02-08T11:16:43+00:00"}