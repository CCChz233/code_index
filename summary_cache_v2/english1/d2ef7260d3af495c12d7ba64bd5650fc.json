{"summary": "A dataset wrapper that generates training examples for the UL2 language model, supporting three pre‑training objectives—short‑span masking, extreme‑span masking, and prefix language modeling—by constructing masked sequences and adding appropriate task tokens.", "business_intent": "Supply a ready‑to‑use data pipeline for pre‑training or fine‑tuning UL2 models, enabling developers and researchers to efficiently produce masked samples for multiple objectives in NLP applications.", "keywords": ["UL2", "dataset", "pretraining", "masking", "short span", "extreme span", "prefix language modeling", "NLP", "language model", "sample generation"], "summary_hash": "f86e70b22093", "cached_at": "2026-02-08T10:00:40+00:00"}