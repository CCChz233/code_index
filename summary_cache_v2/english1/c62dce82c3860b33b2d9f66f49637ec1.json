{"summary": "Utility module that supports bitsandbytes‑based quantization for diffusion models. It offers helpers to verify library availability, manage integration hooks, and convert or replace model parameters and linear layers between standard and bitsandbytes formats, including dequantization of weights.", "business_intent": "Enable seamless use of bitsandbytes low‑bit quantization within the library, allowing models to be efficiently loaded, dequantized, and have their linear components swapped for optimized implementations, thereby reducing memory usage and improving performance.", "keywords": ["bitsandbytes", "quantization", "dequantization", "linear layer replacement", "accelerate integration", "torch compatibility", "utility functions"], "summary_hash": "154222ce1057", "cached_at": "2026-02-09T05:16:20+00:00"}