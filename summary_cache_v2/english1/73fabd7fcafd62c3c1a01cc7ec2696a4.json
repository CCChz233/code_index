{"summary": "The module supplies μP‑scaled optimizer wrappers for Megatron models, offering ready‑to‑use Adam, AdamW, and SGD variants that automatically apply μP gradient scaling, along with utilities for processing parameter groups and guidance for adapting other optimizers.", "business_intent": "Enable developers to train large‑scale neural networks, particularly language models, with stable and efficient μP scaling by providing plug‑and‑play optimizer implementations.", "keywords": ["μP scaling", "optimizer wrapper", "Adam", "AdamW", "SGD", "Megatron", "deep learning", "PyTorch", "parameter groups", "large language models"], "summary_hash": "b65adc91f594", "cached_at": "2026-02-08T11:25:59+00:00"}