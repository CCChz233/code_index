{"summary": "Implements a trainer that fine‑tunes transformer models with the Kullback‑Leibler‑based (KTO) objective, using a reference model to estimate implicit rewards and handling all aspects of data preparation, loss calculation, evaluation, logging, and optional PEFT/LoRA adapter integration.", "business_intent": "Enable developers to efficiently train language models on preference or reward‑based tasks without explicit reward labels, leveraging KL divergence and reference models to improve model alignment and performance while supporting scalable training infrastructures.", "keywords": ["KTO", "KL divergence", "trainer", "fine-tuning", "transformer", "reference model", "reward estimation", "PEFT", "LoRA", "data loading", "evaluation", "logging", "accelerate", "deepspeed", "HuggingFace"], "summary_hash": "ae1fc2aa53e1", "cached_at": "2026-02-09T05:59:10+00:00"}