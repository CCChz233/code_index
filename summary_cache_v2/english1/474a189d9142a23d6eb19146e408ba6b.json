{"summary": "The module provides example training scripts that illustrate how to build, configure, and train speech‑to‑text models using a hybrid RNN‑Transducer and CTC loss architecture with NVIDIA NeMo. It demonstrates preparation of tokenizers (character or sub‑word/BPE), integration with Hydra/YAML configuration, data loading, optimizer setup, and optional fine‑tuning or restoration of pretrained checkpoints.", "business_intent": "Help developers and researchers quickly prototype and fine‑tune high‑accuracy ASR systems that combine RNNT and CTC decoding, reducing development time and showcasing best practices for scalable speech recognition pipelines.", "keywords": ["ASR", "hybrid RNNT", "CTC", "NeMo", "speech-to-text", "training script", "Hydra", "tokenization", "BPE", "character", "PyTorch Lightning", "fine-tuning", "model restoration"], "summary_hash": "bf04959af908", "cached_at": "2026-02-08T11:57:17+00:00"}