{"summary": "Implements a rotary positional embedding for GPTNeoX models that dynamically adjusts NTK scaling based on sequence length, generating cosine and sine caches for efficient attention calculations.", "business_intent": "Provide flexible and accurate positional encoding for transformer models across varying sequence lengths, enhancing model performance and adaptability in production environments.", "keywords": ["rotary embedding", "dynamic NTK scaling", "positional encoding", "GPTNeoX", "transformer", "cosine-sine cache", "neural tangent kernel"], "summary_hash": "6d0957b4b8ca", "cached_at": "2026-02-09T08:28:00+00:00"}