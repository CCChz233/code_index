{"summary": "Provides a multi-head attention module that computes attention outputs and supports removal of selected heads to streamline the model.", "business_intent": "Enable integration of an efficient, configurable attention layer into neural network architectures, allowing model size and inference speed improvements through head pruning.", "keywords": ["attention", "multi-head", "transformer", "neural network", "model compression", "head pruning", "deep learning"], "summary_hash": "b3b2cc2aa669", "cached_at": "2026-02-09T09:44:31+00:00"}