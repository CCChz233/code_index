{"summary": "This package implements a framework for automatically generating synthetic test sets used to evaluate retrieval‑augmented generation (RAG) systems. It defines abstract and concrete query synthesizers that extract themes, concepts, and keyphrases from document clusters, refine question‑answer pairs through iterative prompting, and produce both abstract and specific search queries. A central generator coordinates language‑model prompting, embedding similarity, optional knowledge‑graph enrichment, and document source handling (e.g., LangChain or LlamaIndex) to assemble evaluation datasets, while utilities support data splitting and analytics tracking. Data models describe the resulting test set and its samples, linking each to its originating synthesizer and cost metadata.", "business_intent": "Automate the creation of high‑quality, diverse synthetic evaluation data for RAG pipelines, reducing manual effort in test‑set construction and enabling systematic benchmarking of retrieval and generation performance across varied query styles and complexities.", "keywords": ["synthetic test set", "query generation", "retrieval‑augmented generation", "RAG evaluation", "prompt engineering", "knowledge graph enrichment", "embedding similarity", "LangChain", "LlamaIndex", "data splitting", "analytics"], "summary_hash": "9130b04710e2", "cached_at": "2026-02-08T22:52:44+00:00"}