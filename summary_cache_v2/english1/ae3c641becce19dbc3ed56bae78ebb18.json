{"summary": "Provides a callback that automatically tracks and logs learning‑rate values (and optionally momentum and weight‑decay) from optimizers and their learning‑rate schedulers throughout model training. It supports configurable logging intervals (step or epoch) and systematic naming of the logged metrics for seamless integration with Lightning’s logging system.", "business_intent": "Enable users to monitor training hyper‑parameters in real time, facilitating debugging, hyper‑parameter tuning, and reproducibility by exposing learning‑rate dynamics (and related optimizer state) to logging and visualization tools.", "keywords": ["learning rate", "monitor", "scheduler", "optimizer", "logging", "callback", "PyTorch Lightning", "training", "metrics", "momentum", "weight decay", "interval"], "summary_hash": "254e473dbf50", "cached_at": "2026-02-08T08:55:01+00:00"}