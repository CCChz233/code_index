{"summary": "Encapsulates the configuration parameters for a transformer decoder layer, offering utility accessors for hidden dimensions, feed‑forward size, mixture‑of‑experts status, and loading from NeMo checkpoints.", "business_intent": "Facilitates the construction and integration of decoder layers in neural language models, enabling consistent configuration handling for tasks such as translation, summarization, and language generation.", "keywords": ["decoder", "layer configuration", "hidden size", "ffn hidden size", "mixture of experts", "NeMo", "transformer", "model config", "utility methods"], "summary_hash": "f0d69d19c79c", "cached_at": "2026-02-08T10:13:19+00:00"}