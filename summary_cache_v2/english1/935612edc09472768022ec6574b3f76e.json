{"summary": "Mixin that equips a model core with self‑attention operations, handling the creation of query, key and value tensors and supporting adapter registration for flexible extension.", "business_intent": "Enable reusable self‑attention logic within a model core, simplifying integration of attention layers and adapter modules in transformer architectures.", "keywords": ["self-attention", "mixin", "query tensors", "key tensors", "value tensors", "adapter registration", "transformer", "modular", "neural network", "model core"], "summary_hash": "f43166069017", "cached_at": "2026-02-08T09:51:44+00:00"}