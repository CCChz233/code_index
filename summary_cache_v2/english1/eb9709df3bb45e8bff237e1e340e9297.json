{"summary": "Implements a high‑performance approximation of the Gaussian Error Linear Unit (GeLU) using a tanh‑based formula, exposed as a PyTorch module via a fast C extension.", "business_intent": "Provide a faster yet sufficiently accurate GeLU activation to speed up training and inference of deep‑learning models in PyTorch, reducing computational cost.", "keywords": ["GeLU", "tanh approximation", "C extension", "PyTorch", "activation function", "performance optimization", "neural networks", "fast inference"], "summary_hash": "3ad023a0704c", "cached_at": "2026-02-09T06:23:12+00:00"}