{"summary": "Configuration holder for performing greedy, batched inference with a Recurrent Neural Network Transducer (RNNT) model.", "business_intent": "Allows developers to specify and manage inference parameters such as batch size and decoding settings to enable fast, lowâ€‘latency RNNT decoding in production speech recognition pipelines.", "keywords": ["RNNT", "greedy decoding", "batched inference", "configuration", "speech recognition", "model inference", "parameters"], "summary_hash": "45ec4a761f8b", "cached_at": "2026-02-08T09:31:27+00:00"}