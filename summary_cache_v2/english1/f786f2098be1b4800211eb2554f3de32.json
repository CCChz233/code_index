{"summary": "Implements a Luong‑style attention layer that computes similarity scores between query and key tensors (or uses the value tensor as key), optionally scales and drops out the scores, applies optional padding, causal, and custom masks, normalizes with softmax, and produces a weighted sum of the value tensors, optionally returning the attention distribution.", "business_intent": "Offer a configurable attention component for neural network models that need to focus on relevant parts of sequential data, supporting encoder‑decoder, transformer, and other sequence‑to‑sequence architectures during training and inference.", "keywords": ["attention", "dot-product", "Luong", "query", "key", "value", "softmax", "scaling", "dropout", "masking", "causal mask", "weighted sum", "sequence modeling"], "summary_hash": "a01fbb4def63", "cached_at": "2026-02-09T11:58:51+00:00"}