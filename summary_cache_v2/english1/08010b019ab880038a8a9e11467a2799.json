{"summary": "This test module validates the integration of the Fully Sharded Data Parallel (FSDP) strategy within Lightning Fabric. It provides lightweight trainer utilities and a suite of tests that exercise model wrapping, training steps, gradient clipping, activation checkpointing, compilation re‑application, checkpoint saving/loading (both full and sharded), device placement, and optimizer state handling to ensure the strategy behaves correctly in distributed scenarios.", "business_intent": "Guarantee that the FSDP strategy in Lightning Fabric functions reliably for end‑to‑end distributed training workflows, covering model sharding, checkpoint management, gradient handling, and compatibility with advanced features such as activation checkpointing and torch.compile.", "keywords": ["FSDP", "Lightning Fabric", "distributed training", "testing", "trainer", "model wrapping", "gradient clipping", "activation checkpointing", "checkpointing", "sharded state dict", "optimizer state", "torch.compile"], "summary_hash": "b1674da9b65e", "cached_at": "2026-02-08T08:45:19+00:00"}