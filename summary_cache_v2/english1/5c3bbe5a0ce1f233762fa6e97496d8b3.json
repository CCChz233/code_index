{"summary": "The module implements a metric that evaluates how well a generated response or multi‑turn conversation remains on a specified topic. It defines a class that integrates with the RAGAS metric framework, using LLM‑driven prompts and optional embeddings to compute a relevance score for each dialogue turn.", "business_intent": "Enable developers and data scientists to automatically assess and improve the topical relevance of AI‑generated answers in conversational or retrieval‑augmented generation systems, ensuring higher quality and user satisfaction.", "keywords": ["topic adherence", "relevance score", "dialogue evaluation", "multi‑turn conversation", "LLM metric", "RAGAS", "AI response quality", "prompt‑based scoring", "natural language processing", "evaluation metric"], "summary_hash": "ad0d83a91fba", "cached_at": "2026-02-08T22:50:30+00:00"}