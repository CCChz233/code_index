{"summary": "Implements a trainer that fine-tunes a causal language model using the NashMD algorithm, integrating a reference model, reward model, and pairwise judge to compute rewards, log-probabilities, and losses, while handling data collation, metric computation, and distributed logging.", "business_intent": "Provide a ready-to-use training pipeline for aligning large language models with human preferences through Nash-based multi-objective DPO, reducing engineering effort and accelerating deployment of well-aligned AI systems.", "keywords": ["trainer", "NashMD", "online DPO", "language model fine-tuning", "reward model", "pairwise judge", "data collation", "distributed training", "loss computation", "logprob", "generation", "metrics", "model card"], "summary_hash": "7fff6054e940", "cached_at": "2026-02-09T05:59:30+00:00"}