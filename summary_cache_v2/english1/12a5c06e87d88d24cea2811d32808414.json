{"summary": "Implements a Swin Transformer attention block that applies spatial reduction to the key/value tensors and computes the attention output for vision models.", "business_intent": "Provide an efficient attention component for computerâ€‘vision transformers and enable head pruning to reduce model size and improve inference speed.", "keywords": ["attention", "Swin Transformer", "spatial reduction", "head pruning", "deep learning", "computer vision", "neural network", "model compression"], "summary_hash": "03054046c849", "cached_at": "2026-02-09T09:02:32+00:00"}