{"summary": "Defines a strategy for running Lightning models on a single hardware unit, handling device placement, precision, checkpointing, and providing local implementations of collective communication primitives such as gather, broadcast, synchronization, and reduction.", "business_intent": "Allow users to train and evaluate models on a single GPU or CPU without the complexity or overhead of distributed training, offering a streamlined experience for single-device workloads.", "keywords": ["single device", "strategy", "PyTorch Lightning", "device placement", "precision", "checkpointing", "collective operations", "gather", "broadcast", "synchronization", "reduction", "non-distributed"], "summary_hash": "affd5d6ec556", "cached_at": "2026-02-08T08:52:46+00:00"}