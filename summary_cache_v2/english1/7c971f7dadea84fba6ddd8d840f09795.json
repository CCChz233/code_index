{"summary": "Defines an abstract configuration object that encapsulates the sparsity pattern settings for a self‑attention layer, providing common properties for block‑sparse layouts and serving as a base for specialized configurations.", "business_intent": "Enable developers to specify and manage block‑sparse attention patterns, facilitating model compression and performance optimization in transformer architectures.", "keywords": ["sparsity", "configuration", "self-attention", "block-sparse", "layout", "abstract", "transformer", "model optimization"], "summary_hash": "fbb105d71b25", "cached_at": "2026-02-08T23:19:49+00:00"}