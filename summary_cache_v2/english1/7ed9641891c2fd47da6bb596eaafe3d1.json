{"summary": "Implements a single decoder layer of the Marian transformer model for TensorFlow, encapsulating self‑attention, cross‑attention and feed‑forward sub‑layers and exposing standard Keras build and call interfaces.", "business_intent": "Provides a reusable building block for constructing Marian‑based neural machine translation or other sequence‑to‑sequence models in TensorFlow, facilitating efficient decoding within transformer architectures.", "keywords": ["Marian", "decoder layer", "TensorFlow", "transformer", "self‑attention", "cross‑attention", "feed‑forward", "neural machine translation", "sequence‑to‑sequence", "Keras"], "summary_hash": "0a332fbccc5a", "cached_at": "2026-02-09T11:27:01+00:00"}