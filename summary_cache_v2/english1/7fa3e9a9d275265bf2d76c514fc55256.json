{"summary": "A configuration container for Vision Transformer hybrid models that encapsulates all architectural hyperparameters, backbone selection options, and initialization settings. It inherits from a generic pretrained configuration class and provides fields for transformer dimensions, attention heads, feed‑forward size, activation, dropout rates, layer‑norm epsilon, image and patch dimensions, channel count, and optional backbone specifications (including pretrained loading and timm integration).", "business_intent": "Enable developers and researchers to easily define, customize, and instantiate ViT‑Hybrid models with or without pretrained backbones, controlling model size, depth, and vision-specific settings for downstream computer‑vision tasks.", "keywords": ["configuration", "vision transformer", "hybrid model", "backbone", "pretrained weights", "transformer encoder", "attention heads", "hidden size", "image size", "patch size", "layer normalization", "dropout", "initialization"], "summary_hash": "968786b2ca16", "cached_at": "2026-02-09T11:23:00+00:00"}