{"summary": "Provides a drop-in compatible attention layer for timm models that utilizes xformers' sparsity-aware scaled dot‑product attention to improve efficiency.", "business_intent": "Enable faster and more memory‑efficient vision transformer inference and training by replacing standard timm attention with a sparse implementation.", "keywords": ["attention", "sparse attention", "scaled dot product", "xformers", "timm", "vision transformer", "efficiency"], "summary_hash": "8a576dc51c5d", "cached_at": "2026-02-08T23:18:47+00:00"}