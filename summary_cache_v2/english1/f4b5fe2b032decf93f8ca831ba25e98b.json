{"summary": "A mixin that enhances a transformer‑based model with lightweight adapter modules, providing utilities to add, configure, load, save, merge, and manage adapter weights and settings, while supporting PEFT features such as selective layer activation and weight tying.", "business_intent": "Enable efficient fine‑tuning and inference of large NLP models by leveraging adapter layers to reduce training cost and complexity, and to integrate seamlessly with existing model and optimizer pipelines.", "keywords": ["adapter", "transformer", "PEFT", "fine-tuning", "weight tying", "layer selection", "state dict", "checkpoint", "optimizer groups", "freeze parameters", "config merging", "inference"], "summary_hash": "11002a37fe9c", "cached_at": "2026-02-08T09:43:27+00:00"}