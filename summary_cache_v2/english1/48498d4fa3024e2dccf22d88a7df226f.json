{"summary": "Prepares batches of tokenized text for language model training by dynamically padding sequences to the longest example in a batch, optionally applying masked language modeling, and outputting tensors in the requested framework format.", "business_intent": "Streamline the preprocessing pipeline for training language models, especially masked LM variants, by handling padding, mask generation, and tensor conversion across NumPy, PyTorch, and TensorFlow environments.", "keywords": ["data collator", "language modeling", "dynamic padding", "masked language modeling", "tokenizer", "tensor conversion", "numpy", "pytorch", "tensorflow", "mlm probability", "padding multiple"], "summary_hash": "d84de08a90d6", "cached_at": "2026-02-09T08:06:09+00:00"}