{"summary": "This module implements a diffusion pipeline that converts textual prompts into animated video clips by integrating AnimateDiff motion modeling with Perturbed Attention Guidance. It handles text and image encoding, latent preparation, iterative denoising using a UNet and motion adapter, and final decoding through a VAE, while supporting adapters, LoRA, textual inversion, free‑initialization and free‑noise strategies.", "business_intent": "Offer developers and content creators a high‑level tool to generate customizable, high‑quality animated videos from natural language descriptions, enabling applications in media production, advertising, gaming, and AI‑driven creative workflows.", "keywords": ["diffusion", "text-to-video", "AnimateDiff", "Perturbed Attention Guidance", "video generation", "UNet", "VAE", "CLIP", "motion adapter", "LoRA", "IP‑Adapter", "free initialization", "free noise", "scheduler"], "summary_hash": "8d467346e363", "cached_at": "2026-02-09T05:40:13+00:00"}