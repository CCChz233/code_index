{"summary": "Implements a loss function that computes the sparse categorical cross‑entropy between integer‑encoded true class labels and model predictions, supporting logits or probability inputs and configurable reduction strategies.", "business_intent": "Enable training of multi‑class classification models by providing a differentiable loss that quantifies the discrepancy between predicted distributions and sparse ground‑truth labels, facilitating gradient‑based optimization.", "keywords": ["sparse categorical crossentropy", "classification loss", "integer labels", "logits", "probability distribution", "reduction", "sample weighting", "Keras", "neural network training"], "summary_hash": "48080b981c71", "cached_at": "2026-02-09T11:49:52+00:00"}