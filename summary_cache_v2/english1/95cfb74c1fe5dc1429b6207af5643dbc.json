{"summary": "Provides the Rectified Linear Unit activation, converting negative inputs to zero while leaving positive inputs unchanged, and includes helpers for invoking the activation and determining output specifications.", "business_intent": "Supply a fast, nonâ€‘linear activation component for neural network models to promote sparse activations and improve training efficiency.", "keywords": ["ReLU", "activation", "neural network", "deep learning", "non-linear", "tensor", "forward pass", "output specification", "static method"], "summary_hash": "85a9057c293e", "cached_at": "2026-02-09T11:31:15+00:00"}