{"summary": "Implements a mask for paged block-diagonal attention, translating page tables into key/value tensor masks while accommodating variable sequence lengths.", "business_intent": "Provide an efficient sparse attention mechanism for transformer models that operates on paged memory layouts, reducing computation by restricting attention to block-diagonal regions.", "keywords": ["attention mask", "paged attention", "block diagonal", "sparse transformer", "key/value tensors", "batch processing", "sequence length handling", "memory paging"], "summary_hash": "67cbfcc000d9", "cached_at": "2026-02-08T23:23:37+00:00"}