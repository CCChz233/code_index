{"summary": "This module contains acceptance tests that validate the behavior of the activation cache utilities in the transformer model library. The tests cover accumulation and decomposition of residual streams with layer normalization, inâ€‘memory MLP hooking, verification of logit attribute calculations against reference implementations, and stacking of head and neuron results across different input shapes.", "business_intent": "Guarantee that the activation cache mechanisms reliably capture and manipulate model activations, enabling accurate analysis, debugging, and research on transformer internals.", "keywords": ["activation cache", "transformer", "residual stream", "layer normalization", "MLP hook", "logits", "head results", "neuron results", "testing", "PyTorch", "HookedTransformer"], "summary_hash": "0720fd3d6966", "cached_at": "2026-02-08T13:22:00+00:00"}