{"summary": "A TensorFlow data structure that encapsulates the outputs of a transformer model, including the final hidden states, optional cached key/value tensors for fast autoregressive decoding, and optional per‑layer hidden states, self‑attention maps, and decoder cross‑attention maps.", "business_intent": "Standardize the representation of transformer model results so that downstream applications (e.g., text generation, translation, summarization) can easily access the necessary tensors and benefit from cached attention values to accelerate inference.", "keywords": ["TensorFlow", "transformer", "model output", "hidden states", "attention weights", "cross‑attention", "past key values", "caching", "sequential decoding", "inference acceleration"], "summary_hash": "cfcd033fd4e2", "cached_at": "2026-02-09T06:30:49+00:00"}