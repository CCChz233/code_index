{"summary": "This module implements the Databricks integration for the litellm library, providing classes that configure and execute chat completion and embedding requests against Databricks foundation model APIs. It translates OpenAI‑style parameters to Databricks equivalents, handles HTTP communication (including async streaming), and formats responses into litellm’s unified model response structures.", "business_intent": "Allow applications to access Databricks LLM and embedding services through a standard OpenAI‑compatible interface, simplifying the use of Databricks models for chat and vector generation within the litellm ecosystem.", "keywords": ["Databricks", "LLM", "chat completion", "embedding", "OpenAI compatibility", "API integration", "HTTP streaming", "configuration", "litellm"], "summary_hash": "38697a999ff3", "cached_at": "2026-02-08T07:56:11+00:00"}