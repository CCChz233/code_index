{"summary": "A transformer‑based language model that can operate either as a pure encoder using self‑attention or as a decoder with added cross‑attention layers, supporting seq2seq configurations when initialized accordingly.", "business_intent": "Provide flexible text encoding and generation capabilities for applications such as translation, summarization, and other natural‑language processing tasks that require either representation learning or conditional text generation.", "keywords": ["transformer", "encoder", "decoder", "cross‑attention", "self‑attention", "seq2seq", "language model", "embeddings", "head pruning"], "summary_hash": "75b18a51a9fb", "cached_at": "2026-02-09T08:36:56+00:00"}