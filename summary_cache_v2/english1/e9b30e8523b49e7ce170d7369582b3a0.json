{"summary": "Implements the ALiBi (Attention with Linear Biases) relative position embedding, producing linear bias tensors based on token distances for transformer attention, supporting both auto‑regressive decoding and symmetric encoder contexts.", "business_intent": "Provide an efficient, length‑agnostic positional bias mechanism for transformer models to enhance performance in language modeling and encoder‑decoder applications.", "keywords": ["ALiBi", "relative position embedding", "attention bias", "transformer", "auto-regressive decoder", "encoder", "linear biases", "positional encoding"], "summary_hash": "04d71d690f76", "cached_at": "2026-02-08T09:51:58+00:00"}