{"summary": "Provides an edge-focused softmax activation that normalizes values across graph edges, supporting both forward evaluation and gradient backpropagation for training neural networks.", "business_intent": "Allow graphâ€‘based models to compute and learn attention or importance scores for edges, improving the expressive power of graph neural networks in applications such as recommendation, fraud detection, and network analysis.", "keywords": ["edge softmax", "graph neural network", "attention", "normalization", "activation function", "gradient backpropagation", "deep learning", "edge weighting"], "summary_hash": "8b8ff894ece8", "cached_at": "2026-02-08T23:48:31+00:00"}