{"summary": "Creates an attention mask that enforces both causal (lower‑triangular from the bottom‑right) and a fixed‑size local window, allowing each query to attend only to keys whose distance from the final position is within a specified range.", "business_intent": "Enable transformer models to perform autoregressive processing with limited context, improving efficiency and memory usage while preserving causal dependencies.", "keywords": ["attention mask", "causal", "local window", "transformer", "sequence modeling", "autoregressive", "mask generation", "distance‑based"], "summary_hash": "aa8d00dc9b0b", "cached_at": "2026-02-08T23:22:49+00:00"}