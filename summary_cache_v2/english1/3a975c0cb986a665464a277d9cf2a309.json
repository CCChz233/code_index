{"summary": "A TensorFlow implementation of a Whisper decoder layer that performs self‑attention, cross‑attention with encoder outputs, and feed‑forward transformations to produce token embeddings for speech transcription.", "business_intent": "Provide the core decoding functionality of the Whisper speech‑to‑text model within TensorFlow, enabling end‑to‑end automatic speech recognition and language modeling pipelines.", "keywords": ["TensorFlow", "Whisper", "decoder layer", "transformer", "self‑attention", "cross‑attention", "feed‑forward network", "speech recognition", "automatic speech transcription", "neural network"], "summary_hash": "72a89c1f02fc", "cached_at": "2026-02-09T10:55:38+00:00"}