{"summary": "Defines a PyTorch module that learns a set of scalar weights to combine the hidden states from all transformer layers, producing a weighted average token representation for downstream sentence embedding tasks.", "business_intent": "Enhance the quality of sentence embeddings by aggregating information across multiple transformer layers, thereby improving performance in applications such as semantic search, clustering, and classification.", "keywords": ["weighted pooling", "layer aggregation", "sentence embeddings", "transformer hidden states", "PyTorch module", "representation learning", "semantic similarity"], "summary_hash": "3cf0d4741024", "cached_at": "2026-02-08T13:55:04+00:00"}