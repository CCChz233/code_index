{"summary": "Encodes input sequences with an LSTM, scanning tokens until the end‑of‑sentence marker is encountered and then returns the resulting hidden state as the sequence representation.", "business_intent": "Supply a compact, context‑aware vector for downstream NLP models such as classifiers, translators, or generators that require a fixed‑size encoding of variable‑length text.", "keywords": ["LSTM", "encoder", "EOS token", "hidden state", "sequence representation", "recurrent neural network", "text encoding"], "summary_hash": "60c54f487b3d", "cached_at": "2026-02-09T11:53:00+00:00"}