{"summary": "The module defines an evaluator that measures the effectiveness of sentence embedding models on information retrieval tasks. It computes standard retrieval metrics (e.g., MAP, NDCG, recall@k) by comparing query embeddings against a corpus using configurable similarity functions, handling batching, logging, and optional GPU acceleration.", "business_intent": "Provide a reliable way to benchmark and improve sentence‑embedding models for search and retrieval applications, helping teams assess model suitability for real‑world information retrieval use cases.", "keywords": ["information retrieval", "evaluation", "sentence embeddings", "similarity function", "retrieval metrics", "MAP", "NDCG", "recall@k", "benchmarking", "search", "torch"], "summary_hash": "1e47b2b2dc11", "cached_at": "2026-02-08T13:55:32+00:00"}