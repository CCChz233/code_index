{"summary": "Provides a neural network module that applies squeeze‑and‑excitation style channel attention to its input and then adds the original input back, forming a residual block.", "business_intent": "Boosts model accuracy and training stability by adaptively re‑weighting feature channels while preserving the original signal through a skip connection.", "keywords": ["squeeze-and-excitation", "residual connection", "neural network layer", "channel attention", "deep learning", "feature recalibration", "skip connection"], "summary_hash": "488b5039701a", "cached_at": "2026-02-08T11:49:44+00:00"}