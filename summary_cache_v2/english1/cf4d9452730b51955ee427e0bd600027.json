{"summary": "Encapsulates the computation of horizontal relative position bias for attention mechanisms, providing utilities to prepare inputs and generate bias tensors based on relative horizontal offsets.", "business_intent": "Improve model performance by incorporating learned horizontal positional relationships into attention layers, enabling better spatial reasoning in vision or sequence models.", "keywords": ["relative position", "bias", "horizontal", "attention", "transformer", "positional encoding", "embedding"], "summary_hash": "1a942ab6fdef", "cached_at": "2026-02-09T11:03:19+00:00"}