{"summary": "A modular transformer block that applies self‑attention followed by a feed‑forward network to transform input sequences.", "business_intent": "Enable developers to assemble transformer‑based models for natural language processing, computer vision, or other sequence modeling tasks.", "keywords": ["transformer", "self-attention", "feed-forward", "encoder block", "deep learning", "sequence modeling", "neural network"], "summary_hash": "b32bfedaaaa5", "cached_at": "2026-02-08T13:19:04+00:00"}