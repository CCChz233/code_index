{"summary": "Provides a set of utilities and thin wrappers around torch.distributed primitives to perform differentiable collective operations (copy, gather, reduce, scatter, all-reduce) across model‑parallel and sequence‑parallel regions, handling both forward and backward passes.", "business_intent": "Facilitate scalable distributed training of large neural networks by abstracting and automating communication patterns required for model and sequence parallelism while preserving gradient flow.", "keywords": ["distributed training", "model parallelism", "sequence parallelism", "collective communication", "autograd", "torch.distributed", "gradient propagation", "all-reduce", "gather", "scatter", "reduce"], "summary_hash": "c2459f21d249", "cached_at": "2026-02-08T23:29:57+00:00"}