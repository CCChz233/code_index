{"summary": "Implements a parameterized shifted Softplus activation that computes (1/β)·log(1+exp(β·x))−log(shift) element‑wise, providing a smooth approximation to ReLU with adjustable steepness and offset.", "business_intent": "Supply a configurable, numerically stable activation function for deep‑learning models, enabling fine‑tuning of non‑linearity strength (β) and output shift to improve model expressiveness and training dynamics.", "keywords": ["activation function", "shifted softplus", "beta parameter", "shift offset", "neural networks", "element-wise", "differentiable", "torch", "deep learning"], "summary_hash": "efb18af0e150", "cached_at": "2026-02-08T23:56:54+00:00"}