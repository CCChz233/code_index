{"summary": "A dataset abstraction that prepares examples for prompt‑tuning of frozen GPT models, handling data loading, tokenization, insertion of virtual prompt placeholders, length constraints, optional BOS/EOS tokens, padding, and loss‑mask generation, with distinct collate logic for training and inference.", "business_intent": "Facilitate rapid development of prompt‑learning pipelines for GPT models, supporting both training and generation scenarios.", "keywords": ["GPT", "prompt tuning", "p‑tuning", "dataset", "tokenizer", "virtual prompts", "task templates", "truncation", "padding", "loss mask", "collate function", "training", "inference"], "summary_hash": "d3287cf0adb7", "cached_at": "2026-02-08T10:01:18+00:00"}