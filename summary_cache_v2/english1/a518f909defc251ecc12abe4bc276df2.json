{"summary": "Provides a configurable transformer attention block capable of multi‑head self‑attention and optional cross‑attention, with support for dropout, bias, precision upcasting, token embeddings, and selectable normalization schemes.", "business_intent": "Supply a flexible, reusable attention component for deep learning models that require adaptable attention mechanisms, such as generative or diffusion networks.", "keywords": ["transformer", "attention", "multi-head", "cross-attention", "self-attention", "dropout", "bias", "upcast", "normalization", "layer_norm", "group_norm", "token embedding", "group size"], "summary_hash": "dd352ba85e27", "cached_at": "2026-02-09T04:33:25+00:00"}