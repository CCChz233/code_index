{"summary": "Implements the Swish activation function, a smooth, non‑linear transformation used in neural networks, mathematically equivalent to the SiLU function.", "business_intent": "Provide a ready‑to‑use activation layer that can be integrated into deep learning models to potentially improve training dynamics and model accuracy.", "keywords": ["Swish", "SiLU", "activation function", "neural network", "deep learning", "non-linear"], "summary_hash": "e535360397cf", "cached_at": "2026-02-08T09:23:09+00:00"}