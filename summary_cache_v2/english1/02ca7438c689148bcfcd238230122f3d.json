{"summary": "Container class for the outputs of a Mask2Former transformer decoder, extending the standard model output with per‑layer mask query logits and layer‑normalized intermediate activations, while also providing the usual hidden states and attention tensors.", "business_intent": "Facilitate downstream segmentation tasks by exposing decoder mask predictions and detailed layer‑wise representations, supporting model training, evaluation, and inference pipelines.", "keywords": ["transformer decoder", "mask predictions", "logits", "intermediate hidden states", "layernorm", "cross attentions", "output hidden states", "attention weights", "torch tensors", "segmentation model"], "summary_hash": "34c4017b2f50", "cached_at": "2026-02-09T09:40:23+00:00"}