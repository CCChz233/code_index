{"summary": "Utility module that implements CPU-based all-to-all communication primitives used by distributed sparse optimizer implementations in DGL's PyTorch backend.", "business_intent": "Enable scalable training of graph neural networks by providing efficient inter-process data exchange for sharded sparse parameters across multiple workers.", "keywords": ["distributed", "sparse optimizer", "all-to-all", "CPU tensors", "PyTorch", "DGL", "communication utilities"], "summary_hash": "0900dabc69e6", "cached_at": "2026-02-09T00:46:57+00:00"}