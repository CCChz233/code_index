{"summary": "Defines a causal, block-diagonal attention mask that limits each query to keys inside its block and within a configurable local window, creating a banded attention pattern for transformer models.", "business_intent": "Provide an efficient, memoryâ€‘optimized causal attention mechanism with locality constraints to improve performance and scalability of large sequence models.", "keywords": ["causal attention", "block diagonal", "local window", "banded mask", "transformer", "attention bias", "experimental feature"], "summary_hash": "1339d87fbffc", "cached_at": "2026-02-08T23:23:41+00:00"}