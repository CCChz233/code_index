{"summary": "Encapsulates a transformer‑based decoder model, offering utilities to configure, export, and run inference, while exposing model properties such as embedding dimensions, vocabulary size and maximum sequence length.", "business_intent": "Enable developers to integrate a pre‑trained or custom transformer decoder into NLP applications for tasks like text generation, translation, or summarization, with straightforward export and inference capabilities.", "keywords": ["transformer", "decoder", "language generation", "NLP", "embedding", "export", "inference", "vocab size", "sequence length", "hidden size"], "summary_hash": "a0c71c64df95", "cached_at": "2026-02-08T09:37:37+00:00"}