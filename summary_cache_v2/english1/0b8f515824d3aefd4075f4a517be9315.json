{"summary": "Defines a Lightning Fabric strategy that wraps PyTorch's Fully Sharded Data Parallel (FSDP) to enable memory‑efficient, large‑scale distributed training. It manages FSDP initialization, sharding policies, CPU offloading, activation checkpointing, gradient‑sync control, and integrates checkpoint loading/saving and precision handling within the Fabric plugin architecture.", "business_intent": "Offer developers a ready‑to‑use, high‑performance distributed training solution that reduces memory consumption and simplifies the deployment of large models across multiple GPUs or nodes, while handling checkpointing and precision concerns automatically.", "keywords": ["FSDP", "Fully Sharded Data Parallel", "distributed training", "memory efficiency", "checkpointing", "activation checkpointing", "gradient synchronization", "sharding strategy", "CPU offload", "Lightning Fabric", "parallel strategy", "precision handling", "optimizer state"], "summary_hash": "f9d2f5f59427", "cached_at": "2026-02-08T09:02:22+00:00"}