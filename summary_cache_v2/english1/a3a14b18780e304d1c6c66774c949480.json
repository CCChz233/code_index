{"summary": "Provides a flash‑attention implementation of the MBart attention layer, preserving the original MBartAttention weights while overriding the forward pass to call the flash‑attention API and correctly handle padding tokens. Includes helper methods for reshaping tensors, unpadding inputs, and wrapping the flash‑attention call.", "business_intent": "Speed up and reduce memory usage of MBart models by using flash attention for efficient attention computation, enabling faster training and inference without altering pretrained weights.", "keywords": ["MBart", "FlashAttention", "Transformer", "Attention", "Padding handling", "Efficient computation", "PyTorch", "Module", "Forward pass", "Performance optimization"], "summary_hash": "666626787174", "cached_at": "2026-02-09T11:04:29+00:00"}