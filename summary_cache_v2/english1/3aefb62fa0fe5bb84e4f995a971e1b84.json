{"summary": "Implements the Performer attention mechanism using Flax, providing a linear‑time approximation of softmax attention for transformer models.", "business_intent": "Enable high‑performance, scalable attention in deep learning applications, reducing computational cost for long‑sequence processing.", "keywords": ["attention", "performer", "linear attention", "transformer", "flax", "jax", "efficient", "scalable", "kernel feature map"], "summary_hash": "dbb8f636a58a", "cached_at": "2026-02-09T06:00:29+00:00"}