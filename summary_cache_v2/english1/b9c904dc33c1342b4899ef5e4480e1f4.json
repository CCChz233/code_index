{"summary": "Provides an efficient scaled dot‑product attention processor that fuses projection layers (query, key, value for self‑attention; key and value for cross‑attention) using PyTorch 2.0 fused kernels, acting as a drop‑in component for transformer attention modules.", "business_intent": "Boost performance of transformer models by reducing memory traffic and compute overhead through fused attention projections, offering a seamless, high‑speed alternative to standard attention implementations.", "keywords": ["scaled dot-product attention", "fused projection", "self-attention", "cross-attention", "PyTorch 2.0", "performance optimization", "transformer", "experimental API"], "summary_hash": "c1ef0049f66d", "cached_at": "2026-02-09T04:06:46+00:00"}