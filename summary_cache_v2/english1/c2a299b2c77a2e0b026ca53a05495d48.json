{"summary": "Implements the scaled dot‑product attention mechanism with optional PyTorch 2.0 acceleration, applying layer normalization and rotary positional embeddings to the query and key tensors for the HunyuanDiT transformer.", "business_intent": "Provides a high‑performance attention component for generative AI models, enabling faster and more accurate image or text generation pipelines that rely on the HunyuanDiT architecture.", "keywords": ["scaled dot-product attention", "rotary embedding", "layer normalization", "PyTorch 2.0", "HunyuanDiT", "attention processor", "transformer"], "summary_hash": "75daf7ae827d", "cached_at": "2026-02-09T04:06:27+00:00"}