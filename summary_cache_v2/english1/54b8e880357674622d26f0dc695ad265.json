{"summary": "Implements the Exponential Linear Unit (ELU) activation component, managing its configuration, applying the activation to input tensors, and determining the resulting output characteristics.", "business_intent": "Provide a reusable activation module for neural network architectures to introduce nonâ€‘linear behavior with exponential handling of negative values, supporting model building and inference pipelines.", "keywords": ["ELU", "activation function", "neural network", "deep learning", "non-linear transformation", "forward pass", "output specification", "layer"], "summary_hash": "9b4d6ea9dd5a", "cached_at": "2026-02-09T11:34:08+00:00"}