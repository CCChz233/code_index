{"summary": "A decorator that wraps a transformer model, automatically caching the embeddings produced by each layer during forward passes and, when a specific layer index is set, returning the cached embedding for that layer instead of recomputing it.", "business_intent": "Facilitate efficient reuse and inspection of transformer layer representations for downstream tasks, analysis, or debugging by eliminating redundant computation and providing direct access to intermediate embeddings.", "keywords": ["decorator", "transformer", "embedding cache", "layer outputs", "forward override", "cache retrieval", "neural network optimization", "NLP model"], "summary_hash": "f8f1bdc21134", "cached_at": "2026-02-08T13:44:36+00:00"}