{"summary": "The module implements a suite of dataset utilities for supervised fine‑tuning of multimodal language models. It provides lazy‑loaded image and video readers that can source data from either tar archives or regular directories, preprocesses visual inputs, tokenizes textual prompts, and collates individual examples into padded batches suitable for training.", "business_intent": "To facilitate efficient training of large multimodal AI models by supplying ready‑to‑use, scalable data pipelines that handle mixed text‑image‑video inputs, reducing preprocessing overhead and enabling seamless integration with NeMo training workflows.", "keywords": ["multimodal dataset", "image loading", "video loading", "lazy loading", "tar archive", "supervised fine‑tuning", "batch collator", "tokenization", "NeVA", "NeMo", "preprocessing"], "summary_hash": "d2e61352fe2d", "cached_at": "2026-02-08T11:05:01+00:00"}