{"summary": "A wrapper around the underlying device-specific data loader that maintains global batch size and dataset length information for distributed training, while coordinating random number generator state across preloading threads on XLA devices.", "business_intent": "Enable consistent and efficient data loading in multi‑process XLA training by exposing total batch dimensions and handling RNG synchronization, simplifying integration with distributed pipelines.", "keywords": ["data loading", "distributed training", "XLA", "batch size scaling", "dataset length", "RNG synchronization", "wrapper", "multi‑process"], "summary_hash": "466fc0693dd5", "cached_at": "2026-02-09T02:09:22+00:00"}