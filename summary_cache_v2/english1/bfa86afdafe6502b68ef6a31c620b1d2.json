{"summary": "Implements the Leaky ReLU activation function, applying a piecewise linear transformation to input tensors and managing the resulting output specifications.", "business_intent": "Enable neural network models to use a nonâ€‘saturating activation that preserves gradients for negative inputs, improving training stability and performance.", "keywords": ["Leaky ReLU", "activation function", "neural network", "deep learning", "gradient flow", "output shape", "tensor transformation"], "summary_hash": "63e038f01222", "cached_at": "2026-02-09T11:34:01+00:00"}