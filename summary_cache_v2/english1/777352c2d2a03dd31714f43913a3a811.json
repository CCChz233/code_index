{"summary": "Implements the masked language modeling head for ERNIE models, transforming encoder hidden states into vocabulary logits for token prediction.", "business_intent": "Facilitate fine‑tuning or pre‑training of ERNIE models on masked language modeling tasks by providing a dedicated head that computes prediction scores and optional loss.", "keywords": ["ERNIE", "masked language modeling", "MLM head", "neural network", "forward pass", "logits", "token prediction", "pretraining", "fine-tuning"], "summary_hash": "d5b7f5943d95", "cached_at": "2026-02-09T09:07:53+00:00"}