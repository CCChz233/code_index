{"summary": "Implements a configurable Conformer encoder block that combines multi‑head self‑attention (with selectable absolute or relative positional encoding and optional local attention), a positionwise feed‑forward network, a depthwise convolution module, and dropout layers. It also supports the insertion of global tokens for separate global attention handling.", "business_intent": "Provides a flexible, high‑performance component for building speech and audio processing models (e.g., automatic speech recognition) that require the Conformer architecture with adaptable attention strategies and efficient global context handling.", "keywords": ["Conformer", "encoder layer", "self‑attention", "relative positional encoding", "local attention", "global tokens", "multi‑head attention", "feed‑forward network", "depthwise convolution", "dropout", "speech recognition"], "summary_hash": "7278f0f44cad", "cached_at": "2026-02-08T09:29:20+00:00"}