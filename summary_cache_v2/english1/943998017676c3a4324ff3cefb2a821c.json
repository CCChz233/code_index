{"summary": "Implements an unfused LoRA adapter that projects hidden representations to a dimension four times larger, typically used in transformer feed‑forward layers.", "business_intent": "Provide a lightweight, parameter‑efficient fine‑tuning component for large language models, allowing low‑rank adaptation from hidden size to 4× hidden size without fused operations, thereby enhancing flexibility and compatibility in model customization.", "keywords": ["LoRA", "adapter", "transformer", "feed-forward", "parameter-efficient fine-tuning", "neural network", "unfused implementation", "hidden dimension expansion", "PyTorch"], "summary_hash": "3014006968d0", "cached_at": "2026-02-08T09:50:49+00:00"}