{"summary": "Implements a dense multi‑head attention layer that integrates graph‑derived bias into the attention computation, allowing transformer‑style interactions between node features while respecting graph structure.", "business_intent": "Provide a neural network component for graph representation learning that enhances transformer‑based attention with configurable additive or multiplicative bias, improving performance on graph‑centric tasks.", "keywords": ["multi-head attention", "graph bias", "transformer", "attention bias", "additive bias", "multiplicative bias", "dropout", "neural network module", "graph neural network", "DGL"], "summary_hash": "1792169ff716", "cached_at": "2026-02-08T23:51:54+00:00"}