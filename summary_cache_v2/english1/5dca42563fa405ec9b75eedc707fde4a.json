{"summary": "Provides a collection of helper functions to calculate common evaluation metrics used in the GLUE benchmark for natural language understanding models, such as classification accuracy, F1 score, Matthews correlation coefficient, and Pearson/Spearman correlation.", "business_intent": "Facilitates standardized performance measurement of NLP models on GLUE tasks, supporting model development, comparison, and reporting in research and production environments.", "keywords": ["GLUE", "benchmark", "evaluation metrics", "accuracy", "F1 score", "Matthews correlation", "Pearson correlation", "Spearman correlation", "NLP", "model assessment"], "summary_hash": "022673c14b22", "cached_at": "2026-02-08T11:34:22+00:00"}