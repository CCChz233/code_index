{"summary": "The module defines two metric classes that assess generated answers against predefined criteria. One class computes a numeric score by comparing responses to reference values, handling both single‑turn and multi‑turn interactions. The other class performs a binary pass/fail evaluation, optionally applying repeated self‑consistency checks and majority voting to determine the final outcome.", "business_intent": "Enable developers to quickly evaluate the quality of LLM‑generated content in retrieval‑augmented generation workflows using simple, criteria‑driven metrics, supporting both detailed scoring and binary validation across different conversational contexts.", "keywords": ["evaluation", "criteria", "scoring", "reference", "binary", "single-turn", "multi-turn", "self-consistency", "majority voting", "RAG", "LLM", "metrics"], "summary_hash": "4bf157d9a550", "cached_at": "2026-02-08T22:50:23+00:00"}