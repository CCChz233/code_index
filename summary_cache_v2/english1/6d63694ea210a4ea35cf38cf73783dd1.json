{"summary": "Provides a wrapper that aligns a learning‑rate scheduler with the optimizer’s actual training steps, ensuring the scheduler advances only when a genuine update occurs. It accounts for gradient accumulation, mixed‑precision overflow handling, and integrates with the accelerator’s state management.", "business_intent": "Maintain correct learning‑rate progression in complex training setups (e.g., distributed, mixed‑precision, gradient accumulation) without disrupting the original scheduler logic.", "keywords": ["learning rate scheduler", "optimizer", "gradient accumulation", "mixed precision", "overflow handling", "wrapper", "synchronization", "training step", "accelerator", "state management"], "summary_hash": "f4970515da09", "cached_at": "2026-02-09T02:17:56+00:00"}