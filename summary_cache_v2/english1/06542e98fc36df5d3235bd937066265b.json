{"summary": "Manages lazy creation, persistent storage, and execution of TensorRT engines derived from ONNX models, automatically reverting to PyTorch inference when the compiled engine cannot handle a given input profile.", "business_intent": "Provide high‑performance, production‑ready inference by exploiting TensorRT acceleration while ensuring robustness and flexibility through seamless fallback to the original Torch implementation.", "keywords": ["TensorRT", "ONNX conversion", "engine caching", "lazy export", "persistent engine storage", "fallback to PyTorch", "dynamic input profiles", "inference acceleration", "model deployment"], "summary_hash": "0dc8bd0e4a0d", "cached_at": "2026-02-08T11:05:14+00:00"}