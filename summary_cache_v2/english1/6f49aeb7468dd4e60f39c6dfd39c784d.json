{"summary": "Encapsulates the output layers (heads) used during ERNIE model pre‑training, handling the forward computation of tasks such as masked token prediction and sentence‑level objectives.", "business_intent": "Provide reusable pre‑training head components for training ERNIE language models, simplifying integration of task‑specific loss calculations.", "keywords": ["ERNIE", "pretraining heads", "masked language modeling", "sentence prediction", "forward pass", "neural network module", "transformer"], "summary_hash": "ab9407dc2522", "cached_at": "2026-02-09T09:07:58+00:00"}