{"summary": "This module defines a collection of attention bias classes and utilities for efficient transformer attention operations. It provides various mask implementations (e.g., causal, block‑diagonal, local, padded) that can be added to the attention scores before softmax, enabling custom attention patterns without loading dense tensors into memory. The module also includes internal helpers for managing sequence length information and constructing bias tensors, facilitating optimized memory‑efficient attention kernels.", "business_intent": "To offer a flexible, memory‑efficient way to apply custom attention masks in transformer models, improving performance and supporting diverse sequence layouts such as variable‑length, block‑diagonal, and local attention patterns.", "keywords": ["attention bias", "mask", "causal", "block‑diagonal", "local attention", "padded sequences", "memory‑efficient", "transformer", "torch"], "summary_hash": "73aa00fd022c", "cached_at": "2026-02-08T23:33:09+00:00"}