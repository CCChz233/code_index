{"summary": "TensorFlow layer that implements a Marian encoder block, combining multi‑head self‑attention with a position‑wise feed‑forward network and layer normalizations as defined in the Marian NMT architecture.", "business_intent": "Provides a reusable building block for assembling Marian encoder stacks, enabling efficient training and inference of neural machine translation or other sequence‑to‑sequence models.", "keywords": ["TensorFlow", "Marian", "encoder", "transformer", "self-attention", "feed-forward", "neural machine translation", "NMT", "deep learning", "layer"], "summary_hash": "c6fade3f2caa", "cached_at": "2026-02-09T11:26:58+00:00"}