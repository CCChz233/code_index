{"summary": "CPU‑optimized utilities for computing Recurrent Neural Network Transducer (RNNT) loss and performing inference, including parameter handling, metadata management, and custom LogSoftmax gradient computation, integrated with PyTorch autograd via Numba acceleration.", "business_intent": "Enable training and deployment of speech‑recognition models that use RNNT loss on CPU‑only hardware, reducing reliance on GPUs and supporting resource‑constrained or production environments.", "keywords": ["RNNT", "loss", "CPU", "Numba", "PyTorch autograd", "speech recognition", "ASR", "forward pass", "backward pass", "LogSoftmax gradient", "inference"], "summary_hash": "1bc91eb81941", "cached_at": "2026-02-08T12:07:06+00:00"}