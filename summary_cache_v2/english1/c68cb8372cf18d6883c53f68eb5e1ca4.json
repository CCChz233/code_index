{"summary": "A configurable feed‑forward neural network module that builds a stack of linear layers with optional hidden size, dropout, and batch or layer normalization, optionally starting with a ReLU activation. It supports forward propagation of inputs and reinitialization of its parameters.", "business_intent": "Supply a reusable multi‑layer perceptron component for machine‑learning pipelines that need to transform feature vectors into target representations, such as classification, regression, or embedding generation.", "keywords": ["MLP", "feedforward neural network", "dense layers", "dropout", "normalization", "ReLU", "parameter initialization", "deep learning"], "summary_hash": "ec47909b8eb8", "cached_at": "2026-02-08T23:55:21+00:00"}