{"summary": "Provides a plugin that enables double‑precision (torch.float64) training by converting inputs, modules, and outputs to 64‑bit tensors and managing the appropriate forward and initialization contexts.", "business_intent": "Allows developers and researchers to run neural network training with higher numerical accuracy for scientific, engineering, or financial applications that demand reduced rounding errors.", "keywords": ["double precision", "torch.float64", "high‑precision training", "tensor conversion", "module conversion", "context management", "numerical stability", "scientific computing", "PyTorch plugin"], "summary_hash": "3b02f94a5444", "cached_at": "2026-02-08T08:29:23+00:00"}