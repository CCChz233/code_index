{"summary": "Provides a complete example for building and training a tree-structured Long Short-Term Memory (Tree‑LSTM) network with MXNet and DGL, covering dataset download, word embedding preparation, batch construction, model definition, training loop, evaluation on a development set, and model checkpointing for the Stanford Sentiment Treebank task.", "business_intent": "Enable researchers and developers to quickly experiment with hierarchical graph neural networks for natural language processing, offering a reproducible benchmark and educational reference for sentiment analysis using Tree‑LSTM architectures.", "keywords": ["Tree-LSTM", "MXNet", "DGL", "sentiment analysis", "Stanford Sentiment Treebank", "GloVe embeddings", "graph neural network", "recursive neural network", "training pipeline", "model evaluation", "NLP research"], "summary_hash": "db2b0d64d528", "cached_at": "2026-02-09T00:50:35+00:00"}