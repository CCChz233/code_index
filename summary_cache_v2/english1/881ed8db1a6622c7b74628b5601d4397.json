{"summary": "Implements a quick approximation of the Gaussian Error Linear Unit (GELU) activation, offering faster computation at the cost of some precision.", "business_intent": "Enable faster neural network inference and training by substituting the standard GELU with a lightweight, speed‑optimized version, improving throughput for performance‑critical applications.", "keywords": ["GELU", "activation function", "approximation", "fast computation", "neural networks", "performance optimization", "inference speed", "accuracy trade‑off"], "summary_hash": "ebba0076c470", "cached_at": "2026-02-09T06:23:28+00:00"}