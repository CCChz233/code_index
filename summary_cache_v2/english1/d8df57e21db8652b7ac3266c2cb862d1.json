{"summary": "Implements the Rectified Linear Unit (ReLU) activation with automatic differentiation support, offering forward evaluation and backward gradient computation.", "business_intent": "Facilitate neural network training by providing a ReLU activation that can be applied during the forward pass and propagate gradients during backpropagation.", "keywords": ["ReLU", "activation function", "forward pass", "backward pass", "autograd", "gradient computation", "neural network", "deep learning"], "summary_hash": "013a699722ac", "cached_at": "2026-02-08T11:15:28+00:00"}