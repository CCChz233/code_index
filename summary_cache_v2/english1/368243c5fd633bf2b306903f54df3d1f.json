{"summary": "A PyTorch dataset that preprocesses text examples into token IDs, attention masks, and optional BERT embeddings, providing indexed access and batch collation for model training.", "business_intent": "Facilitate fast and reliable loading of BERT-ready inputs to accelerate NLP model development and deployment.", "keywords": ["BERT", "embedding", "dataset", "PyTorch", "attention mask", "collate", "tokenization", "NLP", "batch processing"], "summary_hash": "e5a388633610", "cached_at": "2026-02-08T09:58:19+00:00"}