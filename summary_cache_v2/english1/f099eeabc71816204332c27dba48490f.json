{"summary": "Encapsulates the operations required for a single training iteration of a BERT model, handling input batch preparation, forward propagation, and loss calculation for both pre‑training and fine‑tuning within the Megatron‑LM framework.", "business_intent": "Provide a reusable component that streamlines BERT model training, enabling efficient execution of large‑scale language model training loops for research and production use cases.", "keywords": ["BERT", "training step", "forward pass", "batch processing", "loss computation", "pretraining", "finetuning", "Megatron-LM", "large language model", "NLP"], "summary_hash": "c36b71e3f82e", "cached_at": "2026-02-09T02:10:11+00:00"}