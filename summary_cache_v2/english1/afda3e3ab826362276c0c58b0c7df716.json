{"summary": "Implements the encoder part of the Autoformer architecture by stacking a configurable number of self‑attention layers to convert input sequences into rich contextual representations.", "business_intent": "Enable deep sequence modeling for tasks like time‑series forecasting and pattern analysis by providing an efficient, configurable transformer encoder that captures long‑range dependencies.", "keywords": ["Autoformer", "encoder", "self-attention", "transformer", "sequence modeling", "time series", "deep learning", "neural network", "layer stacking", "configurable"], "summary_hash": "772378768a3e", "cached_at": "2026-02-09T10:34:31+00:00"}