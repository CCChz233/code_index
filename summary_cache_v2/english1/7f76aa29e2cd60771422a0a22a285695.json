{"summary": "The module defines a neural encoder based on the Squeezeformer architecture for automatic speech recognition. It processes acoustic feature tensors through configurable subsampling, a stack of Squeezeformer layers that combine feed‑forward, convolutional, and self‑attention mechanisms, and optional time‑resolution reduction and recovery stages. An accompanying adapter manager enables insertion and control of adapter modules for fine‑tuning or domain adaptation.", "business_intent": "Supply a flexible, high‑efficiency encoder component that can be integrated into speech‑to‑text systems, allowing developers to customize model depth, subsampling strategies, and adapter‑based adaptation to improve recognition performance across different languages or acoustic conditions.", "keywords": ["Squeezeformer", "encoder", "speech recognition", "ASR", "subsampling", "self‑attention", "convolution", "feed‑forward", "time reduction", "adapters", "NeMo", "PyTorch"], "summary_hash": "daa856eb106d", "cached_at": "2026-02-08T11:07:31+00:00"}